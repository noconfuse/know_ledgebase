# -*- coding: utf-8 -*-
"""
æ™ºèƒ½å…ƒæ•°æ®æå–å™¨
æ ¹æ®ç”¨æˆ·åé¦ˆé‡æ–°è®¾è®¡çš„å…ƒæ•°æ®æå–ç³»ç»Ÿï¼š
1. åŒºåˆ†æ–‡æ¡£çº§å…ƒæ•°æ®ï¼ˆä¸€æ¬¡æ€§æå–ï¼‰å’Œchunkçº§å…ƒæ•°æ®ï¼ˆæ¯ä¸ªchunkæå–ï¼‰
2. åˆå¹¶åŸæœ‰çš„ç»Ÿä¸€æå–å™¨å’Œå¢å¼ºæå–å™¨
3. æ ¹æ®chunkå¤§å°æ™ºèƒ½å†³å®šæå–å“ªäº›å…ƒæ•°æ®
4. æ–‡æ¡£çº§å…ƒæ•°æ®ä¼šè‡ªåŠ¨ç»§æ‰¿åˆ°æ‰€æœ‰chunkä¸­
"""

import logging
import asyncio
import re
from typing import List, Dict, Sequence, Any, Optional
from tenacity import retry, stop_after_attempt, wait_fixed
from llama_index.core.llms.llm import LLM
from pydantic import BaseModel, Field, ValidationError
from llama_index.core.extractors import BaseExtractor
from llama_index.core.program import LLMTextCompletionProgram
from llama_index.core.schema import BaseNode
from llama_index.core.output_parsers import ChainableOutputParser, PydanticOutputParser

from models.metadata_models import DocumentLevelMetadata, ChunkLevelMetadata
from services.metadata_cache_manager import MetadataCacheManager
from config import settings

logger = logging.getLogger(__name__)


# åŠ¨æ€æ¨¡æ¿ç”Ÿæˆå‡½æ•°
def _create_legal_document_template() -> str:
    """ä¸ºæ³•å¾‹æ–‡æ¡£åˆ›å»ºåŸºäºæ¨¡å‹å­—æ®µçš„æ–‡æ¡£çº§æå–æ¨¡æ¿"""
    from models.metadata_models import LegalDocumentMetadata
    
    fields_desc = []
    for field_name, model_field in LegalDocumentMetadata.model_fields.items():
        fields_desc.append(f"- **{field_name}**: {model_field.description}")
    
    return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ–‡æ¡£å†…å®¹ï¼Œæå–æ–‡æ¡£çº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

æ–‡æ¡£å†…å®¹:
----------------
{{context_str}}
----------------

è¯·æå–ä»¥ä¸‹æ³•å¾‹æ–‡æ¡£çº§ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""

def _create_legal_chunk_template() -> str:
    """ä¸ºæ³•å¾‹æ–‡æ¡£åˆ›å»ºåŸºäºæ¨¡å‹å­—æ®µçš„chunkçº§æå–æ¨¡æ¿"""
    from models.metadata_models import ChunkLevelMetadata
    
    fields_desc = []
    for field_name, model_field in ChunkLevelMetadata.model_fields.items():
        fields_desc.append(f"- **{field_name}**: {model_field.description}")
    
    return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ–‡æ¡£ç‰‡æ®µå†…å®¹ï¼Œæå–chunkçº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

æ–‡æœ¬å†…å®¹:
----------------
{{context_str}}
----------------

æ–‡æœ¬é•¿åº¦: {{text_length}} å­—ç¬¦

è¯·æå–ä»¥ä¸‹chunkçº§ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

{{optional_fields}}

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""

def _create_policy_news_document_template() -> str:
    """ä¸ºæ”¿ç­–æ–°é—»æ–‡æ¡£åˆ›å»ºåŸºäºæ¨¡å‹å­—æ®µçš„æ–‡æ¡£çº§æå–æ¨¡æ¿"""
    from models.metadata_models import PolicyDocumentMetadata
    
    fields_desc = []
    for field_name, model_field in PolicyDocumentMetadata.model_fields.items():
        fields_desc.append(f"- **{field_name}**: {model_field.description}")
    
    return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ”¿ç­–æ–°é—»æ–‡æ¡£å†…å®¹ï¼Œæå–æ–‡æ¡£çº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

æ–‡æ¡£å†…å®¹:
----------------
{{context_str}}
----------------

è¯·æå–ä»¥ä¸‹æ”¿ç­–æ–‡æ¡£çº§ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""

def _create_policy_news_chunk_template() -> str:
    """ä¸ºæ”¿ç­–æ–°é—»æ–‡æ¡£åˆ›å»ºåŸºäºæ¨¡å‹å­—æ®µçš„chunkçº§æå–æ¨¡æ¿"""
    from models.metadata_models import ChunkLevelMetadata
    
    fields_desc = []
    for field_name, model_field in ChunkLevelMetadata.model_fields.items():
        fields_desc.append(f"- **{field_name}**: {model_field.description}")
    
    return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ”¿ç­–æ–°é—»æ–‡æ¡£ç‰‡æ®µå†…å®¹ï¼Œæå–chunkçº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

æ–‡æœ¬å†…å®¹:
----------------
{{context_str}}
----------------

æ–‡æœ¬é•¿åº¦: {{text_length}} å­—ç¬¦

è¯·æå–ä»¥ä¸‹chunkçº§ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

{{optional_fields}}

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""

CLASSIFICATION_PROMPT_TEMPLATE = """
Based on the content of the document below, classify it into one of the following categories: 'legal_document' or 'policy_news'.
Return only the category name.

---
{context_str}
---

Category: """

class SmartMetadataExtractor(BaseExtractor):
    """æ™ºèƒ½å…ƒæ•°æ®æå–å™¨"""
    
    llm: LLM = Field(description="è¯­è¨€æ¨¡å‹å®ä¾‹")
    min_chunk_size_for_extraction: int = Field(default=20, description="è¿›è¡Œå…ƒæ•°æ®æå–çš„æœ€å°chunkå¤§å°ï¼Œä½äºæ­¤é•¿åº¦çš„chunkå°†è·³è¿‡æå–")
    min_chunk_size_for_summary: int = Field(default=512, description="ç”Ÿæˆæ‘˜è¦çš„æœ€å°chunkå¤§å°")
    min_chunk_size_for_qa: int = Field(default=1024, description="ç”Ÿæˆé—®ç­”å¯¹çš„æœ€å°chunkå¤§å°")
    max_keywords: int = Field(default=5, description="è¦æå–çš„æœ€å¤§å…³é”®è¯æ•°")
    # ç§»é™¤å†…å­˜ç¼“å­˜ï¼Œæ”¹ä¸ºä½¿ç”¨åŸºäºæ–‡ä»¶åçš„æœ¬åœ°ç¼“å­˜
    persistent_cache_manager: Optional[MetadataCacheManager] = Field(default=None, description="æŒä¹…åŒ–ç¼“å­˜ç®¡ç†å™¨")

    def __init__(self, llm: Any, min_chunk_size_for_extraction: int = None, min_chunk_size_for_summary: int = None, min_chunk_size_for_qa: int = None, max_keywords: int = None, enable_persistent_cache: bool = True, cache_dir: str = "cache/metadata", **kwargs):
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
        if min_chunk_size_for_extraction is None:
            min_chunk_size_for_extraction = settings.MIN_CHUNK_SIZE_FOR_EXTRACTION
        if min_chunk_size_for_summary is None:
            min_chunk_size_for_summary = settings.MIN_CHUNK_SIZE_FOR_SUMMARY
        if min_chunk_size_for_qa is None:
            min_chunk_size_for_qa = settings.MIN_CHUNK_SIZE_FOR_QA
        if max_keywords is None:
            max_keywords = settings.MAX_KEYWORDS
        # åˆå§‹åŒ–æŒä¹…åŒ–ç¼“å­˜ç®¡ç†å™¨
        persistent_cache_manager = MetadataCacheManager(cache_dir) if enable_persistent_cache else None
        
        super().__init__(
            llm=llm, 
            min_chunk_size_for_extraction=min_chunk_size_for_extraction,
            min_chunk_size_for_summary=min_chunk_size_for_summary, 
            min_chunk_size_for_qa=min_chunk_size_for_qa, 
            max_keywords=max_keywords, 
            persistent_cache_manager=persistent_cache_manager,
            **kwargs
        )

    def _create_default_chunk_template(self) -> str:
        """ä¸ºchunkçº§å…ƒæ•°æ®åˆ›å»ºé»˜è®¤çš„é€šç”¨æå–æ¨¡æ¿"""
        
        # ä»Pydanticæ¨¡å‹ä¸­åŠ¨æ€ç”Ÿæˆå­—æ®µæè¿°
        fields_desc = []
        for field_name, model_field in ChunkLevelMetadata.model_fields.items():
            # æ’é™¤å¯é€‰å­—æ®µï¼Œå®ƒä»¬å°†ç”± _create_chunk_program åŠ¨æ€æ·»åŠ 
            if model_field.is_required():
                fields_desc.append(f"- **{field_name}**: {model_field.description}")
        
        return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µå†…å®¹ï¼Œæå–chunkçº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

æ–‡æœ¬å†…å®¹:
----------------
{{context_str}}
----------------

æ–‡æœ¬é•¿åº¦: {{text_length}} å­—ç¬¦

è¯·æå–ä»¥ä¸‹æ ¸å¿ƒä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

{{optional_fields}}

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""

    def _create_default_document_template(self) -> str:
        """ä¸ºæ–‡æ¡£çº§å…ƒæ•°æ®åˆ›å»ºé»˜è®¤çš„é€šç”¨æå–æ¨¡æ¿"""
        
        # ä»Pydanticæ¨¡å‹ä¸­åŠ¨æ€ç”Ÿæˆå­—æ®µæè¿°
        fields_desc = []
        for field_name, model_field in DocumentLevelMetadata.model_fields.items():
            fields_desc.append(f"- **{field_name}**: {model_field.description}")
        
        return f"""
è¯·æ ¹æ®ä»¥ä¸‹å®Œæ•´æ–‡æ¡£å†…å®¹ï¼Œæå–æ–‡æ¡£çº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚è¿™äº›ä¿¡æ¯å°†åº”ç”¨äºæ•´ä¸ªæ–‡æ¡£çš„æ‰€æœ‰éƒ¨åˆ†ã€‚

æ–‡æ¡£å†…å®¹:
----------------
{{context_str}}
----------------

è¯·æå–ä»¥ä¸‹æ–‡æ¡£çº§ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""
    
    def _create_fallback_metadata(self, title: str, doc_id: str) -> Dict[str, Any]:
        """åˆ›å»ºå›é€€å…ƒæ•°æ®"""
        # å°è¯•ä»æŒä¹…åŒ–ç¼“å­˜è·å–æ–‡æ¡£çº§å…ƒæ•°æ®
        doc_metadata = {}
        if self.persistent_cache_manager:
            cached_data = self.persistent_cache_manager.get_cached_metadata(doc_id=doc_id)
            if cached_data:
                doc_metadata = cached_data.get("metadata", {})
        
        fallback_metadata = {
            "title": title,
            "summary": "",
            "keywords": [],
            "qa_pairs": [],
            "is_fallback": True
        }
        
        # ä½¿ç”¨æ™ºèƒ½åˆå¹¶é€»è¾‘
        return self._merge_document_and_chunk_metadata(doc_metadata, fallback_metadata)
    
    def _optimize_document_content_for_extraction(self, text_content: str, doc_id: str) -> str:
        """ä¼˜åŒ–æ–‡æ¡£å†…å®¹ä»¥æé«˜å…ƒæ•°æ®æå–æ•ˆç‡
        
        å¯¹äºè¶…å¤§æ–‡æ¡£ï¼Œé‡‡ç”¨ä»¥ä¸‹ä¼˜åŒ–ç­–ç•¥ï¼š
        1. å¯¹äºè¶…è¿‡é…ç½®é˜ˆå€¼çš„æ–‡æ¡£ï¼Œæå–å…³é”®ç« èŠ‚
        2. å¯¹äºè¶…å¤§æ–‡æ¡£ï¼Œä½¿ç”¨æ–‡æ¡£æ‘˜è¦æˆ–å…³é”®æ®µè½
        3. ä¿ç•™æ–‡æ¡£ç»“æ„ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ç« èŠ‚ç­‰ï¼‰
        
        Args:
            text_content: åŸå§‹æ–‡æ¡£å†…å®¹
            doc_id: æ–‡æ¡£ID
            
        Returns:
            ä¼˜åŒ–åçš„æ–‡æ¡£å†…å®¹
        """
        from config import settings
        
        content_length = len(text_content)
        
        # å°æ–‡æ¡£ç›´æ¥è¿”å›
        if content_length <= settings.DOC_CONTENT_OPTIMIZATION_THRESHOLD:
            return text_content
            
        logger.info(f"ğŸ“Š [CONTENT OPTIMIZER] Optimizing large document: {doc_id} ({content_length} chars)")
        
        try:
            # ç­–ç•¥1: æå–æ–‡æ¡£ç»“æ„å’Œå…³é”®éƒ¨åˆ†
            if content_length <= settings.DOC_LARGE_CONTENT_THRESHOLD:
                return self._extract_key_sections(text_content)
            
            # ç­–ç•¥2: è¶…å¤§æ–‡æ¡£ä½¿ç”¨æ™ºèƒ½æ‘˜è¦
            else:
                return self._extract_document_summary_sections(text_content)
                
        except Exception as e:
            logger.warning(f"âš ï¸ [CONTENT OPTIMIZER] Optimization failed for {doc_id}: {e}, using truncated content")
            # é™çº§ç­–ç•¥ï¼šæˆªå–å‰é…ç½®é•¿åº¦çš„å­—ç¬¦
            fallback_length = settings.DOC_MAX_OPTIMIZED_LENGTH
            return text_content[:fallback_length] + "\n\n[æ–‡æ¡£å†…å®¹å·²æˆªæ–­ä»¥ä¼˜åŒ–å¤„ç†æ•ˆç‡]" if content_length > fallback_length else text_content
    
    def _extract_key_sections(self, text_content: str) -> str:
        """æå–æ–‡æ¡£çš„å…³é”®ç« èŠ‚ï¼ˆé€‚ç”¨äºä¸­ç­‰é•¿åº¦æ–‡æ¡£ï¼‰"""
        from config import settings
        
        lines = text_content.split('\n')
        key_sections = []
        current_section = []
        
        # å®šä¹‰å…³é”®ç« èŠ‚æ ‡è¯†ç¬¦
        section_patterns = [
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |Chapter\s+\d+)',  # ç« èŠ‚
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¡|Article\s+\d+)',  # æ¡æ–‡
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+èŠ‚|Section\s+\d+)',  # èŠ‚
            r'^(\d+\.|\d+ã€|\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\))',  # ç¼–å·åˆ—è¡¨
            r'^(#{1,6}\s+)',  # Markdownæ ‡é¢˜
            r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|[A-Z]\.|\d+\.)\s*[^\s]',  # ä¸­æ–‡åºå·
        ]
        
        import re
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é”®ç« èŠ‚å¼€å§‹
            is_section_start = any(re.match(pattern, line) for pattern in section_patterns)
            
            if is_section_start:
                # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚ï¼ˆå¦‚æœæœ‰å†…å®¹ä¸”ä¸è¶…è¿‡é…ç½®çš„æœ€å¤§ç« èŠ‚é•¿åº¦ï¼‰
                if current_section and len('\n'.join(current_section)) <= settings.DOC_MAX_SECTION_LENGTH:
                    key_sections.extend(current_section)
                    key_sections.append('')  # æ·»åŠ åˆ†éš”ç¬¦
                
                current_section = [line]
            else:
                current_section.append(line)
                
                # å¦‚æœå½“å‰ç« èŠ‚è¿‡é•¿ï¼Œæˆªæ–­
                if len('\n'.join(current_section)) > settings.DOC_MAX_SECTION_LENGTH:
                    key_sections.extend(current_section[:20])  # åªä¿ç•™å‰20è¡Œ
                    key_sections.append('[ç« èŠ‚å†…å®¹å·²æˆªæ–­]')
                    key_sections.append('')
                    current_section = []
        
        # å¤„ç†æœ€åä¸€ä¸ªç« èŠ‚
        if current_section and len('\n'.join(current_section)) <= settings.DOC_MAX_SECTION_LENGTH:
            key_sections.extend(current_section)
        
        result = '\n'.join(key_sections)
        
        # å¦‚æœæå–çš„å†…å®¹ä»ç„¶è¿‡é•¿ï¼Œè¿›ä¸€æ­¥æˆªæ–­
        if len(result) > settings.DOC_MAX_OPTIMIZED_LENGTH:
            result = result[:settings.DOC_MAX_OPTIMIZED_LENGTH] + "\n\n[å†…å®¹å·²ä¼˜åŒ–æˆªæ–­]" 
            
        return result
    
    def _extract_document_summary_sections(self, text_content: str) -> str:
        """æå–è¶…å¤§æ–‡æ¡£çš„æ‘˜è¦éƒ¨åˆ†ï¼ˆé€‚ç”¨äºè¶…é•¿æ–‡æ¡£ï¼‰"""
        from config import settings
        
        lines = text_content.split('\n')
        
        # æå–ç­–ç•¥ï¼šå¼€å¤´ã€å…³é”®æ ‡é¢˜ã€ç»“å°¾
        summary_parts = []
        
        # 1. æ–‡æ¡£å¼€å¤´ï¼ˆä½¿ç”¨é…ç½®çš„ç« èŠ‚é•¿åº¦ï¼‰
        beginning = '\n'.join(lines[:50])  # å‰50è¡Œ
        if len(beginning) > settings.DOC_MAX_SECTION_LENGTH:
            beginning = beginning[:settings.DOC_MAX_SECTION_LENGTH]
        summary_parts.append("=== æ–‡æ¡£å¼€å¤´ ===")
        summary_parts.append(beginning)
        summary_parts.append("")
        
        # 2. æå–ä¸»è¦æ ‡é¢˜å’Œç« èŠ‚ï¼ˆä¸­é—´éƒ¨åˆ†ï¼‰
        import re
        title_patterns = [
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« .*)',
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¡.*)',
            r'^(#{1,3}\s+.*)',  # Markdownä¸»æ ‡é¢˜
            r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€.*)',
        ]
        
        summary_parts.append("=== ä¸»è¦ç« èŠ‚æ ‡é¢˜ ===")
        title_count = 0
        for line in lines[50:-50]:  # è·³è¿‡å¼€å¤´å’Œç»“å°¾
            line = line.strip()
            if any(re.match(pattern, line) for pattern in title_patterns):
                summary_parts.append(line)
                title_count += 1
                if title_count >= 20:  # æœ€å¤š20ä¸ªæ ‡é¢˜
                    break
        summary_parts.append("")
        
        # 3. æ–‡æ¡£ç»“å°¾ï¼ˆä½¿ç”¨é…ç½®çš„ç« èŠ‚é•¿åº¦ï¼‰
        ending = '\n'.join(lines[-20:])  # å20è¡Œ
        if len(ending) > settings.DOC_MAX_SECTION_LENGTH:
            ending = ending[-settings.DOC_MAX_SECTION_LENGTH:]
        summary_parts.append("=== æ–‡æ¡£ç»“å°¾ ===")
        summary_parts.append(ending)
        
        result = '\n'.join(summary_parts)
        
        # ç¡®ä¿ç»“æœä¸è¶…è¿‡é…ç½®çš„æ‘˜è¦æœ€å¤§é•¿åº¦
        if len(result) > settings.DOC_SUMMARY_MAX_LENGTH:
            result = result[:settings.DOC_SUMMARY_MAX_LENGTH] + "\n\n[è¶…å¤§æ–‡æ¡£å·²æ™ºèƒ½æ‘˜è¦]" 
            
        return result

    def _merge_document_and_chunk_metadata(self, doc_metadata: Dict[str, Any], chunk_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†å±‚çº§å­˜å‚¨æ–‡æ¡£çº§å’Œchunkçº§å…ƒæ•°æ®
        
        Args:
            doc_metadata: æ–‡æ¡£çº§å…ƒæ•°æ®
            chunk_metadata: chunkçº§å…ƒæ•°æ®
            
        Returns:
            åˆ†å±‚çº§å­˜å‚¨çš„å…ƒæ•°æ®å­—å…¸
        """
        # åˆ†å±‚çº§å­˜å‚¨ï¼šchunkçº§å…ƒæ•°æ®ä¸ºä¸»ä½“ï¼Œæ–‡æ¡£çº§å…ƒæ•°æ®å•ç‹¬å­˜å‚¨
        layered_metadata = {
            # chunkçº§å…ƒæ•°æ®ä½œä¸ºä¸»ä½“
            **chunk_metadata,
            
            # æ–‡æ¡£çº§å…ƒæ•°æ®å•ç‹¬å­˜å‚¨åœ¨document_metadataå­—æ®µä¸­
            'document_metadata': doc_metadata,
        }
        
        return layered_metadata

    
    async def _classify_and_extract(self, document_text: str, doc_id: str) -> Dict[str, Any]:
        """åˆ†ç±»å¹¶æå–æ–‡æ¡£çº§å…ƒæ•°æ®"""
        # æ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜
        if self.persistent_cache_manager:
            logger.debug(f"Checking persistent cache for doc_id: {doc_id}")
            cached_metadata = self.persistent_cache_manager.get_cached_metadata(
                doc_id=doc_id, content=document_text
            )
            if cached_metadata:
                logger.info(f"Persistent cache hit for doc_id: {doc_id}")
                return cached_metadata

        # 1. åˆ†ç±»æ–‡æ¡£ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†ï¼‰
        try:
            classification_prompt = CLASSIFICATION_PROMPT_TEMPLATE.format(
                context_str=document_text[:4000]  # ä½¿ç”¨ç‰‡æ®µè¿›è¡Œåˆ†ç±»
            )
            
            response = await self.llm.acomplete(classification_prompt)
            category = response.text.strip().lower()
        except Exception as e:
            logger.warning(f"Document classification failed for {doc_id}: {e}")
            logger.warning("Using default 'generic' category due to LLM API error")
            category = "generic"

        # 2. é€‰æ‹©æ¨¡æ¿å’Œå…ƒæ•°æ®æ¨¡å‹
        if "legal" in category:
            from models.metadata_models import LegalDocumentMetadata
            doc_template = _create_legal_document_template()
            chunk_template = _create_legal_chunk_template()
            category_name = "legal_document"
            metadata_model = LegalDocumentMetadata
        elif "policy" in category:
            from models.metadata_models import PolicyDocumentMetadata
            doc_template = _create_policy_news_document_template()
            chunk_template = _create_policy_news_chunk_template()
            category_name = "policy_news"
            metadata_model = PolicyDocumentMetadata
        else:
            from models.metadata_models import DocumentLevelMetadata
            doc_template = self._create_default_document_template()
            chunk_template = self._create_default_chunk_template()
            category_name = "generic"
            metadata_model = DocumentLevelMetadata

        # 3. æå–æ–‡æ¡£çº§å…ƒæ•°æ®
        try:
            document_program = LLMTextCompletionProgram.from_defaults(
                output_cls=metadata_model,
                prompt_template_str=doc_template,
                llm=self.llm,
                verbose=True,
            )
            # æ·»åŠ APIè¯·æ±‚é—´éš”æ§åˆ¶ï¼Œé¿å…é¢‘ç‡é™åˆ¶
            await asyncio.sleep(settings.llm_model_settings.API_REQUEST_INTERVAL)
            
            result = await document_program.acall(context_str=document_text)
            doc_metadata = result.dict()
            doc_metadata["document_category"] = category_name
            
            # ç¼“å­˜ç»“æœï¼ŒåŒ…æ‹¬æ¨¡æ¿
            cache_data = {
                "metadata": doc_metadata,
                "chunk_template": chunk_template,
            }
            
            # ä¿å­˜åˆ°æŒä¹…åŒ–ç¼“å­˜
            if self.persistent_cache_manager:
                logger.info(f"Saving metadata to persistent cache for doc_id: {doc_id}")
                self.persistent_cache_manager.save_metadata_to_cache(
                    doc_id=doc_id, metadata=cache_data, content=document_text
                )
            
            logger.info(f"Document-level metadata extracted and cached (memory + persistent) for {doc_id}")
            return cache_data

        except Exception as e:
            logger.error(f"Error extracting document metadata for {doc_id}: {e}")
            
            # è®°å½•è¯¦ç»†çš„å¤±è´¥æ—¥å¿—
            import traceback
            traceback_info = traceback.format_exc()
            self._log_failed_document(
                doc_id=doc_id,
                document_text=document_text,
                category=category_name,
                error=e,
                traceback_info=traceback_info
            )
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯APIå“åº”æ ¼å¼é”™è¯¯
            if "choices" in str(e) or "KeyError" in str(e):
                logger.warning(f"LLM API response format error detected: {e}")
                logger.warning("This is likely due to API timeout or malformed response")
            
            # è¿”å›åŒ…å«åŸºæœ¬ä¿¡æ¯çš„é»˜è®¤å…ƒæ•°æ®
            fallback_metadata = {
                "document_id": doc_id,
                "document_summary": "å…ƒæ•°æ®æå–å¤±è´¥ - APIå“åº”å¼‚å¸¸",
                "document_category": category_name,
                "extraction_failed": True,
                "error_message": str(e)
            }
            
            cache_data = {
                "metadata": fallback_metadata,
                "chunk_template": chunk_template,
            }
            
            logger.warning(f"Using fallback metadata for {doc_id} due to extraction failure")
            
            return cache_data
    
    def clear_cache(self, doc_id: str = None, include_chunks: bool = True, include_failed_logs: bool = False):
        """æ¸…é™¤ç¼“å­˜
        
        Args:
            doc_id: è¦æ¸…é™¤çš„æ–‡æ¡£IDï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            include_chunks: æ˜¯å¦åŒæ—¶æ¸…é™¤chunkçº§åˆ«çš„ç¼“å­˜
            include_failed_logs: æ˜¯å¦åŒæ—¶æ¸…é™¤å¤±è´¥æ—¥å¿—
        """
        if doc_id:
            # æ¸…é™¤æŒ‡å®šæ–‡æ¡£çš„ç¼“å­˜
            if self.persistent_cache_manager:
                self.persistent_cache_manager.clear_cache(doc_id)
            
            # æ¸…é™¤æŒ‡å®šæ–‡æ¡£çš„å¤±è´¥æ—¥å¿—
            if include_failed_logs:
                self._clear_failed_document_logs(doc_id)
        else:
            # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            if self.persistent_cache_manager:
                self.persistent_cache_manager.clear_cache()
                if include_chunks:
                    self.persistent_cache_manager.clear_chunk_cache()
            
            # æ¸…é™¤æ‰€æœ‰å¤±è´¥æ—¥å¿—
            if include_failed_logs:
                self._clear_all_failed_logs(include_chunks)
            
            cache_type = "document and chunk caches" if include_chunks else "document cache"
            log_type = " and failed logs" if include_failed_logs else ""
            logger.info(f"All {cache_type}{log_type} cleared")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {}
        
        if self.persistent_cache_manager:
            stats['persistent_cache'] = self.persistent_cache_manager.get_cache_stats()
            stats['chunk_cache'] = self.persistent_cache_manager.get_chunk_cache_stats()
        
        # æ·»åŠ å¤±è´¥ç»Ÿè®¡ä¿¡æ¯
        stats['failed_documents'] = self.get_failed_documents_summary()
        stats['failed_chunks'] = self.get_failed_chunks_summary()
        
        return stats
    
    def _should_extract_summary(self, text_length: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æå–æ‘˜è¦"""
        return text_length >= self.min_chunk_size_for_summary
    
    def _should_extract_qa(self, text_length: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æå–é—®ç­”å¯¹"""
        return text_length >= self.min_chunk_size_for_qa
    
    def _generate_failed_chunk_identifier(self, doc_id: str, chunk_index: int, text_content: str) -> str:
        """ç”Ÿæˆå¤±è´¥chunkçš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºåŒ¹é…å’Œåˆ é™¤å¤±è´¥è®°å½•
        
        Args:
            doc_id: æ–‡æ¡£ID
            chunk_index: chunkç´¢å¼•
            text_content: chunkæ–‡æœ¬å†…å®¹
            
        Returns:
            å”¯ä¸€æ ‡è¯†ç¬¦å­—ç¬¦ä¸²
        """
        import hashlib
        import os
        
        # ä½¿ç”¨æ–‡æ¡£IDã€chunkç´¢å¼•å’Œæ–‡æœ¬å†…å®¹çš„hashç”Ÿæˆå”¯ä¸€æ ‡è¯†
        content_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()[:8]
        safe_doc_name = os.path.basename(doc_id).replace("/", "_").replace("\\", "_")
        return f"chunk_{chunk_index}_{safe_doc_name[:30]}_{content_hash}"
    
    def _log_failed_document(self, doc_id: str, document_text: str, category: str,
                           error: Exception, traceback_info: str) -> None:
        """è®°å½•å¤±è´¥çš„æ–‡æ¡£çº§å…ƒæ•°æ®æå–åˆ°æœ¬åœ°æ—¥å¿—ç›®å½•
        
        Args:
            doc_id: æ–‡æ¡£ID
            document_text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            category: æ–‡æ¡£åˆ†ç±»
            error: é”™è¯¯ä¿¡æ¯
            traceback_info: é”™è¯¯å †æ ˆä¿¡æ¯
        """
        try:
            import json
            import os
            from datetime import datetime
            import hashlib
            
            # åˆ›å»ºå¤±è´¥æ—¥å¿—ç›®å½•
            failed_docs_dir = "logs/failed_documents"
            os.makedirs(failed_docs_dir, exist_ok=True)
            
            # ç”Ÿæˆæ—¶é—´æˆ³å’Œå”¯ä¸€æ ‡è¯†ç¬¦
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            content_hash = hashlib.md5(document_text.encode('utf-8')).hexdigest()[:8]
            safe_doc_name = os.path.basename(doc_id).replace("/", "_").replace("\\", "_")
            doc_identifier = f"doc_{safe_doc_name[:30]}_{content_hash}"
            
            # æ„å»ºå¤±è´¥è®°å½•
            failed_record = {
                "timestamp": datetime.now().isoformat(),
                "doc_id": doc_id,
                "doc_identifier": doc_identifier,
                "document_category": category,
                "text_length": len(document_text),
                "error_type": type(error).__name__,
                "error_message": str(error),
                "traceback": traceback_info,
                "text_content": document_text[:5000] + "..." if len(document_text) > 5000 else document_text,  # ä¿å­˜æ›´å¤šå†…å®¹ç”¨äºè°ƒè¯•
                "text_preview": document_text[:500] + "..." if len(document_text) > 500 else document_text
            }
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"{timestamp}_{doc_identifier}.json"
            filepath = os.path.join(failed_docs_dir, filename)
            
            # å†™å…¥å¤±è´¥è®°å½•
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(failed_record, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Failed document logged to: {filepath}")
            
        except Exception as log_error:
            logger.error(f"Failed to log failed document: {log_error}")
    
    def _log_failed_chunk(self, doc_id: str, chunk_index: int, text_content: str,
                         text_length: int, extract_config: Dict[str, Any], 
                         error: Exception, traceback_info: str) -> None:
        """è®°å½•å¤±è´¥çš„chunkåˆ°æœ¬åœ°æ—¥å¿—ç›®å½•
        
        Args:
            doc_id: æ–‡æ¡£ID
            chunk_index: chunkç´¢å¼•
            text_content: chunkæ–‡æœ¬å†…å®¹
            text_length: æ–‡æœ¬é•¿åº¦
            extract_config: æå–é…ç½®
            error: é”™è¯¯ä¿¡æ¯
            traceback_info: é”™è¯¯å †æ ˆä¿¡æ¯
        """
        try:
            import json
            import os
            from datetime import datetime
            
            # åˆ›å»ºå¤±è´¥æ—¥å¿—ç›®å½•
            failed_chunks_dir = "logs/failed_chunks"
            os.makedirs(failed_chunks_dir, exist_ok=True)
            
            # ç”Ÿæˆæ—¶é—´æˆ³å’Œå”¯ä¸€æ ‡è¯†ç¬¦
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            chunk_identifier = self._generate_failed_chunk_identifier(doc_id, chunk_index, text_content)
            
            # æ„å»ºå¤±è´¥è®°å½•
            failed_record = {
                "timestamp": datetime.now().isoformat(),
                "doc_id": doc_id,
                "chunk_index": chunk_index,
                "chunk_identifier": chunk_identifier,  # æ·»åŠ å”¯ä¸€æ ‡è¯†ç¬¦
                "text_length": text_length,
                "extract_config": extract_config,
                "error_type": type(error).__name__,
                "error_message": str(error),
                "traceback": traceback_info,
                "text_content": text_content[:2000] + "..." if len(text_content) > 2000 else text_content,  # é™åˆ¶é•¿åº¦é¿å…æ–‡ä»¶è¿‡å¤§
                "text_preview": text_content[:200] + "..." if len(text_content) > 200 else text_content  # æä¾›é¢„è§ˆ
            }
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨å”¯ä¸€æ ‡è¯†ç¬¦ï¼‰
            filename = f"{timestamp}_{chunk_identifier}.json"
            filepath = os.path.join(failed_chunks_dir, filename)
            
            # å†™å…¥å¤±è´¥è®°å½•
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(failed_record, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Failed chunk logged to: {filepath}")
            
        except Exception as log_error:
            logger.error(f"Failed to log failed chunk: {log_error}")
    
    def _remove_failed_chunk_record(self, doc_id: str, chunk_index: int, text_content: str) -> None:
        """åˆ é™¤æˆåŠŸæå–åçš„å¤±è´¥è®°å½•
        
        Args:
            doc_id: æ–‡æ¡£ID
            chunk_index: chunkç´¢å¼•
            text_content: chunkæ–‡æœ¬å†…å®¹
        """
        try:
            import os
            import json
            import glob
            
            failed_chunks_dir = "logs/failed_chunks"
            if not os.path.exists(failed_chunks_dir):
                return
            
            # ç”Ÿæˆè¦æŸ¥æ‰¾çš„chunkæ ‡è¯†ç¬¦
            chunk_identifier = self._generate_failed_chunk_identifier(doc_id, chunk_index, text_content)
            
            # æŸ¥æ‰¾åŒ¹é…çš„å¤±è´¥è®°å½•æ–‡ä»¶
            pattern = os.path.join(failed_chunks_dir, f"*{chunk_identifier}.json")
            matching_files = glob.glob(pattern)
            
            # åˆ é™¤æ‰¾åˆ°çš„å¤±è´¥è®°å½•æ–‡ä»¶
            for filepath in matching_files:
                try:
                    # éªŒè¯æ–‡ä»¶å†…å®¹ç¡®ä¿æ˜¯æ­£ç¡®çš„è®°å½•
                    with open(filepath, 'r', encoding='utf-8') as f:
                        record = json.load(f)
                        if (record.get('doc_id') == doc_id and 
                            record.get('chunk_index') == chunk_index and
                            record.get('chunk_identifier') == chunk_identifier):
                            os.remove(filepath)
                            logger.info(f"Removed failed chunk record: {filepath}")
                        else:
                            logger.warning(f"Failed chunk record mismatch, skipping: {filepath}")
                except Exception as file_error:
                    logger.error(f"Error processing failed chunk record {filepath}: {file_error}")
            
            if not matching_files:
                logger.debug(f"No failed chunk record found for {chunk_identifier}")
                
        except Exception as e:
            logger.error(f"Error removing failed chunk record: {e}")
    
    def _clear_failed_document_logs(self, doc_id: str) -> None:
        """æ¸…é™¤æŒ‡å®šæ–‡æ¡£çš„å¤±è´¥æ—¥å¿—
        
        Args:
            doc_id: æ–‡æ¡£ID
        """
        try:
            import os
            import json
            import glob
            
            failed_docs_dir = "logs/failed_documents"
            if not os.path.exists(failed_docs_dir):
                return
            
            # æŸ¥æ‰¾åŒ¹é…çš„å¤±è´¥è®°å½•æ–‡ä»¶
            pattern = os.path.join(failed_docs_dir, "*.json")
            matching_files = glob.glob(pattern)
            
            removed_count = 0
            for filepath in matching_files:
                try:
                    # éªŒè¯æ–‡ä»¶å†…å®¹ç¡®ä¿æ˜¯æ­£ç¡®çš„è®°å½•
                    with open(filepath, 'r', encoding='utf-8') as f:
                        record = json.load(f)
                        if record.get('doc_id') == doc_id:
                            os.remove(filepath)
                            removed_count += 1
                            logger.info(f"Removed failed document record: {filepath}")
                except Exception as file_error:
                    logger.error(f"Error processing failed document record {filepath}: {file_error}")
            
            if removed_count == 0:
                logger.debug(f"No failed document records found for {doc_id}")
            else:
                logger.info(f"Removed {removed_count} failed document records for {doc_id}")
                
        except Exception as e:
            logger.error(f"Error clearing failed document logs: {e}")
    
    def _clear_all_failed_logs(self, include_chunks: bool = True) -> None:
        """æ¸…é™¤æ‰€æœ‰å¤±è´¥æ—¥å¿—
        
        Args:
            include_chunks: æ˜¯å¦åŒæ—¶æ¸…é™¤chunkå¤±è´¥æ—¥å¿—
        """
        try:
            import os
            import shutil
            
            # æ¸…é™¤æ–‡æ¡£å¤±è´¥æ—¥å¿—
            failed_docs_dir = "logs/failed_documents"
            if os.path.exists(failed_docs_dir):
                shutil.rmtree(failed_docs_dir)
                logger.info(f"Cleared all failed document logs from {failed_docs_dir}")
            
            # æ¸…é™¤chunkå¤±è´¥æ—¥å¿—
            if include_chunks:
                failed_chunks_dir = "logs/failed_chunks"
                if os.path.exists(failed_chunks_dir):
                    shutil.rmtree(failed_chunks_dir)
                    logger.info(f"Cleared all failed chunk logs from {failed_chunks_dir}")
                    
        except Exception as e:
            logger.error(f"Error clearing all failed logs: {e}")
    
    def get_failed_documents_summary(self) -> Dict[str, Any]:
        """è·å–å¤±è´¥æ–‡æ¡£çš„æ±‡æ€»ä¿¡æ¯
        
        Returns:
            åŒ…å«å¤±è´¥æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            import os
            import json
            import glob
            from collections import defaultdict
            
            failed_docs_dir = "logs/failed_documents"
            if not os.path.exists(failed_docs_dir):
                return {
                    "total_failed_documents": 0,
                    "failed_by_error_type": {},
                    "failed_by_category": {},
                    "all_extraction_successful": True,
                    "message": "No failed documents directory found - all extractions successful"
                }
            
            # æŸ¥æ‰¾æ‰€æœ‰å¤±è´¥è®°å½•æ–‡ä»¶
            pattern = os.path.join(failed_docs_dir, "*.json")
            failed_files = glob.glob(pattern)
            
            if not failed_files:
                return {
                    "total_failed_documents": 0,
                    "failed_by_error_type": {},
                    "failed_by_category": {},
                    "all_extraction_successful": True,
                    "message": "All document metadata extractions successful - no failed documents found"
                }
            
            # ç»Ÿè®¡å¤±è´¥ä¿¡æ¯
            failed_by_error_type = defaultdict(int)
            failed_by_category = defaultdict(int)
            failed_documents = []
            
            for filepath in failed_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        record = json.load(f)
                        
                        error_type = record.get('error_type', 'Unknown')
                        category = record.get('document_category', 'Unknown')
                        
                        failed_by_error_type[error_type] += 1
                        failed_by_category[category] += 1
                        
                        failed_documents.append({
                            'doc_id': record.get('doc_id'),
                            'timestamp': record.get('timestamp'),
                            'error_type': error_type,
                            'category': category,
                            'text_length': record.get('text_length', 0),
                            'error_message': record.get('error_message', '')[:200] + '...' if len(record.get('error_message', '')) > 200 else record.get('error_message', ''),
                            'filepath': filepath
                        })
                        
                except Exception as file_error:
                    logger.error(f"Error reading failed document record {filepath}: {file_error}")
            
            return {
                "total_failed_documents": len(failed_documents),
                "failed_by_error_type": dict(failed_by_error_type),
                "failed_by_category": dict(failed_by_category),
                "failed_documents": failed_documents,
                "all_extraction_successful": False,
                "message": f"Found {len(failed_documents)} failed document metadata extractions"
            }
            
        except Exception as e:
            logger.error(f"Error getting failed documents summary: {e}")
            return {
                "total_failed_documents": 0,
                "failed_by_error_type": {},
                "failed_by_category": {},
                "error": str(e),
                "message": "Error occurred while retrieving failed documents summary"
            }
    
    def get_failed_chunks_summary(self) -> Dict[str, Any]:
        """è·å–å¤±è´¥chunkçš„æ±‡æ€»ä¿¡æ¯
        
        Returns:
            åŒ…å«å¤±è´¥chunkç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            import os
            import json
            import glob
            from collections import defaultdict
            
            failed_chunks_dir = "logs/failed_chunks"
            if not os.path.exists(failed_chunks_dir):
                return {
                    "total_failed_chunks": 0,
                    "failed_by_document": {},
                    "failed_by_error_type": {},
                    "all_extraction_successful": True,
                    "message": "No failed chunks directory found - all extractions successful"
                }
            
            # æŸ¥æ‰¾æ‰€æœ‰å¤±è´¥è®°å½•æ–‡ä»¶
            pattern = os.path.join(failed_chunks_dir, "*.json")
            failed_files = glob.glob(pattern)
            
            if not failed_files:
                return {
                    "total_failed_chunks": 0,
                    "failed_by_document": {},
                    "failed_by_error_type": {},
                    "all_extraction_successful": True,
                    "message": "All metadata extractions successful - no failed chunks found"
                }
            
            # ç»Ÿè®¡å¤±è´¥ä¿¡æ¯
            failed_by_document = defaultdict(list)
            failed_by_error_type = defaultdict(int)
            total_failed = 0
            
            for filepath in failed_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        record = json.load(f)
                        doc_id = record.get('doc_id', 'unknown')
                        chunk_index = record.get('chunk_index', 'unknown')
                        error_type = record.get('error_type', 'Unknown')
                        timestamp = record.get('timestamp', 'unknown')
                        
                        failed_by_document[doc_id].append({
                            'chunk_index': chunk_index,
                            'error_type': error_type,
                            'timestamp': timestamp,
                            'file_path': filepath
                        })
                        failed_by_error_type[error_type] += 1
                        total_failed += 1
                        
                except Exception as file_error:
                    logger.error(f"Error reading failed chunk record {filepath}: {file_error}")
            
            return {
                "total_failed_chunks": total_failed,
                "failed_by_document": dict(failed_by_document),
                "failed_by_error_type": dict(failed_by_error_type),
                "all_extraction_successful": total_failed == 0,
                "message": f"Found {total_failed} failed chunks" if total_failed > 0 else "All extractions successful"
            }
            
        except Exception as e:
            logger.error(f"Error getting failed chunks summary: {e}")
            return {
                "total_failed_chunks": -1,
                "failed_by_document": {},
                "failed_by_error_type": {},
                "all_extraction_successful": False,
                "message": f"Error checking failed chunks: {e}"
            }
    
    def clean_old_failed_records(self, days_old: int = 7) -> int:
        """æ¸…ç†æŒ‡å®šå¤©æ•°ä¹‹å‰çš„å¤±è´¥è®°å½•
        
        Args:
            days_old: æ¸…ç†å¤šå°‘å¤©ä¹‹å‰çš„è®°å½•ï¼Œé»˜è®¤7å¤©
            
        Returns:
            æ¸…ç†çš„æ–‡ä»¶æ•°é‡
        """
        try:
            import os
            import json
            import glob
            from datetime import datetime, timedelta
            
            failed_chunks_dir = "logs/failed_chunks"
            if not os.path.exists(failed_chunks_dir):
                return 0
            
            # è®¡ç®—æˆªæ­¢æ—¶é—´
            cutoff_time = datetime.now() - timedelta(days=days_old)
            
            # æŸ¥æ‰¾æ‰€æœ‰å¤±è´¥è®°å½•æ–‡ä»¶
            pattern = os.path.join(failed_chunks_dir, "*.json")
            failed_files = glob.glob(pattern)
            
            cleaned_count = 0
            for filepath in failed_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        record = json.load(f)
                        timestamp_str = record.get('timestamp')
                        if timestamp_str:
                            record_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                            if record_time < cutoff_time:
                                os.remove(filepath)
                                cleaned_count += 1
                                logger.info(f"Cleaned old failed record: {filepath}")
                except Exception as file_error:
                    logger.error(f"Error processing failed record {filepath}: {file_error}")
            
            logger.info(f"Cleaned {cleaned_count} old failed records (older than {days_old} days)")
            return cleaned_count
            
        except Exception as e:
            logger.error(f"Error cleaning old failed records: {e}")
            return 0
    
    class CustomChunkOutputParser(ChainableOutputParser):
        """è‡ªå®šä¹‰chunkè¾“å‡ºè§£æå™¨ï¼Œå¤„ç†LLMè¾“å‡ºçš„JSONæ ¼å¼é—®é¢˜"""
        
        def __init__(self, output_cls, verbose: bool = False):
            self.output_cls = output_cls
            self.verbose = verbose
            self.pydantic_parser = PydanticOutputParser(output_cls=output_cls)
        
        def parse(self, output: str):
            """è§£æLLMè¾“å‡ºï¼Œå¤„ç†propertiesåŒ…è£…ç­‰æ ¼å¼é—®é¢˜"""
            if self.verbose:
                logger.debug(f"Raw LLM output: {output}")
            
            try:
                # å°è¯•æ ‡å‡†çš„Pydanticè§£æ
                return self.pydantic_parser.parse(output)
            except ValidationError as e:
                logger.warning(f"Standard Pydantic validation failed: {e}")
                
                # å°è¯•å¤„ç†LLMè¾“å‡ºè¢«propertiesåŒ…è£…çš„æƒ…å†µ
                try:
                    cleaned_output = self._handle_properties_wrapper(output)
                    if cleaned_output != output:
                        logger.info("Detected and handled properties wrapper in LLM output")
                        return self.pydantic_parser.parse(cleaned_output)
                except Exception as cleanup_error:
                    logger.debug(f"Properties wrapper handling failed: {cleanup_error}")
                
                # å¦‚æœæ‰€æœ‰å¤„ç†éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºåŸå§‹ValidationError
                logger.error(f"All parsing attempts failed for output: {output[:200]}...")
                raise e
        
        def _handle_properties_wrapper(self, output: str) -> str:
            """å¤„ç†LLMè¾“å‡ºè¢«propertiesç»“æ„åŒ…è£…çš„æƒ…å†µ"""
            import json
            import re
            
            try:
                # é¦–å…ˆå°è¯•è§£æä¸ºJSON
                parsed_json = json.loads(output)
                
                # æ£€æŸ¥æ˜¯å¦æ•°æ®è¢«åŒ…è£…åœ¨propertieså­—æ®µä¸­
                if isinstance(parsed_json, dict) and "properties" in parsed_json:
                    properties_content = parsed_json["properties"]
                    if isinstance(properties_content, dict):
                        # æ£€æŸ¥propertieså†…å®¹æ˜¯å¦åƒå®é™…æ•°æ®è€Œä¸æ˜¯schema
                        if self._looks_like_data_not_schema(properties_content):
                            logger.info("Extracting data from properties wrapper")
                            return json.dumps(properties_content)
                        else:
                            logger.warning("Detected JSON schema format instead of data - LLM returned schema instead of actual values")
                            # è¿™ç§æƒ…å†µä¸‹ï¼ŒLLMè¿”å›äº†schemaè€Œä¸æ˜¯æ•°æ®ï¼Œåº”è¯¥æŠ›å‡ºé”™è¯¯
                            raise ValueError("LLM returned JSON schema instead of actual data")
                
                # å¦‚æœJSONè§£ææˆåŠŸä½†ä¸æ˜¯propertiesåŒ…è£…ï¼Œè¿”å›åŸå§‹è¾“å‡º
                return output
                
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                json_match = re.search(r'\{.*\}', output, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    try:
                        parsed_json = json.loads(json_str)
                        if isinstance(parsed_json, dict) and "properties" in parsed_json:
                            properties_content = parsed_json["properties"]
                            if isinstance(properties_content, dict) and self._looks_like_data_not_schema(properties_content):
                                logger.info("Extracting data from properties wrapper in embedded JSON")
                                return json.dumps(properties_content)
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON parsing failed for extracted content: {e}")
                        # å°è¯•é€šè¿‡æ‹¬å·åŒ¹é…æ¥æå–ç»“æ„åŒ–å­—æ®µ
                        try:
                            cleaned_json = self._extract_json_by_brackets(json_str)
                            if cleaned_json:
                                parsed_json = json.loads(cleaned_json)
                                if isinstance(parsed_json, dict):
                                    # æ£€æŸ¥æ˜¯å¦æœ‰propertiesåŒ…è£…
                                    if "properties" in parsed_json:
                                        properties_content = parsed_json["properties"]
                                        if isinstance(properties_content, dict) and self._looks_like_data_not_schema(properties_content):
                                            logger.info("Successfully extracted data from properties wrapper using bracket matching")
                                            return json.dumps(properties_content)
                                    else:
                                        # ç›´æ¥è¿”å›è§£æçš„JSON
                                        logger.info("Successfully extracted JSON using bracket matching")
                                        return json.dumps(parsed_json)
                        except (json.JSONDecodeError, Exception) as bracket_error:
                            logger.warning(f"Bracket-based JSON extraction also failed: {bracket_error}")
                
                # å¦‚æœæ— æ³•å¤„ç†ï¼Œè¿”å›åŸå§‹è¾“å‡º
                return output
        
        def _looks_like_data_not_schema(self, content: dict) -> bool:
            """åˆ¤æ–­å†…å®¹æ˜¯å¦åƒå®é™…æ•°æ®è€Œä¸æ˜¯JSON schema"""
            # JSON schemaé€šå¸¸åŒ…å«type, description, titleç­‰å­—æ®µ
            schema_indicators = {"type", "description", "title", "items", "required", "enum"}
            
            for key, value in content.items():
                if isinstance(value, dict):
                    # å¦‚æœå€¼æ˜¯å­—å…¸ä¸”åŒ…å«schemaæŒ‡ç¤ºç¬¦ï¼Œå¯èƒ½æ˜¯schema
                    if any(indicator in value for indicator in schema_indicators):
                        return False
                elif isinstance(value, str):
                    # å¦‚æœå€¼æ˜¯å­—ç¬¦ä¸²ä¸”ä¸æ˜¯schemaæè¿°ï¼Œå¯èƒ½æ˜¯å®é™…æ•°æ®
                    continue
                elif isinstance(value, (list, int, float, bool)):
                    # å¦‚æœå€¼æ˜¯åŸºæœ¬æ•°æ®ç±»å‹ï¼Œå¯èƒ½æ˜¯å®é™…æ•°æ®
                    continue
            
            return True

        def _extract_json_by_brackets(self, json_str: str) -> Optional[str]:
            """é€šè¿‡æ‹¬å·åŒ¹é…æ¥æå–å’Œæ¸…ç†JSONå­—ç¬¦ä¸²
            
            Args:
                json_str: å¯èƒ½åŒ…å«æ ¼å¼é—®é¢˜çš„JSONå­—ç¬¦ä¸²
                
            Returns:
                æ¸…ç†åçš„æœ‰æ•ˆJSONå­—ç¬¦ä¸²ï¼Œå¦‚æœæ— æ³•ä¿®å¤åˆ™è¿”å›None
            """
            try:
                # ç§»é™¤å¯èƒ½çš„å‰åç©ºç™½å’Œæ¢è¡Œç¬¦
                json_str = json_str.strip()
                
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª } æ¥ç¡®å®šJSONè¾¹ç•Œ
                start_idx = json_str.find('{')
                end_idx = json_str.rfind('}')
                
                if start_idx == -1 or end_idx == -1 or start_idx >= end_idx:
                    return None
                
                # æå–JSONéƒ¨åˆ†
                json_content = json_str[start_idx:end_idx + 1]
                
                # å°è¯•é€šè¿‡æ‹¬å·åŒ¹é…æ¥éªŒè¯å’Œä¿®å¤JSONç»“æ„
                bracket_count = 0
                quote_count = 0
                in_string = False
                escape_next = False
                valid_end = -1
                
                for i, char in enumerate(json_content):
                    if escape_next:
                        escape_next = False
                        continue
                        
                    if char == '\\' and in_string:
                        escape_next = True
                        continue
                        
                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue
                        
                    if not in_string:
                        if char == '{':
                            bracket_count += 1
                        elif char == '}':
                            bracket_count -= 1
                            if bracket_count == 0:
                                valid_end = i
                                break
                
                if valid_end > 0:
                    # æˆªå–åˆ°æœ‰æ•ˆçš„ç»“æŸä½ç½®
                    cleaned_json = json_content[:valid_end + 1]
                    
                    # å°è¯•ä¸€äº›å¸¸è§çš„ä¿®å¤
                    cleaned_json = self._fix_common_json_issues(cleaned_json)
                    
                    return cleaned_json
                    
                return None
                
            except Exception as e:
                logger.warning(f"Error in bracket-based JSON extraction: {e}")
                return None
        
        def _fix_common_json_issues(self, json_str: str) -> str:
                """ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                
                Args:
                    json_str: éœ€è¦ä¿®å¤çš„JSONå­—ç¬¦ä¸²
                    
                Returns:
                    ä¿®å¤åçš„JSONå­—ç¬¦ä¸²
                """
                try:
                    # ç§»é™¤å¯èƒ½çš„å°¾éšé€—å·
                    json_str = re.sub(r',\s*}', '}', json_str)
                    json_str = re.sub(r',\s*]', ']', json_str)
                    
                    # é€è¡Œå¤„ç†å­—ç¬¦ä¸²å€¼ä¸­çš„æœªè½¬ä¹‰å¼•å·
                    lines = json_str.split('\n')
                    fixed_lines = []
                    
                    for line in lines:
                        if ':' in line and '"' in line:
                            # ç›´æ¥å¤„ç†åŒ…å«å¼•å·çš„å­—ç¬¦ä¸²å€¼
                            parts = line.split(':', 1)
                            if len(parts) == 2:
                                key_part = parts[0]
                                value_part = parts[1].strip()
                                
                                if value_part.startswith('"'):
                                    # å¤„ç†å­—ç¬¦ä¸²å€¼
                                    if value_part.count('"') > 2:  # æœ‰æœªè½¬ä¹‰çš„å¼•å·
                                        # æå–å†…å®¹
                                        if value_part.endswith(','):
                                            content = value_part[1:-2]
                                            trailing = ','
                                        elif value_part.endswith('"'):
                                            content = value_part[1:-1]
                                            trailing = ''
                                        else:
                                            fixed_lines.append(line)
                                            continue
                                        
                                        # è½¬ä¹‰å†…éƒ¨å¼•å·
                                        escaped_content = content.replace('"', '\\"')
                                        fixed_line = f'{key_part}: "{escaped_content}"{trailing}'
                                        fixed_lines.append(fixed_line)
                                    else:
                                        fixed_lines.append(line)
                                else:
                                    fixed_lines.append(line)
                            else:
                                fixed_lines.append(line)
                        else:
                            fixed_lines.append(line)
                    
                    return '\n'.join(fixed_lines)
                    
                except Exception as e:
                    logger.warning(f"Error fixing JSON issues: {e}")
                    return json_str

    
    def _create_chunk_program(self, extract_summary: bool, extract_qa: bool, chunk_template: str) -> LLMTextCompletionProgram:
        """æ ¹æ®éœ€è¦æå–çš„å­—æ®µåˆ›å»ºchunkçº§æå–ç¨‹åº"""
        
        # åŠ¨æ€åˆ›å»ºPydanticæ¨¡å‹çš„å­—æ®µ
        annotations = {
            "title": str,
            "keywords": List[str],
        }
        
        # ä»ChunkLevelMetadataæ¨¡å‹ä¸­è·å–æ­£ç¡®çš„å­—æ®µæè¿°
        title_field = ChunkLevelMetadata.model_fields.get("title")
        keywords_field = ChunkLevelMetadata.model_fields.get("keywords")
        
        field_definitions = {
            "title": Field(..., description=title_field.description if title_field else "chunkæ ‡é¢˜ï¼Œåº”è¯¥ç®€æ´å‡†ç¡®åœ°æ¦‚æ‹¬chunkå†…å®¹"),
            "keywords": Field(..., description=keywords_field.description if keywords_field else "å…³é”®è¯åˆ—è¡¨ï¼Œæå–3-6ä¸ªæœ€é‡è¦çš„å…³é”®è¯"),
        }
        
        optional_fields_prompt = []
        optional_fields_desc = ""

        if extract_summary:
            summary_field = ChunkLevelMetadata.model_fields.get("summary")
            if summary_field:
                annotations["summary"] = Optional[str]
                field_definitions["summary"] = Field(None, description=summary_field.description)
                optional_fields_prompt.append(f"- **summary**: {summary_field.description}")

        if extract_qa:
            qa_field = ChunkLevelMetadata.model_fields.get("questions_answered")
            if qa_field:
                annotations["questions_answered"] = Optional[List[str]]
                field_definitions["questions_answered"] = Field(None, description=qa_field.description)
                optional_fields_prompt.append(f"- **questions_answered**: {qa_field.description}")

        if optional_fields_prompt:
            optional_fields_desc = "\nè¯·æå–ä»¥ä¸‹å¯é€‰ä¿¡æ¯ï¼š\n" + "\n".join(optional_fields_prompt)
        
        # åŠ¨æ€åˆ›å»ºæ¨¡å‹ç±»
        class_dict = {
            "__annotations__": annotations,
            **field_definitions
        }
        
        DynamicChunkMetadata = type("DynamicChunkMetadata", (BaseModel,), class_dict)
        
        # åˆ›å»ºç¨‹åº
        template = chunk_template.replace("{optional_fields}", optional_fields_desc)
        
        # ä½¿ç”¨æ ‡å‡†çš„PydanticOutputParseråˆ›å»ºç¨‹åº
        standard_parser = PydanticOutputParser(output_cls=DynamicChunkMetadata)
        
        return LLMTextCompletionProgram.from_defaults(
            output_parser=standard_parser,
            prompt_template_str=template,
            verbose=True,
            llm=self.llm
        )
    
    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:
        """å¼‚æ­¥æå–æ™ºèƒ½å…ƒæ•°æ®
        
        Args:
            nodes: è¦å¤„ç†çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        logger.info(f"ğŸš€ [METADATA EXTRACTOR] Starting smart metadata extraction for {len(nodes)} nodes")
        
        metadata_list = [None] * len(nodes)  # é¢„åˆ†é…åˆ—è¡¨ï¼Œä¿æŒé¡ºåº
        
        # æ§åˆ¶å¹¶å‘æ•°é‡
        import asyncio
        semaphore = asyncio.Semaphore(8)  # é™åˆ¶å¹¶å‘æ•°é‡
        processed_count = 0
        progress_lock = asyncio.Lock()
        
        async def process_node(idx, node):
            nonlocal processed_count
            
            async with semaphore:
                try:
                    doc_id = node.metadata.get("original_file_path", "unknown")
                    node_type = node.metadata.get("node_type", "chunk")
                    text_content = node.get_content()
                    text_length = len(text_content)
                    
                    # å¤„ç†åŸå§‹æ–‡æ¡£èŠ‚ç‚¹ - æå–æ–‡æ¡£çº§å…ƒæ•°æ®
                    if node_type == "original_document":
                        logger.info(f"ğŸ“‹ [METADATA EXTRACTOR] Processing original document: {doc_id} (length: {text_length})")
                        
                        # ä¼˜åŒ–å¤„ç†ç­–ç•¥ï¼šå¯¹äºè¶…å¤§æ–‡æ¡£ï¼Œä½¿ç”¨æ‘˜è¦æˆ–å…³é”®éƒ¨åˆ†è¿›è¡Œå…ƒæ•°æ®æå–
                        optimized_content = self._optimize_document_content_for_extraction(text_content, doc_id)
                        optimized_length = len(optimized_content)
                        
                        if optimized_length != text_length:
                            logger.info(f"ğŸ”§ [METADATA EXTRACTOR] Document content optimized: {text_length} -> {optimized_length} characters")
                        
                        # æå–æ–‡æ¡£çº§å…ƒæ•°æ®å¹¶ç¼“å­˜
                        cached_data = await self._classify_and_extract(optimized_content, doc_id)
                        doc_metadata = cached_data["metadata"]
                        
                        # ä¸ºåŸå§‹æ–‡æ¡£èŠ‚ç‚¹è®¾ç½®ç‰¹æ®Šçš„å…ƒæ•°æ®
                        original_doc_metadata = {
                            **doc_metadata,
                            "content_optimization_applied": optimized_length != text_length,
                            "original_content_length": text_length,
                            "optimized_content_length": optimized_length
                        }
                        metadata_list[idx] = original_doc_metadata
                        
                    # å¤„ç†åˆ‡åˆ†èŠ‚ç‚¹ - æå–chunkçº§å…ƒæ•°æ®
                    else:
                        logger.debug(f"ğŸ“„ [METADATA EXTRACTOR] Processing chunk {idx+1}: {doc_id} (length: {text_length})")
                        
                        # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºç©ºæˆ–è¿‡çŸ­
                        if not text_content or not text_content.strip():
                            logger.debug(f"Skipping chunk {idx+1}: empty content")
                            metadata_list[idx] = self._create_fallback_metadata("", doc_id)
                            return
                        
                        if text_length < self.min_chunk_size_for_extraction:
                            logger.debug(f"Skipping chunk {idx+1}: too short ({text_length} < {self.min_chunk_size_for_extraction})")
                            metadata_list[idx] = self._create_fallback_metadata("", doc_id)
                            return
                        
                        # è·å–æ–‡æ¡£çº§å…ƒæ•°æ®
                        doc_metadata = {}
                        chunk_template = self._create_default_chunk_template()
                        
                        # å°è¯•ä»æŒä¹…åŒ–ç¼“å­˜è·å–æ–‡æ¡£çº§å…ƒæ•°æ®
                        if self.persistent_cache_manager:
                            cached_data = self.persistent_cache_manager.get_cached_metadata(doc_id=doc_id)
                            if cached_data:
                                doc_metadata = cached_data.get("metadata", {})
                                chunk_template = cached_data.get("chunk_template", self._create_default_chunk_template())
                            else:
                                # å¦‚æœæ²¡æœ‰åŸå§‹æ–‡æ¡£èŠ‚ç‚¹ï¼Œéœ€è¦æå–æ–‡æ¡£çº§å…ƒæ•°æ®
                                logger.warning(f"âš ï¸ [METADATA EXTRACTOR] No original document found for {doc_id}, extracting from chunk")
                                cached_data = await self._classify_and_extract(text_content, doc_id)
                                doc_metadata = cached_data["metadata"]
                                chunk_template = cached_data["chunk_template"]
                        
                        # æ£€æŸ¥æŒä¹…åŒ–ç¼“å­˜
                        extract_summary = self._should_extract_summary(text_length)
                        extract_qa = self._should_extract_qa(text_length)
                        
                        extract_config = {
                            'extract_summary': extract_summary,
                            'extract_qa': extract_qa,
                            'max_keywords': self.max_keywords,
                            'min_chunk_size_for_summary': self.min_chunk_size_for_summary,
                            'min_chunk_size_for_qa': self.min_chunk_size_for_qa
                        }
                        
                        chunk_metadata = None
                        if self.persistent_cache_manager:
                            chunk_metadata = self.persistent_cache_manager.get_chunk_metadata_from_cache(
                                doc_id=doc_id, chunk_text=text_content, chunk_index=idx+1
                            )
                        
                        if chunk_metadata:
                            # ä»ç¼“å­˜åŠ è½½æˆåŠŸ
                            self._remove_failed_chunk_record(doc_id, idx+1, text_content)
                            logger.debug(f"ğŸ’¾ [METADATA EXTRACTOR] Chunk {idx+1} loaded from cache")
                        else:
                            # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡ŒLLMæå–
                            logger.debug(f"ğŸ¤– [METADATA EXTRACTOR] Chunk {idx+1} calling LLM for extraction")
                            chunk_program = self._create_chunk_program(extract_summary, extract_qa, chunk_template)
                            
                            try:
                                # æ·»åŠ APIè¯·æ±‚é—´éš”æ§åˆ¶
                                await asyncio.sleep(settings.llm_model_settings.API_REQUEST_INTERVAL)
                                
                                prompt = chunk_program.prompt.format(
                                    context_str=text_content,
                                    text_length=text_length,
                                    max_keywords=self.max_keywords
                                )
                                
                                raw_response = await self.llm.acomplete(prompt)
                                raw_output = raw_response.text
                                
                                custom_parser = self.CustomChunkOutputParser(
                                    output_cls=chunk_program.output_cls,
                                    verbose=True
                                )
                                result = custom_parser.parse(raw_output)
                                
                                if not result or not hasattr(result, 'dict'):
                                    raise ValueError("LLM returned empty or invalid response")
                                
                                chunk_metadata = result.dict()
                                
                                # ä¿å­˜åˆ°chunkç¼“å­˜
                                if self.persistent_cache_manager:
                                    self.persistent_cache_manager.save_chunk_metadata_to_cache(
                                        doc_id=doc_id, chunk_text=text_content, metadata=chunk_metadata, chunk_index=idx+1
                                    )
                                
                                self._remove_failed_chunk_record(doc_id, idx+1, text_content)
                                logger.debug(f"âœ… [METADATA EXTRACTOR] Chunk {idx+1} LLM extraction successful")
                                
                            except Exception as e:
                                import traceback
                                logger.error(f"âŒ [METADATA EXTRACTOR] Chunk {idx+1} LLM extraction FAILED: {e}")
                                
                                self._log_failed_chunk(
                                    doc_id=doc_id,
                                    chunk_index=idx+1,
                                    text_content=text_content,
                                    text_length=text_length,
                                    extract_config=extract_config,
                                    error=e,
                                    traceback_info=traceback.format_exc()
                                )
                                
                                chunk_metadata = {
                                    "title": "",
                                    "summary": "",
                                    "keywords": [],
                                    "qa_pairs": [] if extract_qa else [],
                                    "extraction_failed": True,
                                    "error_message": str(e)
                                }
                        
                        # ä½¿ç”¨æ™ºèƒ½åˆå¹¶é€»è¾‘åˆå¹¶æ–‡æ¡£çº§å’Œchunkçº§å…ƒæ•°æ®
                        final_metadata = self._merge_document_and_chunk_metadata(doc_metadata, chunk_metadata)
                        metadata_list[idx] = final_metadata
                    
                    # æ›´æ–°è¿›åº¦
                    async with progress_lock:
                        nonlocal processed_count
                        processed_count += 1
                        if processed_count % 10 == 0 or processed_count == len(nodes):
                            logger.info(f"ğŸ“Š [METADATA EXTRACTOR] Progress: {processed_count}/{len(nodes)} nodes processed")
                
                except Exception as e:
                    logger.error(f"Unexpected error processing node {idx+1}: {e}")
                    metadata_list[idx] = {
                        "title": f"Node {idx+1} (å¤„ç†å¼‚å¸¸)",
                        "keywords": [],
                        "summary": "",
                        "qa_pairs": [],
                        "processing_failed": True,
                        "error_message": str(e)
                    }
                    
                    async with progress_lock:
                        processed_count += 1
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰èŠ‚ç‚¹
        tasks = [process_node(idx, node) for idx, node in enumerate(nodes)]
        await asyncio.gather(*tasks)
        
        logger.info(f"ğŸ [METADATA EXTRACTOR] Smart metadata extraction completed for {len(nodes)} nodes")
        return metadata_list
    

    

    
    def extract(self, nodes: Sequence[BaseNode]) -> List[Dict]:
        """åŒæ­¥æå–æ™ºèƒ½å…ƒæ•°æ®"""
        import asyncio
        return asyncio.run(self.aextract(nodes))