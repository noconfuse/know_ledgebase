# -*- coding: utf-8 -*-
"""
chunkçº§å…ƒæ•°æ®æå–å™¨
ä»æ–‡æ¡£çº§å…ƒæ•°æ®ä¸­ç»§æ‰¿ä¿¡æ¯ï¼Œå¹¶æå–chunkç‰¹å®šçš„å…ƒæ•°æ®
æ”¯æŒåŸºäºæ–‡æ¡£åˆ†ç±»çš„åŠ¨æ€å…ƒæ•°æ®å­—æ®µè°ƒæ•´
"""

import logging
import hashlib
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, Tuple
from pydantic import Field
from llama_index.core.schema import BaseNode, TextNode
from llama_index.core.extractors import BaseExtractor
from llama_index.core.llms import LLM
from llama_index.core.program import LLMTextCompletionProgram
from llama_index.core.output_parsers import PydanticOutputParser
from models.metadata_models import ChunkLevelMetadata
from services.metadata_cache_manager import MetadataCacheManager
from services.metadata_failure_manager import MetadataFailureManager
from config import settings

logger = logging.getLogger(__name__)

class ChunkLevelMetadataExtractor(BaseExtractor):
    """chunkçº§å…ƒæ•°æ®æå–å™¨
    
    æå–chunkçº§å…ƒæ•°æ®å¹¶è‡ªåŠ¨ç»§æ‰¿æ–‡æ¡£çº§å…ƒæ•°æ®
    æ”¯æŒå¹¶å‘å¤„ç†å’Œå¤±è´¥é‡è¯•æœºåˆ¶
    """
    
    llm: LLM = Field(description="è¯­è¨€æ¨¡å‹å®ä¾‹")
    min_chunk_size_for_extraction: int = Field(default=100, description="è¿›è¡Œå…ƒæ•°æ®æå–çš„æœ€å°chunkå¤§å°")
    max_keywords: int = Field(default=6, description="æœ€å¤§å…³é”®è¯æ•°é‡")
    cache_manager: Optional[MetadataCacheManager] = Field(default=None, description="ç¼“å­˜ç®¡ç†å™¨")
    max_chunk_length: int = Field(default=2000, description="æœ€å¤§chunké•¿åº¦")
    min_chunk_length: int = Field(default=50, description="æœ€å°chunké•¿åº¦")
    failure_manager: Optional[MetadataFailureManager] = Field(default=None, description="å¤±è´¥è®°å½•ç®¡ç†å™¨")
    
    # å¹¶å‘å’Œé‡è¯•é…ç½®
    max_workers: int = Field(default=4, description="æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°")
    max_retries: int = Field(default=3, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
    retry_delay: float = Field(default=1.0, description="é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰")
    enable_concurrent: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨å¹¶å‘å¤„ç†")
    batch_size: int = Field(default=10, description="æ‰¹å¤„ç†å¤§å°")
    
    def __init__(
        self,
        llm: LLM,
        min_chunk_size_for_extraction: int = 20,
        max_keywords: int = 6,
        max_workers: int = 4,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        enable_concurrent: bool = True,
        batch_size: int = 10,
        **kwargs
    ):
        # ä»kwargsä¸­ç§»é™¤å¯èƒ½å†²çªçš„å‚æ•°
        cache_manager_from_kwargs = kwargs.pop('cache_manager', None)
        failure_manager_from_kwargs = kwargs.pop('failure_manager', None)
        
        # åˆ›å»ºcache_managerå’Œfailure_managerï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        cache_manager = cache_manager_from_kwargs or MetadataCacheManager()
        failure_manager = failure_manager_from_kwargs or MetadataFailureManager()
        max_chunk_length = getattr(settings, 'MAX_CHUNK_LENGTH_FOR_METADATA', 2000)
        min_chunk_length = getattr(settings, 'MIN_CHUNK_LENGTH_FOR_METADATA', 50)
        
        super().__init__(
            llm=llm,
            min_chunk_size_for_extraction=min_chunk_size_for_extraction,
            max_keywords=max_keywords,
            cache_manager=cache_manager,
            max_chunk_length=max_chunk_length,
            min_chunk_length=min_chunk_length,
            failure_manager=failure_manager,
            max_workers=max_workers,
            max_retries=max_retries,
            retry_delay=retry_delay,
            enable_concurrent=enable_concurrent,
            batch_size=batch_size,
            **kwargs
        )
        
    def extract(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        """æå–chunkçº§å…ƒæ•°æ®
        
        Args:
            nodes: è¾“å…¥èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            List[Dict[str, Any]]: æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
        """
        logger.info(f"ğŸš€ [CHUNK METADATA EXTRACTOR] Starting chunk-level metadata extraction for {len(nodes)} chunks")
        
        if self.enable_concurrent and len(nodes) > 1:
            return self._extract_concurrent(nodes)
        else:
            return self._extract_sequential(nodes)
    
    def _extract_sequential(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        """é¡ºåºæå–å…ƒæ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        metadata_list = []
        processed_count = 0
        
        for i, node in enumerate(nodes):
            metadata_dict = {}
            
            if isinstance(node, TextNode):
                text_content = node.get_content()
                text_length = len(text_content)
                
                # æ£€æŸ¥chunkå¤§å°
                if text_length < self.min_chunk_size_for_extraction:
                    logger.debug(f"â­ï¸ Chunk {i+1} too small ({text_length} < {self.min_chunk_size_for_extraction}), skipping")
                    metadata_list.append(metadata_dict)
                    continue
                
                # è·å–æ–‡æ¡£ID
                doc_id = node.metadata.get('original_file_path', 'unknown')
                
                # å°è¯•ä»ç¼“å­˜è·å–
                chunk_id = f"{doc_id}_chunk_{i}"
                cached_metadata = self.cache_manager.get_chunk_metadata_from_cache(doc_id, text_content, i)
                if cached_metadata:
                    logger.debug(f"ğŸ“‹ Chunk metadata cache hit for: {chunk_id}")
                    metadata_dict.update(cached_metadata)
                    
                    # åˆ é™¤ä¹‹å‰çš„å¤±è´¥è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if self.failure_manager:
                        self.failure_manager.remove_chunk_failure(chunk_id)
                    
                    metadata_list.append(metadata_dict)
                    processed_count += 1
                    continue
                
                # è·å–æ–‡æ¡£çº§å…ƒæ•°æ®
                doc_metadata = node.metadata.get('document_metadata', {})
                
                # æå–chunkçº§å…ƒæ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰
                chunk_id = f"{doc_id}_chunk_{i}"
                chunk_metadata = self._extract_chunk_metadata_with_retry(text_content, doc_metadata, chunk_id)
                
                if chunk_metadata:
                    # ç›´æ¥å±•å¼€å…ƒæ•°æ®
                    metadata_dict.update(chunk_metadata)
                    
                    # ç¼“å­˜å…ƒæ•°æ®
                    self.cache_manager.save_chunk_metadata_to_cache(doc_id, text_content, chunk_metadata, i)
                    
                    # åˆ é™¤ä¹‹å‰çš„å¤±è´¥è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if self.failure_manager:
                        self.failure_manager.remove_chunk_failure(chunk_id)
                    
                    processed_count += 1
                    
                    if processed_count % 10 == 0:
                        logger.info(f"ğŸ“Š Processed {processed_count}/{len(nodes)} chunks")
                else:
                    logger.warning(f"âš ï¸ Failed to extract chunk metadata for chunk {i+1}")
                    # è®°å½•å¤±è´¥
                    if self.failure_manager:
                        self.failure_manager.record_chunk_failure(chunk_id, text_content, "å…ƒæ•°æ®æå–å¤±è´¥")
            
            metadata_list.append(metadata_dict)
        
        logger.info(f"ğŸ‰ [CHUNK METADATA EXTRACTOR] Sequential extraction completed. Processed: {processed_count}/{len(nodes)}")
        return metadata_list
    
    def _extract_concurrent(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        """å¹¶å‘æå–å…ƒæ•°æ®"""
        logger.info(f"ğŸ”„ [CHUNK METADATA EXTRACTOR] Using concurrent extraction with {self.max_workers} workers")
        
        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨ï¼Œä¿æŒåŸæœ‰é¡ºåº
        metadata_list = [{} for _ in nodes]
        processed_count = 0
        
        # å‡†å¤‡éœ€è¦å¤„ç†çš„ä»»åŠ¡
        tasks = []
        for i, node in enumerate(nodes):
            if isinstance(node, TextNode):
                text_content = node.get_content()
                text_length = len(text_content)
                
                # æ£€æŸ¥chunkå¤§å°
                if text_length < self.min_chunk_size_for_extraction:
                    logger.debug(f"â­ï¸ Chunk {i+1} too small ({text_length} < {self.min_chunk_size_for_extraction}), skipping")
                    continue
                
                # è·å–æ–‡æ¡£ID
                doc_id = node.metadata.get('original_file_path', 'unknown')
                chunk_id = f"{doc_id}_chunk_{i}"
                
                # æ£€æŸ¥ç¼“å­˜
                cached_metadata = self.cache_manager.get_chunk_metadata_from_cache(doc_id, text_content, i)
                if cached_metadata:
                    logger.debug(f"ğŸ“‹ Chunk metadata cache hit for: {chunk_id}")
                    metadata_list[i].update(cached_metadata)
                    
                    # åˆ é™¤ä¹‹å‰çš„å¤±è´¥è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if self.failure_manager:
                        self.failure_manager.remove_chunk_failure(chunk_id)
                    
                    processed_count += 1
                    continue
                
                # è·å–æ–‡æ¡£çº§å…ƒæ•°æ®
                doc_metadata = node.metadata.get('document_metadata', {})
                
                # æ·»åŠ åˆ°ä»»åŠ¡åˆ—è¡¨
                tasks.append((i, text_content, doc_metadata, chunk_id, doc_id))
        
        # åˆ†æ‰¹å¤„ç†ä»»åŠ¡
        total_tasks = len(tasks)
        if total_tasks == 0:
            logger.info(f"ğŸ‰ [CHUNK METADATA EXTRACTOR] All chunks processed from cache. Total: {processed_count}")
            return metadata_list
        
        logger.info(f"ğŸ“‹ Processing {total_tasks} chunks concurrently in batches of {self.batch_size}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå™¨è¿›è¡Œå¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # åˆ†æ‰¹æäº¤ä»»åŠ¡
            for batch_start in range(0, total_tasks, self.batch_size):
                batch_end = min(batch_start + self.batch_size, total_tasks)
                batch_tasks = tasks[batch_start:batch_end]
                
                # æäº¤å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                future_to_task = {
                    executor.submit(self._process_single_chunk, task): task
                    for task in batch_tasks
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    i, text_content, doc_metadata, chunk_id, doc_id = task
                    
                    try:
                        chunk_metadata = future.result()
                        if chunk_metadata:
                            metadata_list[i].update(chunk_metadata)
                            
                            # ç¼“å­˜å…ƒæ•°æ®
                            self.cache_manager.save_chunk_metadata_to_cache(doc_id, text_content, chunk_metadata, i)
                            
                            # åˆ é™¤ä¹‹å‰çš„å¤±è´¥è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                            if self.failure_manager:
                                self.failure_manager.remove_chunk_failure(chunk_id)
                            
                            processed_count += 1
                        else:
                            logger.warning(f"âš ï¸ Failed to extract chunk metadata for chunk {i+1}")
                            # è®°å½•å¤±è´¥
                            if self.failure_manager:
                                self.failure_manager.record_chunk_failure(chunk_id, text_content, "å…ƒæ•°æ®æå–å¤±è´¥")
                    
                    except Exception as e:
                        logger.error(f"âŒ Error processing chunk {i+1}: {str(e)}")
                        # è®°å½•å¤±è´¥
                        if self.failure_manager:
                            self.failure_manager.record_chunk_failure(chunk_id, text_content, f"å¤„ç†å¼‚å¸¸: {str(e)}")
                
                # æ‰¹æ¬¡å®Œæˆåçš„è¿›åº¦æŠ¥å‘Š
                logger.info(f"ğŸ“Š Completed batch {batch_start//self.batch_size + 1}/{(total_tasks-1)//self.batch_size + 1}. Processed: {processed_count}/{len(nodes)} chunks")
        
        logger.info(f"ğŸ‰ [CHUNK METADATA EXTRACTOR] Concurrent extraction completed. Processed: {processed_count}/{len(nodes)}")
        return metadata_list
    
    def _process_single_chunk(self, task: Tuple[int, str, Dict[str, Any], str, str]) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªchunkçš„å…ƒæ•°æ®æå–ä»»åŠ¡"""
        i, text_content, doc_metadata, chunk_id, doc_id = task
        return self._extract_chunk_metadata_with_retry(text_content, doc_metadata, chunk_id)
    
    async def aextract(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        """å¼‚æ­¥æå–chunkçº§å…ƒæ•°æ®
        
        Args:
            nodes: è¾“å…¥èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            List[Dict[str, Any]]: æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
        """
        logger.info(f"ğŸš€ [CHUNK METADATA EXTRACTOR] Starting async chunk-level metadata extraction for {len(nodes)} chunks")
        
        if self.enable_concurrent and len(nodes) > 1:
            return await self._extract_async_concurrent(nodes)
        else:
            # å¯¹äºå•ä¸ªchunkæˆ–ç¦ç”¨å¹¶å‘æ—¶ï¼Œä½¿ç”¨åŒæ­¥æ–¹æ³•
            return self.extract(nodes)
    
    async def _extract_async_concurrent(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        """å¼‚æ­¥å¹¶å‘æå–å…ƒæ•°æ®"""
        logger.info(f"ğŸ”„ [CHUNK METADATA EXTRACTOR] Using async concurrent extraction")
        
        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨ï¼Œä¿æŒåŸæœ‰é¡ºåº
        metadata_list = [{} for _ in nodes]
        processed_count = 0
        
        # å‡†å¤‡éœ€è¦å¤„ç†çš„ä»»åŠ¡
        tasks = []
        for i, node in enumerate(nodes):
            if isinstance(node, TextNode):
                text_content = node.get_content()
                text_length = len(text_content)
                
                # æ£€æŸ¥chunkå¤§å°
                if text_length < self.min_chunk_size_for_extraction:
                    logger.debug(f"â­ï¸ Chunk {i+1} too small ({text_length} < {self.min_chunk_size_for_extraction}), skipping")
                    continue
                
                # è·å–æ–‡æ¡£ID
                doc_id = node.metadata.get('original_file_path', 'unknown')
                chunk_id = f"{doc_id}_chunk_{i}"
                
                # æ£€æŸ¥ç¼“å­˜
                cached_metadata = self.cache_manager.get_chunk_metadata_from_cache(doc_id, text_content, i)
                if cached_metadata:
                    logger.debug(f"ğŸ“‹ Chunk metadata cache hit for: {chunk_id}")
                    metadata_list[i].update(cached_metadata)
                    
                    # åˆ é™¤ä¹‹å‰çš„å¤±è´¥è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if self.failure_manager:
                        self.failure_manager.remove_chunk_failure(chunk_id)
                    
                    processed_count += 1
                    continue
                
                # è·å–æ–‡æ¡£çº§å…ƒæ•°æ®
                doc_metadata = node.metadata.get('document_metadata', {})
                
                # æ·»åŠ åˆ°ä»»åŠ¡åˆ—è¡¨
                tasks.append((i, text_content, doc_metadata, chunk_id, doc_id))
        
        # åˆ†æ‰¹å¤„ç†ä»»åŠ¡
        total_tasks = len(tasks)
        if total_tasks == 0:
            logger.info(f"ğŸ‰ [CHUNK METADATA EXTRACTOR] All chunks processed from cache. Total: {processed_count}")
            return metadata_list
        
        logger.info(f"ğŸ“‹ Processing {total_tasks} chunks asynchronously in batches of {self.batch_size}")
        
        # åˆ†æ‰¹å¼‚æ­¥å¤„ç†ä»»åŠ¡
        for batch_start in range(0, total_tasks, self.batch_size):
            batch_end = min(batch_start + self.batch_size, total_tasks)
            batch_tasks = tasks[batch_start:batch_end]
            
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
            async_tasks = [
                self._process_single_chunk_async(task)
                for task in batch_tasks
            ]
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            results = await asyncio.gather(*async_tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for task, result in zip(batch_tasks, results):
                i, text_content, doc_metadata, chunk_id, doc_id = task
                
                if isinstance(result, Exception):
                    logger.error(f"âŒ Error processing chunk {i+1}: {str(result)}")
                    # è®°å½•å¤±è´¥
                    if self.failure_manager:
                        self.failure_manager.record_chunk_failure(chunk_id, text_content, f"å¼‚æ­¥å¤„ç†å¼‚å¸¸: {str(result)}")
                elif result:
                    metadata_list[i].update(result)
                    
                    # ç¼“å­˜å…ƒæ•°æ®
                    self.cache_manager.save_chunk_metadata_to_cache(doc_id, text_content, result, i)
                    
                    # åˆ é™¤ä¹‹å‰çš„å¤±è´¥è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if self.failure_manager:
                        self.failure_manager.remove_chunk_failure(chunk_id)
                    
                    processed_count += 1
                else:
                    logger.warning(f"âš ï¸ Failed to extract chunk metadata for chunk {i+1}")
                    # è®°å½•å¤±è´¥
                    if self.failure_manager:
                        self.failure_manager.record_chunk_failure(chunk_id, text_content, "å¼‚æ­¥å…ƒæ•°æ®æå–å¤±è´¥")
            
            # æ‰¹æ¬¡å®Œæˆåçš„è¿›åº¦æŠ¥å‘Š
            logger.info(f"ğŸ“Š Completed async batch {batch_start//self.batch_size + 1}/{(total_tasks-1)//self.batch_size + 1}. Processed: {processed_count}/{len(nodes)} chunks")
        
        logger.info(f"ğŸ‰ [CHUNK METADATA EXTRACTOR] Async concurrent extraction completed. Processed: {processed_count}/{len(nodes)}")
        return metadata_list
    
    async def _process_single_chunk_async(self, task: Tuple[int, str, Dict[str, Any], str, str]) -> Optional[Dict[str, Any]]:
        """å¼‚æ­¥å¤„ç†å•ä¸ªchunkçš„å…ƒæ•°æ®æå–ä»»åŠ¡"""
        i, text_content, doc_metadata, chunk_id, doc_id = task
        # åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡ŒåŒæ­¥çš„é‡è¯•é€»è¾‘
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._extract_chunk_metadata_with_retry, text_content, doc_metadata, chunk_id)
    
    def _generate_chunk_hash(self, text: str) -> str:
        """ç”Ÿæˆchunkçš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _extract_chunk_metadata(self, text: str, doc_metadata: Dict[str, Any] = None) -> Optional[Dict[str, Any]]:
        """æå–chunkçº§å…ƒæ•°æ®"""
        try:
            # ä¼˜åŒ–chunkå†…å®¹é•¿åº¦
            optimized_text = self._optimize_chunk_content(text)
            
            # åˆ›å»ºæå–ç¨‹åº - ä½¿ç”¨CustomOutputParserå¤„ç†LLMè¾“å‡º
            from .custom_output_parser import CustomOutputParser
            custom_parser = CustomOutputParser(output_cls=ChunkLevelMetadata, verbose=False)
            extraction_program = LLMTextCompletionProgram.from_defaults(
                output_parser=custom_parser,
                output_cls=ChunkLevelMetadata,
                llm=self.llm,
                prompt_template_str=self._create_chunk_template(),
                verbose=False
            )
            
            # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
            doc_context = ""
            if doc_metadata:
                doc_context = f"""
æ–‡æ¡£èƒŒæ™¯ä¿¡æ¯ï¼š
- æ–‡æ¡£æ ‡é¢˜ï¼š{doc_metadata.get('title', 'æœªçŸ¥')}
- æ–‡æ¡£ç±»å‹ï¼š{doc_metadata.get('document_type', 'æœªçŸ¥')}
- ä¸»è¦è¯é¢˜ï¼š{', '.join(doc_metadata.get('main_topics', []))}
- å…³é”®å®ä½“ï¼š{', '.join(doc_metadata.get('key_entities', []))}
"""
            
            # æ‰§è¡Œæå–
            result = extraction_program(
                context_str=optimized_text,
                text_length=len(optimized_text),
                doc_context=doc_context,
                max_keywords=self.max_keywords
            )
            
            # è½¬æ¢ä¸ºå­—å…¸
            if hasattr(result, 'dict'):
                return result.dict()
            elif hasattr(result, 'model_dump'):
                return result.model_dump()
            else:
                return dict(result)
                
        except Exception as e:
            logger.error(f"âŒ Error extracting chunk metadata: {str(e)}")
            # è®°å½•è¯¦ç»†çš„å¤±è´¥åŸå› å·²åœ¨ä¸Šå±‚å¤„ç†
            # è¿”å›é»˜è®¤å…ƒæ•°æ®ï¼ŒåŠ¨æ€ç”Ÿæˆå­—æ®µ
            return self._create_default_chunk_metadata()
    
    def _extract_chunk_metadata_with_retry(self, text: str, doc_metadata: Dict[str, Any], chunk_id: str = None) -> Optional[Dict[str, Any]]:
        """å¸¦é‡è¯•é€»è¾‘çš„chunkå…ƒæ•°æ®æå–
        
        Args:
            text: chunkæ–‡æœ¬å†…å®¹
            doc_metadata: æ–‡æ¡£çº§å…ƒæ•°æ®
            chunk_id: chunkæ ‡è¯†ç¬¦ï¼ˆç”¨äºå¤±è´¥è®°å½•ï¼‰
            
        Returns:
            Optional[Dict[str, Any]]: æå–çš„å…ƒæ•°æ®å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›é»˜è®¤å€¼
        """
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                if attempt > 0:
                    logger.info(f"ğŸ”„ Retrying chunk metadata extraction (attempt {attempt + 1}/{self.max_retries + 1}) for chunk: {chunk_id or 'unknown'}")
                    time.sleep(self.retry_delay)
                
                # å°è¯•æå–å…ƒæ•°æ®
                result = self._extract_chunk_metadata(text, doc_metadata)
                
                if result:
                    if attempt > 0:
                        logger.info(f"âœ… Chunk metadata extraction succeeded on retry {attempt + 1} for chunk: {chunk_id or 'unknown'}")
                    return result
                else:
                    last_error = "LLM returned empty result"
                    
            except Exception as e:
                last_error = str(e)
                logger.warning(f"âš ï¸ Attempt {attempt + 1} failed for chunk {chunk_id or 'unknown'}: {last_error}")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œä¸å†é‡è¯•
                if attempt == self.max_retries:
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        logger.error(f"âŒ All {self.max_retries + 1} attempts failed for chunk metadata extraction. Chunk: {chunk_id or 'unknown'}. Last error: {last_error}")
        
        # è®°å½•å¤±è´¥ï¼ˆå¦‚æœæœ‰å¤±è´¥ç®¡ç†å™¨ï¼‰
        if self.failure_manager and chunk_id:
            self.failure_manager.record_chunk_failure(chunk_id, text, f"é‡è¯•{self.max_retries + 1}æ¬¡åä»å¤±è´¥: {last_error}")
        
        # è¿”å›é»˜è®¤å…ƒæ•°æ®
        return self._create_default_chunk_metadata()
    
    def _create_default_chunk_metadata(self) -> Dict[str, Any]:
        """åŠ¨æ€åˆ›å»ºé»˜è®¤chunkçº§å…ƒæ•°æ®"""
        default_metadata = {}
        
        for field_name, model_field in ChunkLevelMetadata.model_fields.items():
            # æ ¹æ®å­—æ®µç±»å‹è®¾ç½®é»˜è®¤å€¼
            if model_field.annotation == str:
                if 'score' in field_name or 'level' in field_name:
                    default_metadata[field_name] = "ä¸­ç­‰"
                elif 'tone' in field_name:
                    default_metadata[field_name] = "ä¸­æ€§"
                elif 'type' in field_name:
                    default_metadata[field_name] = "æœªçŸ¥ç±»å‹"
                elif 'topic' in field_name:
                    default_metadata[field_name] = "æœªçŸ¥ä¸»é¢˜"
                else:
                    default_metadata[field_name] = "æœªçŸ¥"
            elif hasattr(model_field.annotation, '__origin__') and model_field.annotation.__origin__ == list:
                default_metadata[field_name] = []
            elif model_field.annotation == float:
                default_metadata[field_name] = 0.5
            elif model_field.annotation == int:
                default_metadata[field_name] = 0
            else:
                default_metadata[field_name] = None
        
        return default_metadata
    
    def _optimize_chunk_content(self, text: str) -> str:
        """ä¼˜åŒ–chunkå†…å®¹é•¿åº¦"""
        if len(text) <= self.max_chunk_length:
            return text
        
        # å¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œæˆªå–å‰éƒ¨åˆ†å†…å®¹
        truncated = text[:self.max_chunk_length]
        # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
        last_period = truncated.rfind('ã€‚')
        if last_period > self.min_chunk_length:
            return truncated[:last_period + 1]
        
        return truncated + "..."
    
    def _create_chunk_template(self) -> str:
        """åŠ¨æ€åˆ›å»ºchunkçº§å…ƒæ•°æ®æå–æ¨¡æ¿"""
        # ä»ChunkLevelMetadataæ¨¡å‹ä¸­åŠ¨æ€ç”Ÿæˆå­—æ®µæè¿°
        fields_desc = []
        for field_name, model_field in ChunkLevelMetadata.model_fields.items():
            fields_desc.append(f"- **{field_name}**: {model_field.description}")
        
        return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µå†…å®¹ï¼Œç»“åˆæ–‡æ¡£èƒŒæ™¯ä¿¡æ¯ï¼Œæå–chunkçº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

{{doc_context}}

æ–‡æœ¬ç‰‡æ®µå†…å®¹:
----------------
{{context_str}}
----------------

æ–‡æœ¬é•¿åº¦: {{text_length}} å­—ç¬¦
æœ€å¤§å…³é”®è¯æ•°é‡: {{max_keywords}}

è¯·ä»”ç»†åˆ†æè¿™ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

è¯·ä»¥JSONæ ¼å¼è¿”å›æå–çš„å…ƒæ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½åŒ…å«åœ¨å†…ï¼š

{{
  "{list(ChunkLevelMetadata.model_fields.keys())[0]}": "...",
  "{list(ChunkLevelMetadata.model_fields.keys())[1]}": [...],
  "{list(ChunkLevelMetadata.model_fields.keys())[2]}": [...],
  "{list(ChunkLevelMetadata.model_fields.keys())[3]}": [...],
  "{list(ChunkLevelMetadata.model_fields.keys())[4]}": "...",
  "{list(ChunkLevelMetadata.model_fields.keys())[5]}": 0.0,
  "{list(ChunkLevelMetadata.model_fields.keys())[6]}": 0.0,
  "{list(ChunkLevelMetadata.model_fields.keys())[7]}": 0.0,
  "{list(ChunkLevelMetadata.model_fields.keys())[8]}": "...",
  "{list(ChunkLevelMetadata.model_fields.keys())[9]}": "..."
}}

æ³¨æ„äº‹é¡¹ï¼š
- è¯·ç»“åˆæ–‡æ¡£èƒŒæ™¯ä¿¡æ¯æ¥ç†è§£è¿™ä¸ªchunkåœ¨æ•´ä¸ªæ–‡æ¡£ä¸­çš„ä½œç”¨
- å…³é”®è¯åº”è¯¥æ˜¯è¿™ä¸ªchunkä¸­æœ€é‡è¦çš„æ¦‚å¿µï¼Œä¸è¦é‡å¤æ–‡æ¡£çº§çš„é€šç”¨å…³é”®è¯
- å¿…é¡»è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
- æ•°å€¼ç±»å‹å­—æ®µè¯·è¿”å›0.0-1.0ä¹‹é—´çš„æµ®ç‚¹æ•°
- åˆ—è¡¨ç±»å‹å­—æ®µè¯·è¿”å›å­—ç¬¦ä¸²æ•°ç»„
"""