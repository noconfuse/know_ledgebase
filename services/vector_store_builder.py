import asyncio
import os
from tabnanny import verbose
import uuid
import json
import time
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

from llama_index.core import Document, VectorStoreIndex, StorageContext
from llama_index.core.node_parser import SentenceSplitter, HTMLNodeParser
from common.custom_markdown_node_parser import (
    CustomMarkdownNodeParser as MarkdownNodeParser,
)
from llama_index.node_parser.docling import DoclingNodeParser
from llama_index.core.extractors import (
    TitleExtractor,
    KeywordExtractor,
    SummaryExtractor,
    QuestionsAnsweredExtractor,
)
from llama_index.core.ingestion import IngestionPipeline
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.vector_stores.postgres import PGVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.llms.openai_like import OpenAILike
import faiss

from config import settings
from models.database import get_db, SessionLocal
from services.document_docling_processor import DocumentDoclingProcessor
from services.document_html_processor import DocumentHTMLProcessor
from services.document_parser import document_parser, TaskStatus
from common.postgres_vector_store import create_postgres_vector_store_builder
from dao.task_dao import TaskDAO
from models.task_models import VectorStoreTask
from services.model_client_factory import ModelClientFactory

logger = logging.getLogger(__name__)


class VectorStoreBuilder:
    """å‘é‡æ•°æ®åº“æ„å»ºå™¨"""

    _instance = None
    _initialized = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._setup_models()
            self.tasks: Dict[str, VectorStoreTask] = {}
            self.executor = ThreadPoolExecutor(max_workers=2)  # é™åˆ¶å¹¶å‘æ•°

            # åˆå§‹åŒ–ä»»åŠ¡DAO
            self.task_dao = TaskDAO()

            self._initialized = True
            logger.info("VectorStoreBuilder initialized")

    def _setup_models(self):
        """è®¾ç½®æ¨¡å‹"""
        try:
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embed_model = ModelClientFactory.create_embedding_client(
                settings.embedding_model_settings
            )

            self.llm = ModelClientFactory.create_llm_client(settings.llm_model_settings)

        except Exception as e:
            logger.error(f"Failed to initialize models: {e}")
            raise

    async def build_vector_store(
        self, task_id: str, config: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ„å»ºå‘é‡æ•°æ®åº“

        Args:
            task_id: è§£æä»»åŠ¡ID
            config: æ„å»ºé…ç½®

        Returns:
            å‘é‡å­˜å‚¨ä»»åŠ¡ID
        """
        return self.create_vector_store_task_from_parse_task(task_id, config)

    async def _execute_build_task(self, task: VectorStoreTask):
        """æ‰§è¡Œæ„å»ºä»»åŠ¡"""
        try:
            task.status = TaskStatus.RUNNING
            task.started_at = datetime.utcnow()
            task.progress = 5

            # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
            self._update_task_in_db(task)

            logger.info(f"Starting vector store build task {task.task_id}")

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ„å»º
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor, self._build_vector_store_sync, task
            )

            task.result = result
            task.status = TaskStatus.COMPLETED
            task.progress = 100
            task.completed_at = datetime.utcnow()

            # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
            self._update_task_in_db(task)

            logger.info(
                f"Vector store build task {task.task_id} completed successfully"
            )

        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error = str(e)
            task.completed_at = datetime.utcnow()

            # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
            self._update_task_in_db(task)

            logger.error(f"Vector store build task {task.task_id} failed: {e}")

    def _build_vector_store_sync(self, task: VectorStoreTask) -> Dict[str, Any]:
        """åŒæ­¥æ„å»ºå‘é‡æ•°æ®åº“"""
        try:
            # 1. æ”¶é›†æ–‡æ¡£æ–‡ä»¶
            task.progress = 10
            documents = self._collect_documents(task)
            task.total_files = len(documents)

            if not documents:
                raise ValueError("No valid documents found in directory")

            # 2. æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„å¤„ç†æ–‡æ¡£
            task.progress = 20
            all_nodes = []

            # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„
            docs_by_type = {}
            for doc in documents:
                logger.info(doc.metadata)
                file_type = doc.metadata.get("file_type", ".txt")
                if file_type not in docs_by_type:
                    docs_by_type[file_type] = []
                docs_by_type[file_type].append(doc)

            # 3. è®¾ç½®æå–å™¨
            task.progress = 30
            extractors = self._setup_extractors(task.config)

            # 4. ä¸ºæ¯ç§æ–‡ä»¶ç±»å‹åˆ›å»ºä¸“ç”¨å¤„ç†ç®¡é“
            task.progress = 40
            progress_step = 20 / len(docs_by_type)  # åœ¨40-60%ä¹‹é—´åˆ†é…è¿›åº¦

            for file_type, type_docs in docs_by_type.items():
                logger.info(f"Processing {len(type_docs)} {file_type} files")

                # è·å–é€‚åˆè¯¥æ–‡ä»¶ç±»å‹çš„èŠ‚ç‚¹è§£æå™¨
                logger.info(f"Getting node parser for file type: {file_type}")
                node_parsers = self._get_node_parser_for_file_type(
                    file_type, task.config
                )

                # ç¡®ä¿node_parsersæ˜¯åˆ—è¡¨æ ¼å¼
                if not isinstance(node_parsers, list):
                    node_parsers = [node_parsers]

                logger.info(
                    f"Node parsers configured: {[type(parser).__name__ for parser in node_parsers]}"
                )
                logger.info(
                    f"Extractors configured: {[type(extractor).__name__ for extractor in extractors]}"
                )
                logger.info(f"Embed model: {type(self.embed_model).__name__}")

                # åˆ›å»ºè¯¥æ–‡ä»¶ç±»å‹çš„å¤„ç†ç®¡é“
                logger.info(f"Creating ingestion pipeline for {file_type}")
                pipeline = IngestionPipeline(
                    transformations=[*node_parsers, *extractors, self.embed_model]
                )

                # å¤„ç†è¯¥ç±»å‹çš„æ–‡æ¡£
                import time

                start_time = time.time()
                logger.info(
                    f"Starting pipeline processing for {len(type_docs)} {file_type} documents"
                )

                # è®°å½•æ¯ä¸ªæ–‡æ¡£çš„å¤„ç†è¯¦æƒ…
                for i, doc in enumerate(type_docs):
                    logger.info(
                        f"Document {i+1}/{len(type_docs)}: {doc.metadata.get('file_name', 'unknown')} (size: {len(doc.text)} chars)"
                    )

                try:
                    # åˆ›å»ºè‡ªå®šä¹‰çš„ç®¡é“æ¥é€æ­¥å¤„ç†å¹¶è®°å½•æ¯ä¸ªé˜¶æ®µ
                    logger.info("Starting transformation pipeline execution...")

                    # é€ä¸ªæ‰§è¡Œtransformationæ­¥éª¤å¹¶è®°å½•æ—¶é—´
                    current_docs = type_docs
                    for step_idx, transformation in enumerate(pipeline.transformations):
                        step_start = time.time()
                        transformation_name = type(transformation).__name__
                        logger.info(f"\n{'='*60}")
                        logger.info(
                            f"Step {step_idx+1}/{len(pipeline.transformations)}: Starting {transformation_name}"
                        )
                        logger.info(
                            f"Input: {len(current_docs) if hasattr(current_docs, '__len__') else 'N/A'} items"
                        )

                        # å¦‚æœæ˜¯LLMç›¸å…³çš„extractorï¼Œè®°å½•æ›´è¯¦ç»†çš„ä¿¡æ¯
                        if "Extractor" in transformation_name:
                            logger.info(
                                f"ğŸ¤– LLM Extractor detected: {transformation_name}"
                            )
                            logger.info(
                                f"ğŸ“ This step will make LLM API calls - detailed logs will follow"
                            )
                            if hasattr(transformation, "llm"):
                                logger.info(
                                    f"ğŸ”§ LLM model: {type(transformation.llm).__name__}"
                                )

                        try:
                            if hasattr(transformation, "transform"):
                                # å¯¹äºnode parserå’Œå…¶ä»–transformer
                                if step_idx == 0:  # ç¬¬ä¸€æ­¥ï¼Œè¾“å…¥æ˜¯documents
                                    current_docs = transformation.transform(
                                        current_docs
                                    )
                                else:  # åç»­æ­¥éª¤ï¼Œè¾“å…¥æ˜¯nodes
                                    current_docs = transformation.transform(
                                        current_docs
                                    )
                            elif hasattr(transformation, "__call__"):
                                # å¯¹äºembedding modelç­‰
                                current_docs = transformation(current_docs)

                            step_time = time.time() - step_start
                            logger.info(
                                f"âœ… Step {step_idx+1} ({transformation_name}) completed successfully in {step_time:.2f}s"
                            )
                            logger.info(
                                f"ğŸ“Š Output: {len(current_docs) if hasattr(current_docs, '__len__') else 'N/A'} items"
                            )

                            # å¦‚æœæ˜¯LLMç›¸å…³çš„extractorï¼Œè®°å½•æˆåŠŸä¿¡æ¯
                            if "Extractor" in transformation_name:
                                logger.info(
                                    f"ğŸ‰ LLM Extractor {transformation_name} completed successfully"
                                )
                                logger.info(
                                    f"â±ï¸  Total LLM processing time: {step_time:.2f}s"
                                )

                        except Exception as e:
                            step_time = time.time() - step_start
                            logger.error(
                                f"âŒ Step {step_idx+1} ({transformation_name}) FAILED after {step_time:.2f}s"
                            )
                            logger.error(f"ğŸš¨ Error in {transformation_name}: {str(e)}")
                            if "Extractor" in transformation_name:
                                logger.error(
                                    f"ğŸ’¥ LLM Extractor {transformation_name} failed - this is likely the timeout source!"
                                )
                                logger.error(
                                    f"ğŸ” Check the LLM API calls above for timeout or connection issues"
                                )
                            raise

                        logger.info(f"{'='*60}\n")

                    type_nodes = current_docs
                    processing_time = time.time() - start_time
                    logger.info(
                        f"Pipeline processing completed for {file_type} in {processing_time:.2f}s, generated {len(type_nodes)} nodes"
                    )

                    all_nodes.extend(type_nodes)
                    logger.info(f"Total nodes accumulated: {len(all_nodes)}")

                except Exception as e:
                    processing_time = time.time() - start_time
                    logger.error(
                        f"Pipeline processing failed for {file_type} after {processing_time:.2f}s: {e}"
                    )
                    logger.error(f"Error details: {str(e)}")
                    logger.error(
                        f"Error occurred during pipeline execution, last successful step info available in logs above"
                    )
                    raise

                task.progress += progress_step
                logger.info(f"Progress updated to {task.progress:.1f}%")

            # 5. åˆå¹¶æ‰€æœ‰èŠ‚ç‚¹
            task.progress = 60
            nodes = all_nodes
            logger.info(f"Merging nodes completed, total nodes: {len(nodes)}")

            # 5.5 ä¸ºèŠ‚ç‚¹æ·»åŠ é¡µç ä¿¡æ¯
            nodes = self._map_page_info_to_nodes(nodes)
            logger.info(f"Page information mapped to nodes, total nodes: {len(nodes)}")

            # 5.6 è¾“å‡ºæ‰€æœ‰èŠ‚ç‚¹æ–‡æœ¬åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            self._save_nodes_text_to_file(nodes, task.task_id)
            logger.info(f"Nodes text saved to local file for debugging")

            # 6. åˆ›å»ºå‘é‡å­˜å‚¨
            task.progress = 80
            logger.info(f"Starting vector store creation with {len(nodes)} nodes")
            start_time = time.time()

            try:
                # æå–æ–‡ä»¶ä¿¡æ¯ç”¨äºå‘é‡å­˜å‚¨åˆ›å»º
                file_info = self._extract_file_info_from_task(task, documents)
                vector_store, index, actual_index_id = self._create_vector_store(
                    nodes, task.task_id, file_info
                )
                creation_time = time.time() - start_time
                logger.info(
                    f"Vector store creation completed in {creation_time:.2f}s, using index_id: {actual_index_id}"
                )
            except Exception as e:
                creation_time = time.time() - start_time
                logger.error(
                    f"Vector store creation failed after {creation_time:.2f}s: {e}"
                )
                raise

            # 7. ä¿å­˜ç´¢å¼•
            task.progress = 95
            logger.info(
                f"Starting index save for task {task.task_id}, using index_id: {actual_index_id}"
            )
            start_time = time.time()

            try:
                index_path = self._save_index(index, actual_index_id)
                save_time = time.time() - start_time
                logger.info(
                    f"Index saved successfully in {save_time:.2f}s to: {index_path}"
                )
            except Exception as e:
                save_time = time.time() - start_time
                logger.error(f"Index save failed after {save_time:.2f}s: {e}")
                raise

            # 8. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            logger.info("Generating statistics")
            start_time = time.time()
            stats = self._generate_stats(documents, nodes, task)
            stats_time = time.time() - start_time
            logger.info(f"Statistics generated in {stats_time:.2f}s")

            # 9. ä¿å­˜ç´¢å¼•ä¿¡æ¯åˆ°æ•°æ®åº“
            logger.info("Saving index information to database")
            start_time = time.time()
            try:
                # è‡ªåŠ¨ç”Ÿæˆç´¢å¼•æè¿°
                logger.info("Generating automatic description for index")
                index_description = self._generate_auto_description(index, task.config)
                if not index_description:
                    logger.warning("Failed to generate automatic description")
                    index_description = "Auto-generated vector store index"
                else:
                    logger.info("Automatic description generated successfully")

                # æ”¶é›†æ–‡ä»¶ä¿¡æ¯ç”¨äºä¿å­˜åˆ°æ•°æ®åº“
                file_info = self._extract_file_info_from_task(task, documents)
                self._save_index_info_to_db(
                    actual_index_id,
                    index_description,
                    file_info=file_info,
                    document_count=len(documents),
                    node_count=len(nodes),
                    vector_dimension=settings.VECTOR_DIM,
                    processing_config=task.config,
                )
                db_time = time.time() - start_time
                logger.info(f"Index information saved to database in {db_time:.2f}s")
            except Exception as e:
                db_time = time.time() - start_time
                logger.error(
                    f"Failed to save index information to database after {db_time:.2f}s: {e}"
                )
                # ç»§ç»­æ‰§è¡Œï¼Œä¸å½±å“ç´¢å¼•åˆ›å»ºçš„ä¸»æµç¨‹

            result = {
                "index_id": actual_index_id,
                "index_path": index_path,
                "document_count": len(documents),
                "node_count": len(nodes),
                "vector_dimension": settings.VECTOR_DIM,
                "stats": stats,
                "config": task.config,
            }

            return result

        except Exception as e:
            import traceback
            traceback.print_exc()
            logger.error(f"Error building vector store: {e}")
            raise

    def _collect_documents(self, task: VectorStoreTask) -> List[Document]:
        """
        æ”¶é›†ç›®å½•ä¸­çš„æ–‡æ¡£
        """
        # ä½¿ç”¨Sessionä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿åœ¨è®¿é—®å…³è”å¯¹è±¡æ—¶Sessionä»ç„¶æ´»è·ƒ
        with next(get_db()) as db:
            from dao.task_dao import TaskDAO
            task_dao = TaskDAO(db)
            parse_task = task_dao.get_parse_task(task.parse_task_id)
            
            if not parse_task:
                raise ValueError(f"Parse task not found: {task.parse_task_id}")
            
            documents = []
            
            # åˆ¤æ–­ä»»åŠ¡ç±»å‹ - åœ¨Sessionæ´»è·ƒæ—¶è®¿é—®subtasks
            if parse_task.subtasks:
                # ä¸»ä»»åŠ¡ï¼šç›´æ¥éå†å­ä»»åŠ¡ï¼Œæ ¹æ®æ¯ä¸ªå­ä»»åŠ¡çš„ç±»å‹é€‰æ‹©å¤„ç†å™¨
                logger.info(f"Main task {task.parse_task_id} detected, processing {len(parse_task.subtasks)} subtasks")
                for subtask in parse_task.subtasks:
                    if subtask.file_extension in ['.html', '.htm']:
                        logger.info(f"Processing HTML subtask {subtask.task_id} with HTMLProcessor")
                        subtask_docs = DocumentHTMLProcessor().collect_document(subtask)
                    else:
                        logger.info(f"Processing non-HTML subtask {subtask.task_id} with DoclingProcessor")
                        subtask_docs = DocumentDoclingProcessor().collect_document(subtask)
                    documents.extend(subtask_docs)
            else:
                # å­ä»»åŠ¡ï¼šæ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©å¤„ç†å™¨
                if parse_task.file_extension in ['.html', '.htm']:
                    logger.info(f"Subtask {task.parse_task_id} is HTML file, using HTMLProcessor")
                    documents = DocumentHTMLProcessor().collect_document(parse_task)
                else:
                    logger.info(f"Subtask {task.parse_task_id} is non-HTML file, using DoclingProcessor")
                    documents = DocumentDoclingProcessor().collect_document(parse_task)
            
            logger.info(f"Collected {len(documents)} documents from task {task.parse_task_id}")
            return documents


    def _get_node_parser_for_file_type(self, file_type: str, config: Dict[str, Any]):
        """æ ¹æ®æ–‡ä»¶ç±»å‹è·å–åˆé€‚çš„èŠ‚ç‚¹è§£æå™¨"""
        chunk_size = config.get("chunk_size", settings.CHUNK_SIZE)
        chunk_overlap = config.get("chunk_overlap", settings.CHUNK_OVERLAP)

        if file_type == ".md":
            # Markdownæ–‡ä»¶ä½¿ç”¨MarkdownNodeParserï¼Œç„¶åé…åˆSentenceSplitter
            # MarkdownNodeParserä¸»è¦ç”¨äºè§£æmarkdownç»“æ„ï¼Œç„¶åç”¨SentenceSplitterè¿›è¡Œåˆ†å—
            return [
                MarkdownNodeParser.from_defaults(),
                SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),
            ]
        elif file_type in [".html", ".htm"]:
            # HTMLæ–‡ä»¶ä½¿ç”¨HTMLNodeParserï¼Œç„¶åé…åˆSentenceSplitter
            return [
                HTMLNodeParser.from_defaults(),
                SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),
            ]
        elif file_type == ".json":
            # JSONæ–‡ä»¶ç›´æ¥ä½¿ç”¨DoclingNodeParser
            return [DoclingNodeParser()]
        else:
            # æ–‡æœ¬æ–‡ä»¶ç›´æ¥ä½¿ç”¨SentenceSplitter
            return SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

    def _setup_extractors(self, config: Dict[str, Any]) -> List[Any]:
        """è®¾ç½®æå–å™¨"""
        extractors = []

        # ä½¿ç”¨æ–°çš„æ™ºèƒ½å…ƒæ•°æ®æå–å™¨
        logger.info(
            "Using SmartMetadataExtractor with intelligent chunk-level processing"
        )

        from .smart_metadata_extractor import SmartMetadataExtractor

        # åˆ›å»ºå¸¦æœ‰è¯¦ç»†æ—¥å¿—è®°å½•çš„LLMåŒ…è£…å™¨
        def create_logging_llm(llm, extractor_name):
            """
            åˆ›å»ºå¸¦æœ‰è¯¦ç»†æ—¥å¿—è®°å½•çš„LLMåŒ…è£…å™¨ï¼Œä½¿ç”¨LlamaIndexçš„CallbackManager
            """
            from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler
            import llama_index.core

            # è®¾ç½®å…¨å±€è°ƒè¯•å¤„ç†å™¨ä»¥è·å–è¯¦ç»†çš„LLMè°ƒç”¨ä¿¡æ¯
            llama_index.core.set_global_handler("simple")

            # åˆ›å»ºè‡ªå®šä¹‰çš„è°ƒè¯•å¤„ç†å™¨
            debug_handler = LlamaDebugHandler(print_trace_on_end=True)
            callback_manager = CallbackManager([debug_handler])

            # ä¸ºLLMè®¾ç½®callback manager
            if hasattr(llm, "callback_manager"):
                llm.callback_manager = callback_manager

            logger.info(
                f"[{extractor_name}] LLM logging configured for: {type(llm).__name__} with detailed request/response tracking"
            )
            return llm

        smart_llm = create_logging_llm(self.llm, "SmartMetadataExtractor")

        # ç¡®å®šæå–æ¨¡å¼
        extract_mode = config.get("extract_mode", "enhanced")

        # æ·»åŠ æ™ºèƒ½å…ƒæ•°æ®æå–å™¨
        extractors.append(
            SmartMetadataExtractor(
                llm=smart_llm,
                min_chunk_size_for_summary=config.get(
                    "min_chunk_size_for_summary", 500
                ),
                min_chunk_size_for_qa=config.get("min_chunk_size_for_qa", 300),
                max_keywords=config.get("max_keywords", 5),
                num_questions=config.get("num_questions", 3),
                show_progress=True,
                extract_mode=extract_mode,
            )
        )

        logger.info(
            f"SmartMetadataExtractor configured successfully with mode={extract_mode}"
        )
        logger.info(
            f"Configuration: min_summary_size={config.get('min_chunk_size_for_summary', 500)}, min_qa_size={config.get('min_chunk_size_for_qa', 300)}"
        )

        return extractors

    def _determine_vector_store_strategy(
        self, current_index_id: str, file_info: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, bool]:
        """ç¡®å®šå‘é‡å­˜å‚¨ç­–ç•¥ï¼šè¿”å›ç›®æ ‡ç´¢å¼•IDå’Œæ˜¯å¦éœ€è¦æ›´æ–°

        Returns:
            Tuple[str, bool]: (target_index_id, should_update)
        """
        if file_info and file_info.get("file_md5"):
            # æ ¹æ®æ–‡ä»¶MD5æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶çš„ç´¢å¼•
            try:
                # ä½¿ç”¨å·²å¯¼å…¥çš„SessionLocal
                from dao.index_dao import IndexDAO

                db = SessionLocal()
                try:
                    existing_index = IndexDAO.get_index_by_file_md5(
                        db, file_info.get("file_md5")
                    )
                    if existing_index:
                        # æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œä½¿ç”¨ç°æœ‰çš„index_idå’Œå¯¹åº”çš„è¡¨
                        target_index_id = existing_index.index_id
                        logger.info(
                            f"Found existing index for file MD5: {file_info.get('file_md5')}, using existing index_id: {target_index_id}"
                        )
                        return target_index_id, True
                    else:
                        # æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œä½¿ç”¨å½“å‰index_idåˆ›å»ºæ–°çš„
                        logger.info(
                            f"No existing index found for file MD5: {file_info.get('file_md5')}, creating new index with id: {current_index_id}"
                        )
                        return current_index_id, False
                finally:
                    db.close()
            except Exception as e:
                logger.warning(
                    f"Error checking existing index by MD5: {e}, using current index_id: {current_index_id}"
                )
                return current_index_id, False
        else:
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶ä¿¡æ¯ï¼Œä½¿ç”¨å½“å‰index_id
            logger.info(
                f"No file MD5 available, using current index_id: {current_index_id}"
            )
            return current_index_id, False

    def _create_vector_store(
        self,
        nodes: List[Any],
        index_id: str,
        file_info: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Any, Any, str]:
        """åˆ›å»ºå‘é‡å­˜å‚¨

        Returns:
            Tuple[Any, Any, str]: (vector_store, index, actual_index_id)
        """
        import time

        if settings.VECTOR_STORE_TYPE == "postgres":
            # åˆ›å»ºPostgreSQLå‘é‡å­˜å‚¨
            logger.info(
                f"Creating PostgreSQL vector store with dimension {settings.VECTOR_DIM}"
            )
            start_time = time.time()

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒæ–‡ä»¶çš„ç´¢å¼•ï¼Œå¹¶ç¡®å®šä½¿ç”¨çš„ç´¢å¼•IDå’Œè¡¨å
            target_index_id, should_update = self._determine_vector_store_strategy(
                index_id, file_info
            )

            # åˆ›å»ºPostgreSQLå‘é‡å­˜å‚¨æ„å»ºå™¨ï¼ˆä½¿ç”¨ç¡®å®šçš„ç´¢å¼•IDï¼‰
            postgres_builder = create_postgres_vector_store_builder(
                host=settings.POSTGRES_HOST,
                port=settings.POSTGRES_PORT,
                database=settings.POSTGRES_DATABASE,
                user=settings.POSTGRES_USER,
                password=settings.POSTGRES_PASSWORD,
                table_name=f"{settings.POSTGRES_TABLE_NAME}_{target_index_id.replace('-', '_')}",
                embed_dim=settings.VECTOR_DIM,
            )

            if should_update:
                logger.info(f"Updating existing vector store with new data")
                # ä½¿ç”¨æ›´æ–°æ–¹æ³•
                index = postgres_builder.update_index_with_nodes(
                    nodes, self.embed_model
                )
                vector_store = index.storage_context.vector_store
            else:
                logger.info(f"Creating new vector store")
                # åˆ›å»ºå‘é‡å­˜å‚¨
                vector_store = postgres_builder.create_vector_store()
                store_time = time.time() - start_time
                logger.info(f"PostgreSQL vector store created in {store_time:.3f}s")

                # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
                logger.info("Creating storage context")
                start_time = time.time()
                storage_context = StorageContext.from_defaults(
                    vector_store=vector_store
                )
                context_time = time.time() - start_time
                logger.info(f"Storage context created in {context_time:.3f}s")

                # åˆ›å»ºç´¢å¼•
                logger.info(f"Creating vector store index with {len(nodes)} nodes")
                start_time = time.time()
                index = VectorStoreIndex(
                    nodes=nodes,
                    storage_context=storage_context,
                    embed_model=self.embed_model,
                )

            index_time = time.time() - start_time
            logger.info(f"Vector store index processed in {index_time:.2f}s")

            # è¿”å›å®é™…ä½¿ç”¨çš„ç´¢å¼•ID
            return vector_store, index, target_index_id

        else:
            # åˆ›å»ºFAISSç´¢å¼•ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
            logger.info(f"Creating FAISS index with dimension {settings.VECTOR_DIM}")
            start_time = time.time()
            faiss_index = faiss.IndexFlatL2(settings.VECTOR_DIM)
            faiss_time = time.time() - start_time
            logger.info(f"FAISS index created in {faiss_time:.3f}s")

            # åˆ›å»ºå‘é‡å­˜å‚¨
            logger.info("Creating FAISS vector store")
            start_time = time.time()
            vector_store = FaissVectorStore(faiss_index=faiss_index)
            store_time = time.time() - start_time
            logger.info(f"Vector store created in {store_time:.3f}s")

            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            logger.info("Creating storage context")
            start_time = time.time()
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            context_time = time.time() - start_time
            logger.info(f"Storage context created in {context_time:.3f}s")

            # åˆ›å»ºç´¢å¼•
            logger.info(f"Creating vector store index with {len(nodes)} nodes")
            start_time = time.time()
            index = VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                embed_model=self.embed_model,
            )
            index_time = time.time() - start_time
            logger.info(f"Vector store index created in {index_time:.2f}s")

            # å¯¹äºFAISSï¼Œä½¿ç”¨ä¼ å…¥çš„index_id
            return vector_store, index, index_id

    def _save_index(self, index: VectorStoreIndex, index_id: str) -> str:
        """ä¿å­˜ç´¢å¼•"""
        import time

        logger.info(f"Preparing index directory for {index_id}")
        index_dir = Path(settings.INDEX_STORE_PATH) / index_id
        index_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Index directory created: {index_dir}")

        # ä¿å­˜ç´¢å¼•
        logger.info(f"Starting index persistence to {index_dir}")
        start_time = time.time()
        index.storage_context.persist(persist_dir=str(index_dir))
        persist_time = time.time() - start_time

        logger.info(
            f"Index persistence completed in {persist_time:.2f}s to {index_dir}"
        )
        return str(index_dir)

    def _map_page_info_to_nodes(self, nodes: List[Any]) -> List[Any]:
        """å°†é¡µç ä¿¡æ¯æ˜ å°„åˆ°æ–‡æ¡£åˆ‡ç‰‡èŠ‚ç‚¹

        è¿™ä¸ªæ–¹æ³•å°è¯•å°†content_list.jsonä¸­çš„é¡µç ä¿¡æ¯æ˜ å°„åˆ°æ¯ä¸ªæ–‡æ¡£åˆ‡ç‰‡çš„å…ƒæ•°æ®ä¸­ï¼Œ
        ä½¿å¾—åœ¨æ£€ç´¢æ—¶å¯ä»¥çŸ¥é“æ¯ä¸ªåˆ‡ç‰‡æ¥è‡ªåŸå§‹æ–‡æ¡£çš„å“ªä¸€é¡µã€‚

        Args:
            nodes: æ–‡æ¡£åˆ‡ç‰‡èŠ‚ç‚¹åˆ—è¡¨

        Returns:
            æ·»åŠ äº†é¡µç ä¿¡æ¯çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        from pathlib import Path

        # æŒ‰æ–‡æ¡£åˆ†ç»„èŠ‚ç‚¹
        nodes_by_doc = {}
        for node in nodes:
            if not hasattr(node, "metadata"):
                continue

            doc_path = node.metadata.get("source_file")
            if not doc_path:
                continue

            if doc_path not in nodes_by_doc:
                nodes_by_doc[doc_path] = []
            nodes_by_doc[doc_path].append(node)

        # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„èŠ‚ç‚¹
        for doc_path, doc_nodes in nodes_by_doc.items():
            # æ ¹æ®æ–‡æ¡£è·¯å¾„æ„å»ºcontent_list.jsonè·¯å¾„
            doc_file = Path(doc_path)
            base_name = doc_file.stem
            content_list_path = doc_file.parent / f"{base_name}_content_list.json"

            if not content_list_path.exists():
                logger.warning(f"No content_list.json found for document: {doc_path}")
                continue

            try:
                with open(content_list_path, "r", encoding="utf-8") as f:
                    content_list = json.load(f)
            except Exception as e:
                logger.error(
                    f"Failed to load content_list from {content_list_path}: {e}"
                )
                continue

            if not isinstance(content_list, list) or not content_list:
                continue

            # åˆ›å»ºæ–‡æœ¬åˆ°é¡µç çš„æ˜ å°„
            text_to_page = {}
            for item in content_list:
                if item.get("type") == "text" and "text" in item and "page_idx" in item:
                    text = item["text"]
                    page_idx = item["page_idx"]
                    text_to_page[text] = page_idx

            # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…é¡µç 
            for node in doc_nodes:
                if not hasattr(node, "text") or not node.text:
                    continue

                # å°è¯•ç›´æ¥åŒ¹é…
                if node.text in text_to_page:
                    node.metadata["page_idx"] = text_to_page[node.text]
                    continue

                # å°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆæŸ¥æ‰¾èŠ‚ç‚¹æ–‡æœ¬ä¸­åŒ…å«çš„æœ€é•¿content_listæ–‡æœ¬ï¼‰
                best_match = None
                best_match_length = 0
                for text, page_idx in text_to_page.items():
                    if text in node.text and len(text) > best_match_length:
                        best_match = text
                        best_match_length = len(text)

                if best_match:
                    node.metadata["page_idx"] = text_to_page[best_match]
                    continue

                # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œå°è¯•åå‘åŒ¹é…ï¼ˆæŸ¥æ‰¾åŒ…å«èŠ‚ç‚¹æ–‡æœ¬çš„æœ€çŸ­content_listæ–‡æœ¬ï¼‰
                best_match = None
                best_match_length = float("inf")
                for text, page_idx in text_to_page.items():
                    if node.text in text and len(text) < best_match_length:
                        best_match = text
                        best_match_length = len(text)

                if best_match:
                    node.metadata["page_idx"] = text_to_page[best_match]

            # ç»Ÿè®¡æ·»åŠ äº†é¡µç çš„èŠ‚ç‚¹æ•°é‡
            nodes_with_page = sum(
                1 for node in doc_nodes if "page_idx" in node.metadata
            )
            logger.info(
                f"Added page information to {nodes_with_page}/{len(doc_nodes)} nodes for document: {doc_path}"
            )

        return nodes

    def _generate_stats(
        self, documents: List[Document], nodes: List[Any], task: VectorStoreTask
    ) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        total_chars = sum(len(doc.text) for doc in documents)
        avg_chunk_size = (
            sum(len(getattr(node, "text", "")) for node in nodes) / len(nodes)
            if nodes
            else 0
        )

        # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
        file_types = {}
        for file_path in task.processed_files:
            ext = Path(file_path).suffix.lower()
            file_types[ext] = file_types.get(ext, 0) + 1

        # ç»Ÿè®¡æœ‰é¡µç ä¿¡æ¯çš„èŠ‚ç‚¹æ•°é‡
        nodes_with_page = sum(
            1
            for node in nodes
            if hasattr(node, "metadata") and "page_idx" in node.metadata
        )

        return {
            "total_characters": total_chars,
            "average_chunk_size": avg_chunk_size,
            "file_types": file_types,
            "nodes_with_page_info": nodes_with_page,
            "nodes_with_page_percentage": (
                f"{nodes_with_page/len(nodes)*100:.2f}%" if nodes else "0%"
            ),
            "processing_time": (
                time.time() - task.started_at.timestamp() if task.started_at else 0
            ),
        }

    def _save_nodes_text_to_file(self, nodes: List[Any], task_id: str):
        """å°†æ‰€æœ‰èŠ‚ç‚¹çš„æ–‡æœ¬å†…å®¹ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç”¨äºè°ƒè¯•"""
        try:
            # åˆ›å»ºè°ƒè¯•è¾“å‡ºç›®å½•
            debug_dir = Path("debug_nodes")
            debug_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = debug_dir / f"nodes_text_{task_id}_{timestamp}.txt"
            
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(f"èŠ‚ç‚¹æ–‡æœ¬è°ƒè¯•è¾“å‡º\n")
                f.write(f"ä»»åŠ¡ID: {task_id}\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\n")
                f.write(f"æ€»èŠ‚ç‚¹æ•°: {len(nodes)}\n")
                f.write("=" * 80 + "\n\n")
                
                for i, node in enumerate(nodes):
                    f.write(f"èŠ‚ç‚¹ {i+1}/{len(nodes)}:\n")
                    f.write("-" * 40 + "\n")
                    
                    # è¾“å‡ºèŠ‚ç‚¹ID
                    if hasattr(node, 'node_id'):
                        f.write(f"èŠ‚ç‚¹ID: {node.node_id}\n")
                    
                    # è¾“å‡ºå…ƒæ•°æ®
                    if hasattr(node, 'metadata') and node.metadata:
                        f.write(f"å…ƒæ•°æ®: {json.dumps(node.metadata, ensure_ascii=False, indent=2)}\n")
                    
                    # è¾“å‡ºæ–‡æœ¬å†…å®¹
                    if hasattr(node, 'text'):
                        text_content = node.text if node.text else "[ç©ºæ–‡æœ¬]"
                        f.write(f"æ–‡æœ¬å†…å®¹ (é•¿åº¦: {len(text_content)}å­—ç¬¦):\n")
                        f.write(f"{text_content}\n")
                    else:
                        f.write("æ–‡æœ¬å†…å®¹: [æ— æ–‡æœ¬å±æ€§]\n")
                    
                    # è¾“å‡ºåµŒå…¥å‘é‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if hasattr(node, 'embedding') and node.embedding:
                        f.write(f"åµŒå…¥å‘é‡: å·²ç”Ÿæˆ (ç»´åº¦: {len(node.embedding)})\n")
                    else:
                        f.write("åµŒå…¥å‘é‡: æœªç”Ÿæˆ\n")
                    
                    f.write("\n" + "=" * 80 + "\n\n")
            
            logger.info(f"èŠ‚ç‚¹æ–‡æœ¬å·²ä¿å­˜åˆ°è°ƒè¯•æ–‡ä»¶: {output_file}")
            logger.info(f"å…±ä¿å­˜ {len(nodes)} ä¸ªèŠ‚ç‚¹çš„æ–‡æœ¬å†…å®¹")
            
        except Exception as e:
            logger.error(f"ä¿å­˜èŠ‚ç‚¹æ–‡æœ¬åˆ°æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    def _save_index_info_to_db(
        self,
        index_id: str,
        index_description: Optional[str] = None,
        file_info: Optional[Dict[str, Any]] = None,
        document_count: Optional[int] = None,
        node_count: Optional[int] = None,
        vector_dimension: Optional[int] = None,
        processing_config: Optional[Dict[str, Any]] = None,
    ):
        """ä¿å­˜ç´¢å¼•ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            from models.database import SessionLocal
            from dao.index_dao import IndexDAO

            # åˆ›å»ºæ•°æ®åº“ä¼šè¯
            db = SessionLocal()
            try:
                existing_index = None

                # å¦‚æœæœ‰æ–‡ä»¶ä¿¡æ¯ï¼Œä¼˜å…ˆæ ¹æ®æ–‡ä»¶MD5æŸ¥æ‰¾ç°æœ‰ç´¢å¼•
                if file_info and file_info.get("file_md5"):
                    existing_index = IndexDAO.get_index_by_file_md5(
                        db, file_info.get("file_md5")
                    )

                # å¦‚æœæ ¹æ®MD5æ²¡æ‰¾åˆ°ï¼Œå†æ ¹æ®ç´¢å¼•IDæŸ¥æ‰¾
                if not existing_index:
                    existing_index = IndexDAO.get_index_by_id(db, index_id)

                if existing_index:
                    # æ›´æ–°ç°æœ‰ç´¢å¼•ä¿¡æ¯
                    if index_description is not None:
                        existing_index.index_description = index_description
                    if document_count is not None:
                        existing_index.document_count = document_count
                    if node_count is not None:
                        existing_index.node_count = node_count
                    if vector_dimension is not None:
                        existing_index.vector_dimension = vector_dimension
                    if processing_config is not None:
                        existing_index.processing_config = processing_config

                    # æ›´æ–°ç´¢å¼•IDï¼ˆå¦‚æœä¸åŒï¼‰
                    if existing_index.index_id != index_id:
                        existing_index.index_id = index_id

                    db.commit()
                    db.refresh(existing_index)
                    logger.info(
                        f"Updated existing index info for file MD5: {file_info.get('file_md5') if file_info else 'N/A'}, index ID: {index_id}"
                    )
                else:
                    # åˆ›å»ºæ–°ç´¢å¼•ä¿¡æ¯
                    create_params = {
                        "index_id": index_id,
                        "index_description": index_description,
                        "document_count": document_count,
                        "node_count": node_count,
                        "vector_dimension": vector_dimension,
                        "processing_config": processing_config,
                    }

                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if file_info:
                        create_params.update(
                            {
                                "file_md5": file_info.get("file_md5"),
                                "file_path": file_info.get("file_path"),
                                "file_name": file_info.get("file_name"),
                                "file_size": file_info.get("file_size"),
                                "file_extension": file_info.get("file_extension"),
                                "mime_type": file_info.get("mime_type"),
                            }
                        )

                    IndexDAO.create_index(db, **create_params)
                    logger.info(f"Created new index info in database: {index_id}")
            finally:
                db.close()
        except Exception as e:
            logger.error(f"Error saving index info to database: {e}")
            raise

    def _extract_file_info_from_task(
        self, task: VectorStoreTask, documents: List[Any]
    ) -> Optional[Dict[str, Any]]:
        """ä»ä»»åŠ¡å’Œæ–‡æ¡£ä¸­æå–æ–‡ä»¶ä¿¡æ¯"""
        try:
            import hashlib
            from pathlib import Path

            # å°è¯•ä»è§£æä»»åŠ¡è·å–åŸå§‹æ–‡ä»¶ä¿¡æ¯
            if task.parse_task_id:
                from .document_parser import document_parser

                parse_task = document_parser.get_task(task.parse_task_id)
                if parse_task:
                    original_file_path = parse_task.get("file_path")
                    if original_file_path and Path(original_file_path).exists():
                        file_path = Path(original_file_path)

                        # è®¡ç®—æ–‡ä»¶MD5
                        file_md5 = None
                        try:
                            with open(file_path, "rb") as f:
                                file_content = f.read()
                                file_md5 = hashlib.md5(file_content).hexdigest()
                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate MD5 for {file_path}: {e}"
                            )

                        # è·å–æ–‡ä»¶ä¿¡æ¯
                        file_info = parse_task.get("file_info", {})

                        return {
                            "file_md5": file_md5,
                            "file_path": str(file_path),
                            "file_name": file_path.name,
                            "file_size": file_info.get("size")
                            or file_path.stat().st_size,
                            "file_extension": file_path.suffix,
                            "mime_type": file_info.get("mime_type"),
                        }

            # å¦‚æœæ— æ³•ä»è§£æä»»åŠ¡è·å–ï¼Œå°è¯•ä»æ–‡æ¡£å…ƒæ•°æ®è·å–
            if documents:
                first_doc = documents[0]
                if hasattr(first_doc, "metadata") and first_doc.metadata:
                    metadata = first_doc.metadata
                    original_file_path = metadata.get("original_file_path")
                    if original_file_path and Path(original_file_path).exists():
                        file_path = Path(original_file_path)

                        # è®¡ç®—æ–‡ä»¶MD5
                        file_md5 = None
                        try:
                            with open(file_path, "rb") as f:
                                file_content = f.read()
                                file_md5 = hashlib.md5(file_content).hexdigest()
                        except Exception as e:
                            logger.warning(
                                f"Failed to calculate MD5 for {file_path}: {e}"
                            )

                        return {
                            "file_md5": file_md5,
                            "file_path": str(file_path),
                            "file_name": metadata.get("original_file_name")
                            or file_path.name,
                            "file_size": metadata.get("file_size"),
                            "file_extension": metadata.get("file_extension")
                            or file_path.suffix,
                            "mime_type": metadata.get("mime_type"),
                        }

            return None

        except Exception as e:
            logger.error(f"Error extracting file info from task: {e}")
            return None

    def _generate_auto_description(
        self, index: Any, config: Dict[str, Any]
    ) -> Optional[str]:
        """è‡ªåŠ¨ç”Ÿæˆç´¢å¼•æè¿°"""
        try:
            from .auto_index_description_generator import AutoIndexDescriptionGenerator

            # åˆ›å»ºæè¿°ç”Ÿæˆå™¨
            description_generator = AutoIndexDescriptionGenerator(llm=self.llm)

            # ç”Ÿæˆæè¿°
            sample_size = config.get("description_sample_size", 50)
            description = description_generator.generate_description(index, sample_size)

            logger.info("Automatic index description generated successfully")
            return description

        except Exception as e:
            logger.error(f"Error generating automatic index description: {str(e)}")
            return None

    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        task = self.tasks.get(task_id)
        return task.to_dict() if task else None

    def get_all_tasks(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
        return [task.to_dict() for task in self.tasks.values()]

    def create_vector_store_task_from_parse_task(
        self, task_id: str, config: Dict[str, Any] = None
    ) -> str:
        """ä»è§£æä»»åŠ¡åˆ›å»ºå‘é‡å­˜å‚¨æ„å»ºä»»åŠ¡"""
        # è·å–è§£æä»»åŠ¡ä¿¡æ¯
        parse_task = self.task_dao.get_parse_task(task_id)
        if not parse_task:
            raise ValueError(f"Parse task not found: {task_id}")

        if parse_task.status != "COMPLETED":
            raise ValueError(
                f"Parse task {task_id} is not completed. Current status: {parse_task.status}"
            )

        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸»ä»»åŠ¡ï¼ˆparent_task_idä¸ºNoneè¡¨ç¤ºä¸»ä»»åŠ¡ï¼‰
        is_main_task = parse_task.parent_task_id is None
        output_dir = None
        
        if is_main_task:
            # ä¸»ä»»åŠ¡ï¼ˆç›®å½•è§£æï¼‰æ— éœ€æ ¡éªŒoutput_directory
            # ä¸»ä»»åŠ¡é€šè¿‡å­ä»»åŠ¡æ¥å¤„ç†å…·ä½“æ–‡ä»¶ï¼Œè‡ªèº«ä¸ç›´æ¥äº§ç”Ÿè¾“å‡ºç›®å½•
            logger.info(f"Main task {task_id} detected, skipping output_directory validation")
        else:
            # å­ä»»åŠ¡ï¼ˆå•æ–‡ä»¶è§£æï¼‰å¿…é¡»æœ‰output_directory
            output_dir = parse_task.output_directory
            if not output_dir:
                raise ValueError(f"No output directory found for subtask {task_id}")
            
            # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(output_dir):
                raise ValueError(f"Output directory does not exist: {output_dir}")
            
            logger.info(f"Subtask {task_id} validated, output_directory: {output_dir}")

        # åˆ›å»ºå‘é‡å­˜å‚¨ä»»åŠ¡ID
        vector_task_id = str(uuid.uuid4())

        # è®¾ç½®é»˜è®¤é…ç½®
        task_config = config or {
            "extract_keywords": True,
            "extract_summary": True,
            "generate_qa": True,
            "chunk_size": settings.CHUNK_SIZE,
            "chunk_overlap": settings.CHUNK_OVERLAP,
        }

        # æ·»åŠ è§£æä»»åŠ¡å…³è”
        task_config["parse_task_id"] = task_id

        # åˆ›å»ºä»»åŠ¡æ•°æ®
        task_data = {
            "task_id": vector_task_id,
            "parse_task_id": task_id,
            "status": "PENDING",
            "progress": 0,
            "config": task_config,
            "processed_files": [],
            "total_files": 0,
        }

        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“å¹¶è·å–ä»»åŠ¡å¯¹è±¡
        task = self.task_dao.create_vector_store_task(task_data)
        if task:
            self.tasks[vector_task_id] = task

        # å¼‚æ­¥æ‰§è¡Œæ„å»º
        asyncio.create_task(self._execute_build_task(task))

        logger.info(
            f"Created vector store build task {vector_task_id} from parse task {task_id}, output dir: {output_dir}"
        )
        return vector_task_id

    def _update_task_in_db(self, task: VectorStoreTask):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€"""
        try:
            update_data = {
                "status": task.status,
                "progress": task.progress,
                "result": task.result,
                "error": task.error,
                "processed_files": task.processed_files,
                "total_files": task.total_files,
            }

            if task.started_at:
                update_data["started_at"] = task.started_at
            if task.completed_at:
                update_data["completed_at"] = task.completed_at

            # å¦‚æœä»»åŠ¡å®Œæˆï¼Œä¿å­˜ç´¢å¼•ä¿¡æ¯
            if task.status == TaskStatus.COMPLETED and task.result:
                update_data["index_id"] = task.result.get("index_id")
                update_data["total_documents"] = task.result.get("stats", {}).get(
                    "total_documents", 0
                )
                update_data["total_nodes"] = task.result.get("stats", {}).get(
                    "total_nodes", 0
                )

            self.task_dao.update_vector_store_task(task.task_id, update_data)
            logger.debug(f"Vector store task {task.task_id} status updated in database")
        except Exception as e:
            logger.error(
                f"Failed to update vector store task {task.task_id} in database: {e}"
            )

    def cleanup_expired_tasks(self):
        """æ¸…ç†è¿‡æœŸä»»åŠ¡"""
        current_time = datetime.utcnow()
        expired_tasks = [
            task_id
            for task_id, task in self.tasks.items()
            if task.created_at
            and (current_time - task.created_at).total_seconds()
            > settings.TASK_EXPIRE_TIME
        ]

        for task_id in expired_tasks:
            del self.tasks[task_id]
            logger.info(f"Cleaned up expired vector store task: {task_id}")


# å…¨å±€æ„å»ºå™¨å®ä¾‹
vector_store_builder = VectorStoreBuilder()
