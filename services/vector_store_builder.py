import asyncio
import os
from tabnanny import verbose
import uuid
import json
import time
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import logging
from datetime import datetime

from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter, HTMLNodeParser, MarkdownNodeParser
from llama_index.node_parser.docling import DoclingNodeParser
from services.document_level_metadata_extractor import DocumentLevelMetadataExtractor
from services.chunk_level_metadata_extractor import ChunkLevelMetadataExtractor
from llama_index.core.ingestion import IngestionPipeline


# ä½¿ç”¨å·²å¯¼å…¥çš„SessionLocal
from dao.index_dao import IndexDAO

from config import settings
from models.database import get_db, SessionLocal
from services.document_docling_processor import DocumentDoclingProcessor
from services.document_html_processor import DocumentHTMLProcessor
from services.document_parser import document_parser, TaskStatus
from common.postgres_vector_store import create_postgres_vector_store_builder
from dao.task_dao import TaskDAO
from models.task_models import ParseTask, VectorStoreTask
from services.model_client_factory import ModelClientFactory

logger = logging.getLogger(__name__)



class VectorStoreBuilder:
    """å‘é‡æ•°æ®åº“æ„å»ºå™¨"""

    _instance = None
    _initialized = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._setup_models()
            self.tasks: Dict[str, VectorStoreTask] = {}
            # self.executor = ThreadPoolExecutor(max_workers=2)  # é™åˆ¶å¹¶å‘æ•° - ä¼¼ä¹æœªä½¿ç”¨ï¼Œæš‚æ—¶æ³¨é‡Šæ‰ä»¥è§£å†³è¿›ç¨‹æŒ‚èµ·é—®é¢˜

            # åˆå§‹åŒ–ä»»åŠ¡DAO
            self.task_dao = TaskDAO()
            
            # é‡ç½®æœåŠ¡ä¸­æ–­æ—¶çš„RUNNINGçŠ¶æ€ä»»åŠ¡
            self._reset_running_tasks_on_startup()

            self._initialized = True
            logger.info("VectorStoreBuilder initialized")
    
    def _reset_running_tasks_on_startup(self):
        """åœ¨æœåŠ¡å¯åŠ¨æ—¶é‡ç½®æ‰€æœ‰RUNNINGçŠ¶æ€çš„å‘é‡ä»»åŠ¡ä¸ºFAILEDçŠ¶æ€"""
        try:
            # è·å–æ‰€æœ‰RUNNINGçŠ¶æ€çš„å‘é‡ä»»åŠ¡
            running_tasks = self.task_dao.list_vector_store_tasks(status=TaskStatus.RUNNING)
            
            if running_tasks:
                logger.info(f"Found {len(running_tasks)} RUNNING vector store tasks, resetting to FAILED status")
                
                for task in running_tasks:
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºFAILED
                    update_data = {
                        "status": TaskStatus.FAILED,
                        "error": "Service was interrupted, task reset to failed status",
                        "completed_at": datetime.utcnow()
                    }
                    
                    self.task_dao.update_vector_store_task(task.task_id, update_data)
                    logger.info(f"Reset vector store task {task.task_id} from RUNNING to FAILED")
                
                logger.info(f"Successfully reset {len(running_tasks)} RUNNING tasks to FAILED status")
            else:
                logger.info("No RUNNING vector store tasks found during startup")
                
        except Exception as e:
            logger.error(f"Error resetting RUNNING tasks on startup: {e}")
    
    def _clear_gpu_memory_if_needed(self):
        """åœ¨éœ€è¦æ—¶æ¸…ç†GPUå†…å­˜"""
        try:
            import torch
            import gc
            
            if torch.cuda.is_available() and settings.USE_GPU:
                # è·å–å½“å‰GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                device = torch.device(settings.GPU_DEVICE)
                total_memory = torch.cuda.get_device_properties(device).total_memory
                allocated_memory = torch.cuda.memory_allocated(device)
                cached_memory = torch.cuda.memory_reserved(device)
                
                # è®¡ç®—å†…å­˜ä½¿ç”¨ç‡
                memory_usage_ratio = cached_memory / total_memory
                
                logger.info(f"GPUå†…å­˜ä½¿ç”¨æƒ…å†µ: {cached_memory / 1024**3:.2f}GB / {total_memory / 1024**3:.2f}GB ({memory_usage_ratio*100:.1f}%)")
                
                # å¦‚æœå†…å­˜ä½¿ç”¨ç‡è¶…è¿‡70%ï¼Œè¿›è¡Œæ¸…ç†
                if memory_usage_ratio > 0.7:
                    logger.warning(f"GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({memory_usage_ratio*100:.1f}%)ï¼Œå¼€å§‹æ¸…ç†...")
                    
                    # æ¸…ç†PyTorchç¼“å­˜
                    torch.cuda.empty_cache()
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                    
                    # å†æ¬¡æ£€æŸ¥å†…å­˜
                    new_cached_memory = torch.cuda.memory_reserved(device)
                    new_usage_ratio = new_cached_memory / total_memory
                    
                    logger.info(f"æ¸…ç†åGPUå†…å­˜ä½¿ç”¨: {new_cached_memory / 1024**3:.2f}GB / {total_memory / 1024**3:.2f}GB ({new_usage_ratio*100:.1f}%)")
                    
                    if new_usage_ratio < memory_usage_ratio:
                        logger.info(f"âœ… GPUå†…å­˜æ¸…ç†æˆåŠŸï¼Œé‡Šæ”¾äº† {(cached_memory - new_cached_memory) / 1024**3:.2f}GB")
                    else:
                        logger.warning("âš ï¸  GPUå†…å­˜æ¸…ç†æ•ˆæœæœ‰é™ï¼Œå»ºè®®é™ä½æ‰¹å¤„ç†å¤§å°æˆ–é‡å¯åº”ç”¨")
                else:
                    logger.info("âœ… GPUå†…å­˜ä½¿ç”¨æ­£å¸¸ï¼Œæ— éœ€æ¸…ç†")
        except Exception as e:
            logger.warning(f"GPUå†…å­˜æ£€æŸ¥å¤±è´¥: {e}")

    def _setup_models(self):
        """è®¾ç½®æ¨¡å‹"""
        try:
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embed_model = ModelClientFactory.create_embedding_client(
                settings.embedding_model_settings
            )

            self.llm = ModelClientFactory.create_llm_client(settings.llm_model_settings)

        except Exception as e:
            logger.error(f"Failed to initialize models: {e}")
            raise

    async def build_vector_store(self, task_id: str, config: Dict[str, Any]) -> str:
        """æ„å»ºå‘é‡æ•°æ®åº“

        Args:
            task_id: è§£æä»»åŠ¡ID
            config: æ„å»ºé…ç½®

        Returns:
            å‘é‡å­˜å‚¨ä»»åŠ¡ID
        """
        return self.create_vector_store_task_from_parse_task(task_id, config)

    async def _execute_build_task(self, task: VectorStoreTask):
        """æ‰§è¡Œæ„å»ºä»»åŠ¡"""
        try:
            task.status = TaskStatus.RUNNING
            task.started_at = datetime.utcnow()
            task.progress = 5

            # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
            self._update_task_in_db(task)

            logger.info(f"Starting vector store build task {task.task_id}")

            # ç›´æ¥æ‰§è¡Œå¼‚æ­¥æ„å»º
            result = await self._build_vector_store_sync(task)

            task.result = result
            task.status = TaskStatus.COMPLETED
            task.progress = 100
            task.completed_at = datetime.utcnow()

            # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
            self._update_task_in_db(task)

            logger.info(
                f"Vector store build task {task.task_id} completed successfully"
            )

        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error = str(e)
            task.completed_at = datetime.utcnow()

            # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
            self._update_task_in_db(task)

            logger.error(f"Vector store build task {task.task_id} failed: {e}")

    def test_collection_documents(self, task_id: str):
        """æµ‹è¯•æ”¶é›†æ–‡æ¡£"""
        task = self.task_dao.get_parse_task(task_id)
        if not task:
            raise ValueError(f"Task {task_id} not found")
        
        all_tasks = self._collect_all_tasks_recursively(task)
        
        # è½¬æ¢ä¸ºå­—å…¸å¹¶å¤„ç†datetimeå­—æ®µ
        result = []
        for task in all_tasks:
            task_dict = task.to_dict()
            # å°†datetimeå­—æ®µè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            for key, value in task_dict.items():
                if hasattr(value, 'isoformat'):  # datetimeå¯¹è±¡æœ‰isoformatæ–¹æ³•
                    task_dict[key] = value.isoformat() if value else None
            result.append(task_dict)
        
        return result

    async def _build_vector_store_sync(self, task: VectorStoreTask) -> Dict[str, Any]:
        """åŒæ­¥æ„å»ºå‘é‡æ•°æ®åº“"""
        try:
            # 1. æ”¶é›†æ–‡æ¡£æ–‡ä»¶
            task.progress = 10
            documents_by_type, origin_file_path = self._collect_documents(task)
            total_files = sum(len(docs) for docs in documents_by_type.values())
            task.total_files = total_files

            all_nodes = []
            all_doc_ids = []

            # 3. è®¾ç½®æå–å™¨
            task.progress = 30
            # æ–‡æ¡£çº§å…ƒæ•°æ®æå–å™¨ï¼ˆåœ¨åˆ‡åˆ†å‰æå–ï¼‰
            document_extractor = DocumentLevelMetadataExtractor(
                llm=self.llm,
                enable_persistent_cache=task.config.get("enable_persistent_cache", True),
                cache_dir=task.config.get("cache_dir", "cache/metadata"),
            )
            
            # chunkçº§å…ƒæ•°æ®æå–å™¨ï¼ˆåœ¨åˆ‡åˆ†åæå–ï¼‰
            chunk_extractor = ChunkLevelMetadataExtractor(
                llm=self.llm,
                min_chunk_size_for_extraction=task.config.get(
                    "min_chunk_size_for_extraction", 20 # å°äº20æ²¡æœ‰æ„ä¹‰äº†
                ),
                max_keywords=task.config.get("max_keywords", 6),
            )

            # 4. ä¸ºæ¯ç§æ–‡ä»¶ç±»å‹åˆ›å»ºä¸“ç”¨å¤„ç†ç®¡é“
            task.progress = 40
            progress_step = 20 / total_files  # åœ¨40-60%ä¹‹é—´åˆ†é…è¿›åº¦

            for file_type, type_docs in documents_by_type.items():
                logger.info(f"Processing {len(type_docs)} {file_type} files")

                # è·å–é€‚åˆè¯¥æ–‡ä»¶ç±»å‹çš„èŠ‚ç‚¹è§£æå™¨
                logger.info(f"Getting node parser for file type: {file_type}")
                
                # é¦–å…ˆåº”ç”¨æ–‡æ¡£çº§å…ƒæ•°æ®æå–å™¨è·å–æ–‡æ¡£ç±»å‹
                logger.info("Extracting document-level metadata to determine document category")
                doc_metadata_list = await document_extractor.aextract(type_docs)
                
                # å¤„ç†æ¯ä¸ªæ–‡æ¡£ï¼Œæ ¹æ®å…¶ç±»å‹é€‰æ‹©åˆé€‚çš„è§£æå™¨
                processed_docs = []
                for i, doc in enumerate(type_docs):
                    # è·å–æ–‡æ¡£ç±»å‹
                    doc_metadata = doc_metadata_list[i] if i < len(doc_metadata_list) else {}
                    document_category = doc_metadata.get('document_category', 'general_document')
                    
                    # å°†å…ƒæ•°æ®æ·»åŠ åˆ°æ–‡æ¡£ä¸­
                    for key, value in doc_metadata.items():
                        doc.metadata[key] = value
                    
                    # è·å–é€‚åˆè¯¥æ–‡æ¡£ç±»å‹çš„èŠ‚ç‚¹è§£æå™¨
                    logger.info(f"Document {i+1}/{len(type_docs)} classified as: {document_category}")
                    node_parsers = self._get_node_parser_for_file_type(
                        file_type, task.config, document_category
                    )
                    
                    # ç¡®ä¿node_parsersæ˜¯åˆ—è¡¨æ ¼å¼
                    if not isinstance(node_parsers, list):
                        node_parsers = [node_parsers]
                    
                    # åˆ›å»ºè¯¥æ–‡æ¡£çš„å¤„ç†ç®¡é“
                    doc_pipeline = IngestionPipeline(
                        transformations=[*node_parsers, chunk_extractor, self.embed_model]
                    )
                    
                    # å¤„ç†å•ä¸ªæ–‡æ¡£
                    try:
                        # é€ä¸ªæ‰§è¡Œtransformationæ­¥éª¤
                        current_doc = [doc]  # å•æ–‡æ¡£åˆ—è¡¨
                        for step_idx, transformation in enumerate(doc_pipeline.transformations):
                            if hasattr(transformation, "transform"):
                                current_doc = transformation.transform(current_doc)
                            elif hasattr(transformation, "__call__"):
                                current_doc = transformation(current_doc)
                            elif hasattr(transformation, "aextract"):
                                current_doc = await transformation.aextract(current_doc)
                        
                        # æ·»åŠ å¤„ç†åçš„èŠ‚ç‚¹
                        processed_docs.extend(current_doc)
                        logger.info(f"Document {i+1} processed with {document_category} strategy, generated {len(current_doc)} nodes")
                    except Exception as e:
                        logger.error(f"Error processing document {i+1} with {document_category} strategy: {str(e)}")
                        # å¦‚æœå•ä¸ªæ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ–‡æ¡£
                        continue
                
                # ä½¿ç”¨å¤„ç†åçš„èŠ‚ç‚¹
                type_nodes = processed_docs
                logger.info(f"All documents processed, total nodes: {len(type_nodes)}")
                
                # è·³è¿‡åç»­çš„ç®¡é“å¤„ç†ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨æ¯ä¸ªæ–‡æ¡£çº§åˆ«å¤„ç†äº†
                current_docs = type_nodes
                
                # è®°å½•æ¯ä¸ªæ–‡æ¡£çš„å¤„ç†è¯¦æƒ…
                for i, doc in enumerate(type_docs):
                    logger.info(
                        f"Document {i+1}/{len(type_docs)}: {doc.metadata.get('file_name', 'unknown')} (size: {len(doc.text)} chars)"
                    )
                    all_doc_ids.append(doc.doc_id)
                
                # æ·»åŠ å¤„ç†åçš„èŠ‚ç‚¹åˆ°æ€»åˆ—è¡¨
                all_nodes.extend(type_nodes)
                logger.info(f"Total nodes accumulated: {len(all_nodes)}")

                task.progress += progress_step
                logger.info(f"Progress updated to {task.progress:.1f}%")

            # 5. åˆå¹¶æ‰€æœ‰èŠ‚ç‚¹
            task.progress = 60
            task.total_nodes = len(all_nodes)
            nodes = all_nodes
            logger.info(f"Merging nodes completed, total nodes: {len(nodes)}")
            
            # æ³¨æ„ï¼šç°åœ¨DocumentLevelMetadataExtractorè¿”å›å…ƒæ•°æ®å­—å…¸ï¼Œä¸ä¼šå‘all_nodesæ·»åŠ åŸå§‹æ–‡æ¡£èŠ‚ç‚¹
            # NodeParserä¼šè‡ªåŠ¨ç»§æ‰¿æ–‡æ¡£å…ƒæ•°æ®å¹¶ç”Ÿæˆå¸¦æœ‰node_type='chunk'çš„èŠ‚ç‚¹
            task.total_nodes = len(nodes)

            # 5.5 ä¸ºèŠ‚ç‚¹æ·»åŠ é¡µç ä¿¡æ¯
            nodes = self._map_page_info_to_nodes(nodes)
            logger.info(f"Page information mapped to nodes, total nodes: {len(nodes)}")

            # 5.6 è¾“å‡ºæ‰€æœ‰èŠ‚ç‚¹æ–‡æœ¬åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            logger.info(f"ğŸ” å‡†å¤‡ä¿å­˜èŠ‚ç‚¹è°ƒè¯•ä¿¡æ¯ï¼ŒèŠ‚ç‚¹æ•°é‡: {len(nodes)}")
            self._save_nodes_text_to_file(nodes, task.task_id)
            logger.info(f"âœ… Nodes text saved to local file for debugging")

            # 6. åˆ›å»ºå‘é‡å­˜å‚¨
            task.progress = 80
            logger.info(f"Starting vector store creation with {len(nodes)} nodes")
            
            # åœ¨åˆ›å»ºå‘é‡å­˜å‚¨å‰æ¸…ç†GPUå†…å­˜
            self._clear_gpu_memory_if_needed()
            
            start_time = time.time()

            try:
                # ä¾æ®parse_task_id æ¥åˆ›å»ºå‘é‡ç´¢å¼•æˆ–æ•°æ®åº“
                vector_store, index, actual_index_id = (
                    self._create_or_update_vector_store(
                        nodes,
                        origin_file_path,
                        all_doc_ids,
                    )
                )
                creation_time = time.time() - start_time
                logger.info(
                    f"Vector store creation completed in {creation_time:.2f}s, using index_id: {actual_index_id}"
                )
            except Exception as e:
                creation_time = time.time() - start_time
                logger.error(
                    f"Vector store creation failed after {creation_time:.2f}s: {e}"
                )
                raise

            # 7. ä¿å­˜ç´¢å¼•
            task.progress = 95
            logger.info(
                f"Starting index save for task {task.task_id}, using index_id: {actual_index_id}"
            )

            # 8. ä¿å­˜ç´¢å¼•ä¿¡æ¯åˆ°æ•°æ®åº“
            logger.info("Saving index information to database")
            start_time = time.time()
            try:
                # è‡ªåŠ¨ç”Ÿæˆç´¢å¼•æè¿°
                logger.info("Generating automatic description for index")
                index_description = self._generate_auto_description(index, task.config)
                if not index_description:
                    logger.warning("Failed to generate automatic description")
                    index_description = "Auto-generated vector store index"
                else:
                    logger.info("Automatic description generated successfully")

                self._save_index_info_to_db(
                    origin_file_path,
                    actual_index_id,
                    index_description,
                    document_count=len(all_doc_ids),
                    node_count=len(nodes),
                    vector_dimension=settings.VECTOR_DIM,
                    processing_config=task.config,
                )
                db_time = time.time() - start_time
                logger.info(f"Index information saved to database in {db_time:.2f}s")
            except Exception as e:
                db_time = time.time() - start_time
                logger.error(
                    f"Failed to save index information to database after {db_time:.2f}s: {e}"
                )
                # ç»§ç»­æ‰§è¡Œï¼Œä¸å½±å“ç´¢å¼•åˆ›å»ºçš„ä¸»æµç¨‹

            result = {
                "index_id": actual_index_id,
                "node_count": len(nodes),
                "vector_dimension": settings.VECTOR_DIM,
                "config": task.config,
            }

            return result

        except Exception as e:
            logger.error(f"Error building vector store: {e}")
            raise

    def _process_document_task(self, doc_task) -> List[Document]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹å¤„ç†å•ä¸ªæ–‡æ¡£ä»»åŠ¡å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
        if doc_task.file_extension in [".html", ".htm"]:
            logger.info(f"Processing HTML task {doc_task.task_id} with HTMLProcessor")
            return DocumentHTMLProcessor().collect_document(doc_task)
        else:
            logger.info(
                f"Processing non-HTML task {doc_task.task_id} with DoclingProcessor"
            )
            return DocumentDoclingProcessor().collect_document(doc_task)

    def _collect_documents(
        self, task: VectorStoreTask
    ) -> Tuple[Dict[str, List[Document]], str]:
        """
        æ”¶é›†ç›®å½•ä¸­çš„æ–‡æ¡£
        """
        with next(get_db()) as db:
            task_dao = TaskDAO(db)
            parse_task = task_dao.get_parse_task(task.parse_task_id)

            if not parse_task:
                raise ValueError(f"Parse task not found: {task.parse_task_id}")

            documents_by_type = {}

            # é€’å½’æ”¶é›†æ‰€æœ‰å±‚çº§çš„ä»»åŠ¡
            all_tasks = self._collect_all_tasks_recursively(parse_task)

            logger.warning(
                f"Found {len(all_tasks)} task(s) to process for document collection (including recursive subtasks)."
            )

            for task_item in all_tasks:
                docs = self._process_document_task(task_item)
                for doc in docs:
                    file_type = doc.metadata.get("file_type")  # æŒ‰file_typeåˆ†ç±»
                    documents_by_type.setdefault(file_type, []).append(doc)

            total_docs = sum(len(docs) for docs in documents_by_type.values())
            logger.info(
                f"Collected {total_docs} documents from task {task.parse_task_id}, grouped by file type."
            )
            return documents_by_type, parse_task.file_path

    def _collect_all_tasks_recursively(self, parse_task):
        """
        é€’å½’æ”¶é›†æ‰€æœ‰å±‚çº§çš„ä»»åŠ¡

        ç”±äº task_dao.get_parse_task æ–¹æ³•åªèƒ½åŠ è½½ä¸€å±‚å­ä»»åŠ¡ï¼Œ
        è¿™ä¸ªæ–¹æ³•ä¼šé€šè¿‡å¤šæ¬¡æŸ¥è¯¢æ•°æ®åº“æ¥è·å–æ‰€æœ‰å±‚çº§çš„å­ä»»åŠ¡ã€‚

        Args:
            parse_task: æ ¹ä»»åŠ¡

        Returns:
            List: åŒ…å«æ‰€æœ‰å±‚çº§ä»»åŠ¡çš„åˆ—è¡¨
        """
        all_tasks = []
        task_dao = self.task_dao

        def collect_tasks(task: ParseTask):
            """é€’å½’æ”¶é›†ä»»åŠ¡çš„å†…éƒ¨å‡½æ•°"""
            # åŸºäºKNOWLEDGE_BASE_DIRå’Œtask_idæ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨
            from config import settings
            expected_output_dir = os.path.join(settings.KNOWLEDGE_BASE_DIR, "outputs", task.task_id)
            if os.path.exists(expected_output_dir):
                all_tasks.append(task)
            
            # å¦‚æœä»»åŠ¡æœ‰å­ä»»åŠ¡ï¼Œé€’å½’å¤„ç†å­ä»»åŠ¡
            if task.subtasks:
                for subtask in task.subtasks:
                    # å¯¹äºæ¯ä¸ªå­ä»»åŠ¡ï¼Œé‡æ–°ä»æ•°æ®åº“åŠ è½½ä»¥è·å–å…¶å­ä»»åŠ¡
                    loaded_subtask = task_dao.get_parse_task(subtask.task_id)
                    if loaded_subtask:
                        collect_tasks(loaded_subtask)
                    else:
                        # å¦‚æœæ— æ³•åŠ è½½å­ä»»åŠ¡ï¼Œåˆ™ä½¿ç”¨å½“å‰ä»»åŠ¡
                        collect_tasks(subtask)
            elif not os.path.exists(expected_output_dir):
                # å¦‚æœæ²¡æœ‰å­ä»»åŠ¡ä¸”æ²¡æœ‰è¾“å‡ºç›®å½•ï¼Œè¯´æ˜æ˜¯å¶å­èŠ‚ç‚¹ä½†æ²¡æœ‰è¾“å‡ºï¼Œä¹Ÿæ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
                all_tasks.append(task)

        collect_tasks(parse_task)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¶å­èŠ‚ç‚¹ä»»åŠ¡ï¼Œè¯´æ˜æ ¹ä»»åŠ¡æœ¬èº«å°±æ˜¯å¶å­èŠ‚ç‚¹
        if not all_tasks:
            all_tasks.append(parse_task)

        logger.info(
            f"Recursively collected {len(all_tasks)} leaf tasks from parse_task {parse_task.task_id}"
        )
        return all_tasks

    def _get_node_parser_for_file_type(self, file_type: str, config: Dict[str, Any], document_category: str = None):
        """æ ¹æ®æ–‡ä»¶ç±»å‹å’Œæ–‡æ¡£ç±»å‹è·å–åˆé€‚çš„èŠ‚ç‚¹è§£æå™¨
        
        Args:
            file_type: æ–‡ä»¶ç±»å‹ï¼ˆå¦‚.pdf, .mdç­‰ï¼‰
            config: é…ç½®å­—å…¸
            document_category: æ–‡æ¡£ç±»å‹ï¼ˆlegal_document, policy_document, general_documentï¼‰
        """
        # æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´åˆ†å—ç­–ç•¥
        chunk_size, chunk_overlap = self._adjust_chunk_params_by_document_type(
            document_category, config
        )
        
        logger.info(f"ğŸ“Š Chunking strategy for {document_category or 'unknown'} document: chunk_size={chunk_size}, chunk_overlap={chunk_overlap}")

        # å¯¹äºæ”¿ç­–æ–‡æ¡£ï¼Œä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥è§£æå™¨
        # if document_category == 'policy_document':
            
        #     from llama_index.core.node_parser import SemanticSplitterNodeParser
        #     logger.info(f"ğŸ¯ Using semantic splitter for {document_category}")
        #     return [SemanticSplitterNodeParser(
        #         embed_model=self.embed_model,
        #     )]

        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŸºç¡€è§£æå™¨
        if file_type == ".md":
            # Markdownæ–‡ä»¶ä½¿ç”¨MarkdownNodeParserï¼Œç„¶åé…åˆSentenceSplitter
            return [
                MarkdownNodeParser.from_defaults(),
                SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),
            ]
        elif file_type in [".html", ".htm"]:
            # HTMLæ–‡ä»¶ä½¿ç”¨HTMLNodeParserï¼Œç„¶åé…åˆSentenceSplitter
            return [
                HTMLNodeParser.from_defaults(),
                SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),
            ]
        elif file_type == ".json":
            # JSONæ–‡ä»¶ç›´æ¥ä½¿ç”¨DoclingNodeParser
            return [DoclingNodeParser()]
        else:
            # æ–‡æœ¬æ–‡ä»¶ç›´æ¥ä½¿ç”¨SentenceSplitter
            return [SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)]
    
    def _adjust_chunk_params_by_document_type(self, document_category: str, config: Dict[str, Any]) -> Tuple[int, int]:
        """æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´åˆ†å—å‚æ•°
        
        Args:
            document_category: æ–‡æ¡£ç±»å‹
            config: é…ç½®å­—å…¸
            
        Returns:
            Tuple[int, int]: è°ƒæ•´åçš„(chunk_size, chunk_overlap)
        """
        if not document_category:
            return settings.GENERAL_CHUNK_SIZE, settings.GENERAL_CHUNK_OVERLAP
            
        # æ³•å¾‹æ–‡æ¡£ï¼šä½¿ç”¨è¾ƒå¤§çš„åˆ†å—ä»¥ä¿æŒæ³•æ¡å®Œæ•´æ€§
        if document_category == 'legal_document':
            chunk_size = config.get("legal_chunk_size", settings.LEGAL_CHUNK_SIZE)
            chunk_overlap = config.get("legal_chunk_overlap", settings.LEGAL_CHUNK_OVERLAP)
            return chunk_size, chunk_overlap
            
        # æ”¿ç­–æ–‡æ¡£ï¼šä½¿ç”¨ä¸­ç­‰åˆ†å—ä»¥å¹³è¡¡å†…å®¹å®Œæ•´æ€§å’Œæ£€ç´¢ç²¾åº¦
        elif document_category == 'policy_document':
            chunk_size = config.get("policy_chunk_size", settings.POLICY_CHUNK_SIZE)
            chunk_overlap = config.get("policy_chunk_overlap", settings.POLICY_CHUNK_OVERLAP)
            return chunk_size, chunk_overlap
            
        # é€šç”¨æ–‡æ¡£ï¼šä½¿ç”¨é»˜è®¤åˆ†å—ç­–ç•¥
        else:  # general_document
            chunk_size = config.get("general_chunk_size", settings.GENERAL_CHUNK_SIZE)
            chunk_overlap = config.get("general_chunk_overlap", settings.GENERAL_CHUNK_OVERLAP)
            return chunk_size, chunk_overlap

    def _determine_vector_store_strategy(
        self, origin_file_path: str
    ) -> Tuple[str, bool]:
        """
        æ ¹æ®ä»»åŠ¡IDç¡®å®šå‘é‡å­˜å‚¨ç­–ç•¥ï¼šè¿”å›ç›®æ ‡ç´¢å¼•IDå’Œæ˜¯å¦éœ€è¦æ›´æ–°

        Returns:
            Tuple[str, bool]: (target_index_id, should_update)
        """
        if origin_file_path:
            # æ ¹æ®æ–‡ä»¶MD5æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ–‡ä»¶çš„ç´¢å¼•
            try:
                db = SessionLocal()
                try:
                    existing_index = IndexDAO.get_index_by_origin_file_path(
                        db, origin_file_path
                    )
                    if existing_index:
                        # æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œä½¿ç”¨ç°æœ‰çš„index_idå’Œå¯¹åº”çš„è¡¨
                        target_index_id = existing_index.index_id
                        logger.info(
                            f"Found existing index for origin_file_path: {origin_file_path}, using existing index_id: {target_index_id}"
                        )
                        return target_index_id, True
                    else:
                        # æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œä½¿ç”¨å½“å‰origin_file_pathåˆ›å»ºæ–°çš„
                        logger.info(
                            f"No existing index found for origin_file_path: {origin_file_path}, creating new index with id: {origin_file_path}"
                        )
                        return str(uuid.uuid4()), False
                finally:
                    db.close()
            except Exception as e:
                logger.warning(
                    f"Error checking existing index by origin_file_path: {e}, using current origin_file_path: {origin_file_path}"
                )
                return str(uuid.uuid4()), False  # ä¿æŒè¿”å›origin_file_path
        else:
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶ä¿¡æ¯ï¼Œä½¿ç”¨å½“å‰origin_file_path
            logger.info(
                f"No file MD5 available, using current origin_file_path: {origin_file_path}"
            )
            return str(uuid.uuid4()), False

    def _create_or_update_vector_store(
        self,
        nodes: List[Any],
        origin_file_path: str,
        all_doc_ids: List[str],
    ) -> Tuple[Any, Any, str]:
        """åˆ›å»ºå‘é‡å­˜å‚¨

        Returns:
            Tuple[Any, Any, str]: (vector_store, index, actual_index_id)
        """
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒæ–‡ä»¶çš„ç´¢å¼•ï¼Œå¹¶ç¡®å®šä½¿ç”¨çš„ç´¢å¼•IDå’Œè¡¨å
        target_index_id, should_update = self._determine_vector_store_strategy(
            origin_file_path
        )

        # åˆ›å»ºPostgreSQLå‘é‡å­˜å‚¨
        logger.info(
            f"Creating PostgreSQL vector store with dimension {settings.VECTOR_DIM}"
        )
        start_time = time.time()
        # åˆ›å»ºPostgreSQLå‘é‡å­˜å‚¨æ„å»ºå™¨ï¼ˆä½¿ç”¨ç¡®å®šçš„ç´¢å¼•IDï¼‰
        postgres_builder = create_postgres_vector_store_builder(
            host=settings.POSTGRES_HOST,
            port=settings.POSTGRES_PORT,
            database=settings.POSTGRES_DATABASE,
            user=settings.POSTGRES_USER,
            password=settings.POSTGRES_PASSWORD,
            table_name=f"{settings.POSTGRES_TABLE_NAME}_{target_index_id.replace('-', '_')}",
            embed_dim=settings.VECTOR_DIM,
        )

        if should_update:
            logger.info(f"Updating existing vector store with new data")
            index = postgres_builder.update_index_with_nodes(
                nodes, self.embed_model, all_doc_ids
            )
            vector_store = index.storage_context.vector_store
        else:
            logger.info(f"Creating new vector store")
            # ä½¿ç”¨postgres_builderçš„ç»Ÿä¸€å­˜å‚¨ä¸Šä¸‹æ–‡ç®¡ç†æ¥åˆ›å»ºç´¢å¼•
            logger.info(f"Creating vector store index with {len(nodes)} nodes using unified storage context")
            start_time = time.time()
            index = postgres_builder.create_index_from_nodes(nodes, self.embed_model)
            vector_store = index.storage_context.vector_store

        index_time = time.time() - start_time
        logger.info(f"Vector store index processed in {index_time:.2f}s")

        # è¿”å›å®é™…ä½¿ç”¨çš„ç´¢å¼•ID
        return vector_store, index, target_index_id

    def _map_page_info_to_nodes(self, nodes: List[Any]) -> List[Any]:
        """å°†é¡µç ä¿¡æ¯æ˜ å°„åˆ°æ–‡æ¡£åˆ‡ç‰‡èŠ‚ç‚¹

        è¿™ä¸ªæ–¹æ³•å°è¯•å°†content_list.jsonä¸­çš„é¡µç ä¿¡æ¯æ˜ å°„åˆ°æ¯ä¸ªæ–‡æ¡£åˆ‡ç‰‡çš„å…ƒæ•°æ®ä¸­ï¼Œ
        ä½¿å¾—åœ¨æ£€ç´¢æ—¶å¯ä»¥çŸ¥é“æ¯ä¸ªåˆ‡ç‰‡æ¥è‡ªåŸå§‹æ–‡æ¡£çš„å“ªä¸€é¡µã€‚

        Args:
            nodes: æ–‡æ¡£åˆ‡ç‰‡èŠ‚ç‚¹åˆ—è¡¨

        Returns:
            æ·»åŠ äº†é¡µç ä¿¡æ¯çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        from pathlib import Path
        from dao.task_dao import TaskDAO
        from utils.document_utils import truncate_filename
        from config import settings

        # æŒ‰æ–‡æ¡£åˆ†ç»„èŠ‚ç‚¹
        nodes_by_doc = {}
        for node in nodes:
            if not hasattr(node, "metadata"):
                continue

            doc_path = node.metadata.get("original_file_path")  # æ‹¿åˆ°å‘é‡åŒ–çš„æºæ–‡ä»¶
            if not doc_path:
                continue

            if doc_path not in nodes_by_doc:
                nodes_by_doc[doc_path] = []
            nodes_by_doc[doc_path].append(node)

        # åˆå§‹åŒ–TaskDAO
        task_dao = TaskDAO()

        # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„èŠ‚ç‚¹
        for doc_path, doc_nodes in nodes_by_doc.items():
            # é€šè¿‡æ–‡ä»¶è·¯å¾„æŸ¥è¯¢è§£æä»»åŠ¡
            parse_task = task_dao.get_parse_task_by_file_path(doc_path)
            if not parse_task:
                logger.warning(f"No parse task found for document: {doc_path}")
                continue

            # æ„å»ºæ­£ç¡®çš„content_list.jsonè·¯å¾„
            output_dir = os.path.join(settings.KNOWLEDGE_BASE_DIR, "outputs", parse_task.task_id)
            
            # è·å–æ–‡ä»¶åå¹¶æ„å»ºcontent_listè·¯å¾„
            doc_file = Path(doc_path)
            base_file_name, _ = os.path.splitext(doc_file.name)
            truncated_base_name = truncate_filename(base_file_name, max_length=60, preserve_extension=False)
            content_list_path = Path(output_dir) / f"{truncated_base_name}_content_list.json"

            if not content_list_path.exists():
                logger.warning(f"No content_list.json found for document: {doc_path} at {content_list_path}")
                continue

            try:
                with open(content_list_path, "r", encoding="utf-8") as f:
                    content_list = json.load(f)
            except Exception as e:
                logger.error(
                    f"Failed to load content_list from {content_list_path}: {e}"
                )
                continue

            if not isinstance(content_list, list) or not content_list:
                continue

            # åˆ›å»ºæ–‡æœ¬åˆ°é¡µç çš„æ˜ å°„
            text_to_page = {}
            for item in content_list:
                if item.get("type") == "text" and "text" in item and "page_idx" in item:
                    text = item["text"]
                    page_idx = item["page_idx"]
                    text_to_page[text] = page_idx

            # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…é¡µç 
            for node in doc_nodes:
                if not hasattr(node, "text") or not node.text:
                    continue

                # å°è¯•ç›´æ¥åŒ¹é…
                if node.text in text_to_page:
                    node.metadata["page_idx"] = text_to_page[node.text]
                    continue

                # å°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆæŸ¥æ‰¾èŠ‚ç‚¹æ–‡æœ¬ä¸­åŒ…å«çš„æœ€é•¿content_listæ–‡æœ¬ï¼‰
                best_match = None
                best_match_length = 0
                for text, page_idx in text_to_page.items():
                    if text in node.text and len(text) > best_match_length:
                        best_match = text
                        best_match_length = len(text)

                if best_match:
                    node.metadata["page_idx"] = text_to_page[best_match]
                    continue

                # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œå°è¯•åå‘åŒ¹é…ï¼ˆæŸ¥æ‰¾åŒ…å«èŠ‚ç‚¹æ–‡æœ¬çš„æœ€çŸ­content_listæ–‡æœ¬ï¼‰
                best_match = None
                best_match_length = float("inf")
                for text, page_idx in text_to_page.items():
                    if node.text in text and len(text) < best_match_length:
                        best_match = text
                        best_match_length = len(text)

                if best_match:
                    node.metadata["page_idx"] = text_to_page[best_match]

            # ç»Ÿè®¡æ·»åŠ äº†é¡µç çš„èŠ‚ç‚¹æ•°é‡
            nodes_with_page = sum(
                1 for node in doc_nodes if "page_idx" in node.metadata
            )
            logger.info(
                f"Added page information to {nodes_with_page}/{len(doc_nodes)} nodes for document: {doc_path}"
            )

        return nodes

    def _detect_global_circular_references(self, nodes: List[Any]):
        """æ£€æµ‹èŠ‚ç‚¹é—´çš„å…¨å±€å¾ªç¯å¼•ç”¨"""
        logger.info(f"ğŸ” å¼€å§‹æ£€æµ‹ {len(nodes)} ä¸ªèŠ‚ç‚¹çš„å¾ªç¯å¼•ç”¨...")
        
        # åˆ›å»ºèŠ‚ç‚¹IDåˆ°èŠ‚ç‚¹çš„æ˜ å°„
        node_map = {}
        for i, node in enumerate(nodes):
            if hasattr(node, 'node_id'):
                node_map[node.node_id] = (i, node)
            else:
                logger.warning(f"èŠ‚ç‚¹ {i} æ²¡æœ‰node_idå±æ€§")
        
        logger.info(f"ğŸ“Š åˆ›å»ºäº† {len(node_map)} ä¸ªèŠ‚ç‚¹çš„IDæ˜ å°„")
        
        # æ£€æŸ¥æ¯ä¸ªèŠ‚ç‚¹çš„å…ƒæ•°æ®ä¸­æ˜¯å¦å¼•ç”¨äº†å…¶ä»–èŠ‚ç‚¹
        circular_refs = []
        node_references = []
        
        for i, node in enumerate(nodes):
            if not hasattr(node, 'metadata') or not node.metadata:
                continue
                
            current_node_id = getattr(node, 'node_id', f'node_{i}')
            
            # é€’å½’æ£€æŸ¥å…ƒæ•°æ®ä¸­çš„å¼•ç”¨
            refs = self._find_node_references_in_object(node.metadata, node_map, current_node_id, path="metadata")
            if refs:
                node_references.extend(refs)
        
        # åˆ†æå¼•ç”¨å…³ç³»
        if node_references:
            for ref in node_references:
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¾ªç¯å¼•ç”¨
                if ref['source'] == ref['target']:
                    circular_refs.append(ref)
                    logger.error(f"ğŸ”„ å‘ç°è‡ªå¼•ç”¨å¾ªç¯: {ref['source']} -> {ref['target']}")
        
        if circular_refs:
            logger.error(f"âŒ å‘ç° {len(circular_refs)} ä¸ªå¾ªç¯å¼•ç”¨!")
            for ref in circular_refs:
                logger.error(f"  ğŸ”„ å¾ªç¯å¼•ç”¨: {ref['source']} (è·¯å¾„: {ref['path']})")
        else:
            logger.info("âœ… æœªå‘ç°ç›´æ¥çš„å¾ªç¯å¼•ç”¨")
        
        return circular_refs, node_references
    
    def _find_node_references_in_object(self, obj, node_map: dict, source_node_id: str, path: str = "", visited=None):
        """é€’å½’æŸ¥æ‰¾å¯¹è±¡ä¸­çš„èŠ‚ç‚¹å¼•ç”¨"""
        if visited is None:
            visited = set()
        
        # é˜²æ­¢æ— é™é€’å½’
        obj_id = id(obj)
        if obj_id in visited:
            return []
        visited.add(obj_id)
        
        references = []
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯èŠ‚ç‚¹å¯¹è±¡
            if hasattr(obj, 'node_id') and obj.node_id in node_map:
                references.append({
                    'source': source_node_id,
                    'target': obj.node_id,
                    'path': path,
                    'object_type': type(obj).__name__
                })
            
            # é€’å½’æ£€æŸ¥å­—å…¸
            elif isinstance(obj, dict):
                for key, value in obj.items():
                    new_path = f"{path}.{key}" if path else key
                    refs = self._find_node_references_in_object(value, node_map, source_node_id, new_path, visited.copy())
                    references.extend(refs)
            
            # é€’å½’æ£€æŸ¥åˆ—è¡¨
            elif isinstance(obj, (list, tuple)):
                for i, item in enumerate(obj):
                    new_path = f"{path}[{i}]" if path else f"[{i}]"
                    refs = self._find_node_references_in_object(item, node_map, source_node_id, new_path, visited.copy())
                    references.extend(refs)
            
            # æ£€æŸ¥å¯¹è±¡å±æ€§
            elif hasattr(obj, '__dict__'):
                for attr_name, attr_value in obj.__dict__.items():
                    new_path = f"{path}.{attr_name}" if path else attr_name
                    refs = self._find_node_references_in_object(attr_value, node_map, source_node_id, new_path, visited.copy())
                    references.extend(refs)
        
        except Exception as e:
            # å¿½ç•¥æ£€æŸ¥è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œé¿å…å½±å“ä¸»æµç¨‹
            pass
        
        return references

    def _save_nodes_text_to_file(self, nodes: List[Any], task_id: str):
        """å°†æ‰€æœ‰èŠ‚ç‚¹çš„æ–‡æœ¬å†…å®¹ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ç”¨äºè°ƒè¯•"""
        
        # é¢„å…ˆæ£€æŸ¥èŠ‚ç‚¹é—´çš„å¾ªç¯å¼•ç”¨
        circular_refs, node_references = self._detect_global_circular_references(nodes)
        
        try:
            # åˆ›å»ºè°ƒè¯•è¾“å‡ºç›®å½•
            debug_dir = Path("debug_nodes")
            debug_dir.mkdir(exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = debug_dir / f"nodes_text_{task_id}_{timestamp}.txt"

            with open(output_file, "w", encoding="utf-8") as f:
                f.write(f"èŠ‚ç‚¹æ–‡æœ¬è°ƒè¯•è¾“å‡º\n")
                f.write(f"ä»»åŠ¡ID: {task_id}\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\n")
                f.write(f"æ€»èŠ‚ç‚¹æ•°: {len(nodes)}\n")
                
                # å†™å…¥å¾ªç¯å¼•ç”¨æ£€æµ‹ç»“æœ
                f.write("\nğŸ” å¾ªç¯å¼•ç”¨æ£€æµ‹ç»“æœ:\n")
                f.write("-" * 40 + "\n")
                if circular_refs:
                    f.write(f"âŒ å‘ç° {len(circular_refs)} ä¸ªå¾ªç¯å¼•ç”¨:\n")
                    for ref in circular_refs:
                        f.write(f"  ğŸ”„ {ref['source']} -> {ref['target']} (è·¯å¾„: {ref['path']})\n")
                else:
                    f.write("âœ… æœªå‘ç°å¾ªç¯å¼•ç”¨\n")
                
                if node_references:
                    f.write(f"\nğŸ”— èŠ‚ç‚¹å¼•ç”¨å…³ç³» (å…± {len(node_references)} ä¸ª):\n")
                    for ref in node_references:
                        f.write(f"  - {ref['source']} -> {ref['target']} (è·¯å¾„: {ref['path']}, ç±»å‹: {ref['object_type']})\n")
                else:
                    f.write("\nğŸ”— æœªå‘ç°èŠ‚ç‚¹é—´å¼•ç”¨å…³ç³»\n")
                
                f.write("\n" + "=" * 80 + "\n\n")
                f.flush()  # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº

                for i, node in enumerate(nodes):
                    try:
                        logger.debug(f"ğŸ“„ å¤„ç†èŠ‚ç‚¹ {i+1}/{len(nodes)}")
                        f.write(f"èŠ‚ç‚¹ {i+1}/{len(nodes)}:\n")
                        f.write("-" * 40 + "\n")

                        # è¾“å‡ºèŠ‚ç‚¹ID
                        if hasattr(node, "node_id"):
                            f.write(f"èŠ‚ç‚¹ID: {node.node_id}\n")
                            logger.debug(f"èŠ‚ç‚¹ID: {node.node_id}")
                        else:
                            f.write("èŠ‚ç‚¹ID: [æ— IDå±æ€§]\n")
                            logger.debug("èŠ‚ç‚¹æ— IDå±æ€§")

                        # è¾“å‡ºèŠ‚ç‚¹ç±»å‹
                        f.write(f"èŠ‚ç‚¹ç±»å‹: {type(node).__name__}\n")
                        logger.debug(f"èŠ‚ç‚¹ç±»å‹: {type(node).__name__}")

                        # è¾“å‡ºå…ƒæ•°æ®ï¼ˆå¢å¼ºå¾ªç¯å¼•ç”¨æ£€æµ‹ï¼‰
                        if hasattr(node, "metadata") and node.metadata:
                            try:
                                # å°è¯•ç›´æ¥åºåˆ—åŒ–
                                metadata_str = json.dumps(node.metadata, ensure_ascii=False, indent=2)
                                f.write(f"å…ƒæ•°æ®: {metadata_str}\n")
                                logger.debug(f"å…ƒæ•°æ®é•¿åº¦: {len(metadata_str)}å­—ç¬¦")
                            except Exception as meta_e:
                                f.write(f"å…ƒæ•°æ®: [åºåˆ—åŒ–å¤±è´¥: {meta_e}]\n")
                                
                                # è¯¦ç»†åˆ†æå…ƒæ•°æ®ç»“æ„
                                f.write("ğŸ” å…ƒæ•°æ®è¯¦ç»†åˆ†æ:\n")
                                
                                try:
                                    # åˆ†æå…ƒæ•°æ®çš„é”®å€¼å¯¹
                                    f.write(f"  - å…ƒæ•°æ®ç±»å‹: {type(node.metadata).__name__}\n")
                                    f.write(f"  - å…ƒæ•°æ®é”®æ•°é‡: {len(node.metadata) if hasattr(node.metadata, '__len__') else 'N/A'}\n")
                                    
                                    if isinstance(node.metadata, dict):
                                        f.write("  - å…ƒæ•°æ®é”®åˆ—è¡¨:\n")
                                        for key in node.metadata.keys():
                                            f.write(f"    * {key}: {type(node.metadata[key]).__name__}\n")
                                            
                                            # æ£€æŸ¥æ¯ä¸ªå€¼æ˜¯å¦å¯åºåˆ—åŒ–
                                            try:
                                                json.dumps(node.metadata[key], ensure_ascii=False)
                                                f.write(f"      âœ… å¯åºåˆ—åŒ–\n")
                                            except Exception as key_e:
                                                f.write(f"      âŒ ä¸å¯åºåˆ—åŒ–: {key_e}\n")
                                                
                                                # è¿›ä¸€æ­¥åˆ†æä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
                                                obj = node.metadata[key]
                                                f.write(f"        - å¯¹è±¡ç±»å‹: {type(obj).__name__}\n")
                                                f.write(f"        - å¯¹è±¡æ¨¡å—: {type(obj).__module__}\n")
                                                
                                                # æ£€æŸ¥æ˜¯å¦æ˜¯èŠ‚ç‚¹å¯¹è±¡
                                                if hasattr(obj, 'node_id'):
                                                    f.write(f"        - ğŸ”— æ£€æµ‹åˆ°èŠ‚ç‚¹å¼•ç”¨: {obj.node_id}\n")
                                                
                                                # æ£€æŸ¥æ˜¯å¦æœ‰å¾ªç¯å¼•ç”¨
                                                if obj is node:
                                                    f.write(f"        - ğŸ”„ æ£€æµ‹åˆ°è‡ªå¼•ç”¨å¾ªç¯!\n")
                                                    logger.error(f"å‘ç°è‡ªå¼•ç”¨å¾ªç¯: {key}")
                                                elif hasattr(obj, '__dict__'):
                                                    f.write(f"        - å¯¹è±¡å±æ€§æ•°é‡: {len(obj.__dict__)}\n")
                                                    for attr_name, attr_value in obj.__dict__.items():
                                                        if attr_value is node:
                                                            f.write(f"        - ğŸ”„ æ£€æµ‹åˆ°å¾ªç¯å¼•ç”¨: {attr_name} -> å½“å‰èŠ‚ç‚¹\n")
                                                            logger.error(f"å‘ç°å¾ªç¯å¼•ç”¨: {key}.{attr_name} -> å½“å‰èŠ‚ç‚¹")
                                                        elif hasattr(attr_value, 'node_id') and hasattr(node, 'node_id') and attr_value.node_id == node.node_id:
                                                            f.write(f"        - ğŸ”„ æ£€æµ‹åˆ°èŠ‚ç‚¹IDå¾ªç¯å¼•ç”¨: {attr_name}\n")
                                                            logger.error(f"å‘ç°èŠ‚ç‚¹IDå¾ªç¯å¼•ç”¨: {key}.{attr_name}")
                                    
                                    # å°è¯•åˆ›å»ºå®‰å…¨çš„å…ƒæ•°æ®å‰¯æœ¬
                                    f.write("  - å°è¯•åˆ›å»ºå®‰å…¨çš„å…ƒæ•°æ®å‰¯æœ¬:\n")
                                    safe_metadata = {}
                                    for key, value in node.metadata.items():
                                        try:
                                            json.dumps(value, ensure_ascii=False)
                                            safe_metadata[key] = value
                                            f.write(f"    âœ… {key}: å·²åŒ…å«\n")
                                        except:
                                            safe_metadata[key] = f"<ä¸å¯åºåˆ—åŒ–çš„{type(value).__name__}å¯¹è±¡>"
                                            f.write(f"    âš ï¸ {key}: å·²æ›¿æ¢ä¸ºå ä½ç¬¦\n")
                                    
                                    safe_metadata_str = json.dumps(safe_metadata, ensure_ascii=False, indent=2)
                                    f.write(f"  - å®‰å…¨å…ƒæ•°æ®: {safe_metadata_str}\n")
                                    
                                except Exception as analysis_e:
                                    f.write(f"  - å…ƒæ•°æ®åˆ†æå¤±è´¥: {analysis_e}\n")
                                    logger.error(f"å…ƒæ•°æ®åˆ†æå¤±è´¥: {analysis_e}")
                        else:
                            f.write("å…ƒæ•°æ®: [æ— å…ƒæ•°æ®æˆ–ä¸ºç©º]\n")
                            logger.debug("èŠ‚ç‚¹æ— å…ƒæ•°æ®")

                        # è¾“å‡ºæ–‡æœ¬å†…å®¹
                        text_content = None
                        if hasattr(node, "text"):
                            text_content = node.text if node.text else "[ç©ºæ–‡æœ¬]"
                            f.write(f"æ–‡æœ¬å†…å®¹ (é•¿åº¦: {len(text_content)}å­—ç¬¦):\n")
                            f.write(f"{text_content}\n")
                            logger.debug(f"æ–‡æœ¬å†…å®¹é•¿åº¦: {len(text_content)}å­—ç¬¦")
                        elif hasattr(node, "get_content"):
                            try:
                                text_content = node.get_content()
                                text_content = text_content if text_content else "[ç©ºæ–‡æœ¬]"
                                f.write(f"æ–‡æœ¬å†…å®¹ (é€šè¿‡get_contentè·å–ï¼Œé•¿åº¦: {len(text_content)}å­—ç¬¦):\n")
                                f.write(f"{text_content}\n")
                                logger.debug(f"é€šè¿‡get_contentè·å–æ–‡æœ¬ï¼Œé•¿åº¦: {len(text_content)}å­—ç¬¦")
                            except Exception as content_e:
                                f.write(f"æ–‡æœ¬å†…å®¹: [get_contentè°ƒç”¨å¤±è´¥: {content_e}]\n")
                        else:
                            f.write("æ–‡æœ¬å†…å®¹: [æ— æ–‡æœ¬å±æ€§]\n")
                            logger.debug("èŠ‚ç‚¹æ— æ–‡æœ¬å±æ€§")

                        # è¾“å‡ºåµŒå…¥å‘é‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        if hasattr(node, "embedding") and node.embedding:
                            f.write(f"åµŒå…¥å‘é‡: å·²ç”Ÿæˆ (ç»´åº¦: {len(node.embedding)})\n")
                            logger.debug(f"åµŒå…¥å‘é‡ç»´åº¦: {len(node.embedding)}")
                        else:
                            f.write("åµŒå…¥å‘é‡: æœªç”Ÿæˆ\n")
                            logger.debug("èŠ‚ç‚¹æ— åµŒå…¥å‘é‡")

                        f.write("\n" + "=" * 80 + "\n\n")
                        
                        # æ¯10ä¸ªèŠ‚ç‚¹åˆ·æ–°ä¸€æ¬¡ç¼“å†²åŒº
                        if (i + 1) % 10 == 0:
                            f.flush()
                            logger.debug(f"å·²å¤„ç† {i+1} ä¸ªèŠ‚ç‚¹ï¼Œç¼“å†²åŒºå·²åˆ·æ–°")
                            
                    except Exception as node_e:
                        error_msg = f"å¤„ç†èŠ‚ç‚¹ {i+1} æ—¶å‡ºé”™: {node_e}"
                        f.write(f"é”™è¯¯: {error_msg}\n")
                        f.write("\n" + "=" * 80 + "\n\n")
                        logger.error(error_msg)
                        continue

                f.flush()  # æœ€ç»ˆåˆ·æ–°

            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦æ­£ç¡®å†™å…¥
            if output_file.exists():
                file_size = output_file.stat().st_size
            else:
                logger.error("âŒ è°ƒè¯•æ–‡ä»¶æœªæˆåŠŸåˆ›å»º")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜èŠ‚ç‚¹æ–‡æœ¬åˆ°æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    def _save_index_info_to_db(
        self,
        origin_file_path: str,
        index_id: str,
        index_description: Optional[str] = None,
        document_count: Optional[int] = None,
        node_count: Optional[int] = None,
        vector_dimension: Optional[int] = None,
        processing_config: Optional[Dict[str, Any]] = None,
    ):
        """ä¿å­˜ç´¢å¼•ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:

            # åˆ›å»ºæ•°æ®åº“ä¼šè¯
            db = SessionLocal()
            try:
                existing_index = IndexDAO.get_index_by_origin_file_path(
                    db, origin_file_path
                )

                if existing_index:
                    # æ›´æ–°ç°æœ‰ç´¢å¼•ä¿¡æ¯
                    if index_description is not None:
                        existing_index.index_description = index_description
                    if document_count is not None:
                        existing_index.document_count = document_count
                    if node_count is not None:
                        existing_index.node_count = node_count
                    if vector_dimension is not None:
                        existing_index.vector_dimension = vector_dimension
                    if processing_config is not None:
                        existing_index.processing_config = processing_config

                    # æ›´æ–°ç´¢å¼•IDï¼ˆå¦‚æœä¸åŒï¼‰
                    if existing_index.index_id != index_id:
                        existing_index.index_id = index_id

                    db.commit()
                    db.refresh(existing_index)
                    logger.info(
                        f"Updated existing index info for origin_file_path: {origin_file_path}, index ID: {index_id}"
                    )
                else:
                    # åˆ›å»ºæ–°ç´¢å¼•ä¿¡æ¯
                    create_params = {
                        "index_id": index_id,
                        "index_description": index_description,
                        "document_count": document_count,
                        "node_count": node_count,
                        "vector_dimension": vector_dimension,
                        "processing_config": processing_config,
                        "origin_file_path": origin_file_path,
                    }

                    IndexDAO.create_index(db, **create_params)
                    logger.info(f"Created new index info in database: {index_id}")
            finally:
                db.close()
        except Exception as e:
            logger.error(f"Error saving index info to database: {e}")
            raise

    def _generate_auto_description(
        self, index: Any, config: Dict[str, Any]
    ) -> Optional[str]:
        """è‡ªåŠ¨ç”Ÿæˆç´¢å¼•æè¿°"""
        try:
            from .auto_index_description_generator import AutoIndexDescriptionGenerator

            # åˆ›å»ºæè¿°ç”Ÿæˆå™¨
            description_generator = AutoIndexDescriptionGenerator(llm=self.llm)

            # ç”Ÿæˆæè¿°
            sample_size = config.get("description_sample_size", 50)
            description = description_generator.generate_description(index, sample_size)

            logger.info("Automatic index description generated successfully")
            return description

        except Exception as e:
            logger.error(f"Error generating automatic index description: {str(e)}")
            return None

    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        task = self.tasks.get(task_id)
        return task.to_dict() if task else None

    def get_all_tasks(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
        return [task.to_dict() for task in self.tasks.values()]

    def create_vector_store_task_from_parse_task(
        self, task_id: str, config: Dict[str, Any]
    ) -> str:
        """ä»è§£æä»»åŠ¡åˆ›å»ºå‘é‡å­˜å‚¨æ„å»ºä»»åŠ¡"""
        # è·å–è§£æä»»åŠ¡ä¿¡æ¯
        parse_task = self.task_dao.get_parse_task(task_id)
        if not parse_task:
            raise ValueError(f"Parse task not found: {task_id}")

        if parse_task.status != TaskStatus.COMPLETED:
            raise ValueError(
                f"Parse task {task_id} is not completed. Current status: {parse_task.status}"
            )

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒè§£æä»»åŠ¡IDçš„å‘é‡ä»»åŠ¡
        existing_vector_tasks = self.task_dao.get_vector_tasks_by_parse_task(task_id)
        if existing_vector_tasks:
            # æŸ¥æ‰¾érunningçŠ¶æ€çš„ä»»åŠ¡
            for existing_task in existing_vector_tasks:
                if existing_task.status != TaskStatus.RUNNING:
                    logger.info(
                        f"Found existing vector store task {existing_task.task_id} for parse task {task_id} with status {existing_task.status}, executing vectorization"
                    )
                    # å°†ç°æœ‰ä»»åŠ¡æ·»åŠ åˆ°å†…å­˜ä¸­çš„ä»»åŠ¡å­—å…¸ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                    if existing_task.task_id not in self.tasks:
                        self.tasks[existing_task.task_id] = existing_task
                    
                    
                    # é‡æ–°æ‰§è¡Œå‘é‡åŒ–
                    existing_task.status = TaskStatus.PENDING
                    existing_task.progress = 0
                    existing_task.error = None
                    
                    # å¼‚æ­¥æ‰§è¡Œæ„å»º
                    asyncio.create_task(self._execute_build_task(existing_task))
                    
                    logger.info(
                        f"Restarted vector store build task {existing_task.task_id} from parse task {task_id}"
                    )
                    return existing_task.task_id
            
            # å¦‚æœåªæœ‰runningçŠ¶æ€çš„ä»»åŠ¡ï¼Œè®°å½•æ—¥å¿—ä½†ç»§ç»­åˆ›å»ºæ–°ä»»åŠ¡
            running_tasks = [t for t in existing_vector_tasks if t.status == TaskStatus.RUNNING]
            if running_tasks:
                logger.info(
                    f"Found {len(running_tasks)} running vector store task(s) for parse task {task_id}, creating new task"
                )

        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸»ä»»åŠ¡ï¼ˆparent_task_idä¸ºNoneè¡¨ç¤ºä¸»ä»»åŠ¡ï¼‰
        is_main_task = parse_task.parent_task_id is None
        output_dir = None

        if is_main_task:
            # ä¸»ä»»åŠ¡ï¼ˆç›®å½•è§£æï¼‰æ— éœ€æ ¡éªŒoutput_directory
            # ä¸»ä»»åŠ¡é€šè¿‡å­ä»»åŠ¡æ¥å¤„ç†å…·ä½“æ–‡ä»¶ï¼Œè‡ªèº«ä¸ç›´æ¥äº§ç”Ÿè¾“å‡ºç›®å½•
            logger.info(
                f"Main task {task_id} detected, skipping output_directory validation"
            )
        else:
            # å­ä»»åŠ¡ï¼ˆå•æ–‡ä»¶è§£æï¼‰åŸºäºKNOWLEDGE_BASE_DIRå’Œtask_idæ„å»ºè¾“å‡ºç›®å½•
            from config import settings
            output_dir = os.path.join(settings.KNOWLEDGE_BASE_DIR, "outputs", task_id)

            # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(output_dir):
                raise ValueError(f"Output directory does not exist: {output_dir}")

            logger.info(f"Subtask {task_id} validated, output_directory: {output_dir}")

        # åˆ›å»ºå‘é‡å­˜å‚¨ä»»åŠ¡ID
        vector_task_id = str(uuid.uuid4())

        # åˆ›å»ºä»»åŠ¡æ•°æ®
        task_data = {
            "task_id": vector_task_id,
            "parse_task_id": task_id,
            "status": "PENDING",
            "progress": 0,
            "config": config,
            "processed_files": [],
            "total_files": 0,
        }

        # ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“å¹¶è·å–ä»»åŠ¡å¯¹è±¡
        task = self.task_dao.create_vector_store_task(task_data)
        if task:
            self.tasks[vector_task_id] = task

        # å¼‚æ­¥æ‰§è¡Œæ„å»º
        asyncio.create_task(self._execute_build_task(task))

        logger.info(
            f"Created vector store build task {vector_task_id} from parse task {task_id}, output dir: {output_dir}"
        )
        return vector_task_id

    def _update_task_in_db(self, task: VectorStoreTask):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€"""
        try:
            update_data = {
                "status": task.status,
                "progress": task.progress,
                "result": task.result,
                "error": task.error,
                "processed_files": task.processed_files,
                "total_files": task.total_files,
                "total_nodes": task.total_nodes,
                "config": task.config,
                "index_id": task.index_id,
            }

            if task.started_at:
                update_data["started_at"] = task.started_at
            if task.completed_at:
                update_data["completed_at"] = task.completed_at

            self.task_dao.update_vector_store_task(task.task_id, update_data)
            logger.debug(f"Vector store task {task.task_id} status updated in database")
        except Exception as e:
            logger.error(
                f"Failed to update vector store task {task.task_id} in database: {e}"
            )

    def cleanup_expired_tasks(self):
        """æ¸…ç†è¿‡æœŸä»»åŠ¡"""
        current_time = datetime.utcnow()
        expired_tasks = [
            task_id
            for task_id, task in self.tasks.items()
            if task.created_at
            and (current_time - task.created_at).total_seconds()
            > settings.TASK_EXPIRE_TIME
        ]

        for task_id in expired_tasks:
            del self.tasks[task_id]
            logger.info(f"Cleaned up expired vector store task: {task_id}")


# å…¨å±€æ„å»ºå™¨å®ä¾‹
vector_store_builder = VectorStoreBuilder()
