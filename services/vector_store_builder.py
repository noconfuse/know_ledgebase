import asyncio
import json
from tabnanny import verbose
import uuid
import time
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor

from llama_index.core import Document, VectorStoreIndex, StorageContext
from llama_index.core.node_parser import SentenceSplitter, HTMLNodeParser
from common.custom_markdown_node_parser import CustomMarkdownNodeParser as MarkdownNodeParser
from llama_index.core.extractors import (
    TitleExtractor,
    KeywordExtractor,
    SummaryExtractor,
    QuestionsAnsweredExtractor
)
from llama_index.core.ingestion import IngestionPipeline
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.vector_stores.postgres import PGVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.llms.openai_like import OpenAILike
import faiss

from config import settings
from services.document_parser import document_parser, TaskStatus
from common.postgres_vector_store import create_postgres_vector_store_builder

logger = logging.getLogger(__name__)

class VectorStoreTask:
    """å‘é‡æ•°æ®åº“æ„å»ºä»»åŠ¡"""
    def __init__(self, task_id: str, directory_path: str, config: Dict[str, Any]):
        self.task_id = task_id
        self.directory_path = directory_path
        self.config = config
        self.status = TaskStatus.PENDING
        self.progress = 0
        self.result = None
        self.error = None
        self.created_at = time.time()
        self.started_at = None
        self.completed_at = None
        self.processed_files = []
        self.total_files = 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "task_id": self.task_id,
            "directory_path": self.directory_path,
            "status": self.status,
            "progress": self.progress,
            "result": self.result,
            "error": self.error,
            "created_at": self.created_at,
            "started_at": self.started_at,
            "completed_at": self.completed_at,
            "processed_files": self.processed_files,
            "total_files": self.total_files,
            "index_description": self.config.get("index_description")
        }

class VectorStoreBuilder:
    """å‘é‡æ•°æ®åº“æ„å»ºå™¨"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self._setup_models()
            self.tasks: Dict[str, VectorStoreTask] = {}
            self.executor = ThreadPoolExecutor(max_workers=2)  # é™åˆ¶å¹¶å‘æ•°
            self._initialized = True
            logger.info("VectorStoreBuilder initialized")
    
    def _setup_models(self):
        """è®¾ç½®æ¨¡å‹"""
        try:
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.embed_model = HuggingFaceEmbedding(
                model_name=settings.EMBED_MODEL_PATH,
                device=settings.GPU_DEVICE if settings.USE_GPU else "cpu",
                trust_remote_code=True
            )
            
            # åˆå§‹åŒ–LLMï¼ˆç”¨äºæ‘˜è¦å’Œé—®ç­”ç”Ÿæˆï¼‰
            if settings.LLM_API_BASE and settings.LLM_API_KEY:
                # ä½¿ç”¨ç¬¬ä¸‰æ–¹API
                self.llm = OpenAILike(
                    api_base=settings.LLM_API_BASE,
                    api_key=settings.LLM_API_KEY,
                    api_version=settings.LLM_API_VERSION,
                    model=settings.LLM_MODEL_NAME,
                    max_tokens=settings.LLM_MAX_TOKENS,
                    temperature=settings.LLM_TEMPERATURE,
                    is_chat_model=True,
                )
                logger.info(f"Using external LLM API: {settings.LLM_API_BASE}")
            else:
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                self.llm = HuggingFaceLLM(
                    model_name=settings.LLM_MODEL_PATH,
                    device_map="auto" if settings.USE_GPU else None,
                    model_kwargs={
                        "torch_dtype": "auto",
                        "trust_remote_code": True
                    },
                    tokenizer_kwargs={
                        "trust_remote_code": True
                    },
                    max_new_tokens=settings.LLM_MAX_TOKENS,
                    temperature=settings.LLM_TEMPERATURE
                )
                logger.info(f"Using local LLM: {settings.LLM_MODEL_PATH}")
            
            logger.info("Models initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize models: {e}")
            raise
    
    async def build_vector_store(
        self, 
        directory_path: str, 
        config: Optional[Dict[str, Any]] = None,
        index_description: Optional[str] = None
    ) -> str:
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        
        # éªŒè¯ç›®å½•
        if not Path(directory_path).exists():
            raise FileNotFoundError(f"Directory not found: {directory_path}")
        
        if not Path(directory_path).is_dir():
            raise ValueError(f"Path is not a directory: {directory_path}")
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task_config = config or {
            "extract_keywords": True,
            "extract_summary": True,
            "generate_qa": True,
            "chunk_size": settings.CHUNK_SIZE,
            "chunk_overlap": settings.CHUNK_OVERLAP
        }
        
        # æ·»åŠ ç´¢å¼•æè¿°
        if index_description:
            task_config["index_description"] = index_description
        
        task = VectorStoreTask(task_id, directory_path, task_config)
        self.tasks[task_id] = task
        
        # å¼‚æ­¥æ‰§è¡Œæ„å»º
        asyncio.create_task(self._execute_build_task(task))
        
        logger.info(f"Created vector store build task {task_id} for directory: {directory_path}")
        return task_id
    
    async def _execute_build_task(self, task: VectorStoreTask):
        """æ‰§è¡Œæ„å»ºä»»åŠ¡"""
        try:
            task.status = TaskStatus.RUNNING
            task.started_at = time.time()
            task.progress = 5
            
            logger.info(f"Starting vector store build task {task.task_id}")
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ„å»º
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor, 
                self._build_vector_store_sync, 
                task
            )
            
            task.result = result
            task.status = TaskStatus.COMPLETED
            task.progress = 100
            task.completed_at = time.time()
            
            logger.info(f"Vector store build task {task.task_id} completed successfully")
            
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error = str(e)
            task.completed_at = time.time()
            logger.error(f"Vector store build task {task.task_id} failed: {e}")
    
    def _build_vector_store_sync(self, task: VectorStoreTask) -> Dict[str, Any]:
        """åŒæ­¥æ„å»ºå‘é‡æ•°æ®åº“"""
        try:
            # 1. æ”¶é›†æ–‡æ¡£æ–‡ä»¶
            task.progress = 10
            documents = self._collect_documents(task)
            task.total_files = len(documents)
            
            if not documents:
                raise ValueError("No valid documents found in directory")
            
            # 2. æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„å¤„ç†æ–‡æ¡£
            task.progress = 20
            all_nodes = []
            
            # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„
            docs_by_type = {}
            for doc in documents:
                logger.info(doc.metadata)
                file_type = doc.metadata.get('file_type', '.txt')
                if file_type not in docs_by_type:
                    docs_by_type[file_type] = []
                docs_by_type[file_type].append(doc)
            
            # 3. è®¾ç½®æå–å™¨
            task.progress = 30
            extractors = self._setup_extractors(task.config)
            
            # 4. ä¸ºæ¯ç§æ–‡ä»¶ç±»å‹åˆ›å»ºä¸“ç”¨å¤„ç†ç®¡é“
            task.progress = 40
            progress_step = 20 / len(docs_by_type)  # åœ¨40-60%ä¹‹é—´åˆ†é…è¿›åº¦
            
            for file_type, type_docs in docs_by_type.items():
                 logger.info(f"Processing {len(type_docs)} {file_type} files")
                 
                 # è·å–é€‚åˆè¯¥æ–‡ä»¶ç±»å‹çš„èŠ‚ç‚¹è§£æå™¨
                 logger.info(f"Getting node parser for file type: {file_type}")
                 node_parsers = self._get_node_parser_for_file_type(file_type, task.config)
                 
                 # ç¡®ä¿node_parsersæ˜¯åˆ—è¡¨æ ¼å¼
                 if not isinstance(node_parsers, list):
                     node_parsers = [node_parsers]
                 
                 logger.info(f"Node parsers configured: {[type(parser).__name__ for parser in node_parsers]}")
                 logger.info(f"Extractors configured: {[type(extractor).__name__ for extractor in extractors]}")
                 logger.info(f"Embed model: {type(self.embed_model).__name__}")
                 
                 # åˆ›å»ºè¯¥æ–‡ä»¶ç±»å‹çš„å¤„ç†ç®¡é“
                 logger.info(f"Creating ingestion pipeline for {file_type}")
                 pipeline = IngestionPipeline(
                     transformations=[
                         *node_parsers,
                         *extractors,
                         self.embed_model
                     ]
                 )
                 
                 # å¤„ç†è¯¥ç±»å‹çš„æ–‡æ¡£
                 import time
                 start_time = time.time()
                 logger.info(f"Starting pipeline processing for {len(type_docs)} {file_type} documents")
                 
                 # è®°å½•æ¯ä¸ªæ–‡æ¡£çš„å¤„ç†è¯¦æƒ…
                 for i, doc in enumerate(type_docs):
                     logger.info(f"Document {i+1}/{len(type_docs)}: {doc.metadata.get('file_name', 'unknown')} (size: {len(doc.text)} chars)")
                 
                 try:
                     # åˆ›å»ºè‡ªå®šä¹‰çš„ç®¡é“æ¥é€æ­¥å¤„ç†å¹¶è®°å½•æ¯ä¸ªé˜¶æ®µ
                     logger.info("Starting transformation pipeline execution...")
                     
                     # é€ä¸ªæ‰§è¡Œtransformationæ­¥éª¤å¹¶è®°å½•æ—¶é—´
                     current_docs = type_docs
                     for step_idx, transformation in enumerate(pipeline.transformations):
                         step_start = time.time()
                         transformation_name = type(transformation).__name__
                         logger.info(f"\n{'='*60}")
                         logger.info(f"Step {step_idx+1}/{len(pipeline.transformations)}: Starting {transformation_name}")
                         logger.info(f"Input: {len(current_docs) if hasattr(current_docs, '__len__') else 'N/A'} items")
                         
                         # å¦‚æœæ˜¯LLMç›¸å…³çš„extractorï¼Œè®°å½•æ›´è¯¦ç»†çš„ä¿¡æ¯
                         if 'Extractor' in transformation_name:
                             logger.info(f"ğŸ¤– LLM Extractor detected: {transformation_name}")
                             logger.info(f"ğŸ“ This step will make LLM API calls - detailed logs will follow")
                             if hasattr(transformation, 'llm'):
                                 logger.info(f"ğŸ”§ LLM model: {type(transformation.llm).__name__}")
                         
                         try:
                             if hasattr(transformation, 'transform'):
                                 # å¯¹äºnode parserå’Œå…¶ä»–transformer
                                 if step_idx == 0:  # ç¬¬ä¸€æ­¥ï¼Œè¾“å…¥æ˜¯documents
                                     current_docs = transformation.transform(current_docs)
                                 else:  # åç»­æ­¥éª¤ï¼Œè¾“å…¥æ˜¯nodes
                                     current_docs = transformation.transform(current_docs)
                             elif hasattr(transformation, '__call__'):
                                 # å¯¹äºembedding modelç­‰
                                 current_docs = transformation(current_docs)
                             
                             step_time = time.time() - step_start
                             logger.info(f"âœ… Step {step_idx+1} ({transformation_name}) completed successfully in {step_time:.2f}s")
                             logger.info(f"ğŸ“Š Output: {len(current_docs) if hasattr(current_docs, '__len__') else 'N/A'} items")
                             
                             # å¦‚æœæ˜¯LLMç›¸å…³çš„extractorï¼Œè®°å½•æˆåŠŸä¿¡æ¯
                             if 'Extractor' in transformation_name:
                                 logger.info(f"ğŸ‰ LLM Extractor {transformation_name} completed successfully")
                                 logger.info(f"â±ï¸  Total LLM processing time: {step_time:.2f}s")
                         
                         except Exception as e:
                             step_time = time.time() - step_start
                             logger.error(f"âŒ Step {step_idx+1} ({transformation_name}) FAILED after {step_time:.2f}s")
                             logger.error(f"ğŸš¨ Error in {transformation_name}: {str(e)}")
                             if 'Extractor' in transformation_name:
                                 logger.error(f"ğŸ’¥ LLM Extractor {transformation_name} failed - this is likely the timeout source!")
                                 logger.error(f"ğŸ” Check the LLM API calls above for timeout or connection issues")
                             raise
                         
                         logger.info(f"{'='*60}\n")
                     
                     type_nodes = current_docs
                     processing_time = time.time() - start_time
                     logger.info(f"Pipeline processing completed for {file_type} in {processing_time:.2f}s, generated {len(type_nodes)} nodes")
                     
                     all_nodes.extend(type_nodes)
                     logger.info(f"Total nodes accumulated: {len(all_nodes)}")
                     
                 except Exception as e:
                     processing_time = time.time() - start_time
                     logger.error(f"Pipeline processing failed for {file_type} after {processing_time:.2f}s: {e}")
                     logger.error(f"Error details: {str(e)}")
                     logger.error(f"Error occurred during pipeline execution, last successful step info available in logs above")
                     raise
                 
                 task.progress += progress_step
                 logger.info(f"Progress updated to {task.progress:.1f}%")
            
            # 5. åˆå¹¶æ‰€æœ‰èŠ‚ç‚¹
            task.progress = 60
            nodes = all_nodes
            logger.info(f"Merging nodes completed, total nodes: {len(nodes)}")
            
            # 5.5 ä¸ºèŠ‚ç‚¹æ·»åŠ é¡µç ä¿¡æ¯
            nodes = self._map_page_info_to_nodes(nodes)
            logger.info(f"Page information mapped to nodes, total nodes: {len(nodes)}")
            
            # 6. åˆ›å»ºå‘é‡å­˜å‚¨
            task.progress = 80
            logger.info(f"Starting vector store creation with {len(nodes)} nodes")
            start_time = time.time()
            
            try:
                vector_store, index = self._create_vector_store(nodes, task.task_id)
                creation_time = time.time() - start_time
                logger.info(f"Vector store creation completed in {creation_time:.2f}s")
            except Exception as e:
                creation_time = time.time() - start_time
                logger.error(f"Vector store creation failed after {creation_time:.2f}s: {e}")
                raise
            
            # 7. ä¿å­˜ç´¢å¼•
            task.progress = 95
            logger.info(f"Starting index save for task {task.task_id}")
            start_time = time.time()
            
            try:
                index_path = self._save_index(index, task.task_id)
                save_time = time.time() - start_time
                logger.info(f"Index saved successfully in {save_time:.2f}s to: {index_path}")
            except Exception as e:
                save_time = time.time() - start_time
                logger.error(f"Index save failed after {save_time:.2f}s: {e}")
                raise
            
            # 8. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            logger.info("Generating statistics")
            start_time = time.time()
            stats = self._generate_stats(documents, nodes, task)
            stats_time = time.time() - start_time
            logger.info(f"Statistics generated in {stats_time:.2f}s")
            
            # 9. ä¿å­˜ç´¢å¼•ä¿¡æ¯åˆ°æ•°æ®åº“
            logger.info("Saving index information to database")
            start_time = time.time()
            try:
                self._save_index_info_to_db(task.task_id, task.config.get("index_description"))
                db_time = time.time() - start_time
                logger.info(f"Index information saved to database in {db_time:.2f}s")
            except Exception as e:
                db_time = time.time() - start_time
                logger.error(f"Failed to save index information to database after {db_time:.2f}s: {e}")
                # ç»§ç»­æ‰§è¡Œï¼Œä¸å½±å“ç´¢å¼•åˆ›å»ºçš„ä¸»æµç¨‹
            
            result = {
                "index_id": task.task_id,
                "index_path": index_path,
                "document_count": len(documents),
                "node_count": len(nodes),
                "vector_dimension": settings.VECTOR_DIM,
                "stats": stats,
                "config": task.config
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Error building vector store: {e}")
            raise
    
    def _collect_documents(self, task: VectorStoreTask) -> List[Document]:
        """æ”¶é›†ç›®å½•ä¸­çš„æ–‡æ¡£"""
        documents = []
        directory = Path(task.directory_path)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯knowledge/outputsæ ¼å¼çš„ç›®å½•
        if self._is_knowledge_outputs_format(directory):
            return self._collect_from_knowledge_outputs(task, directory)
        else:
            return documents
    
    def _is_knowledge_outputs_format(self, directory: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯knowledge/outputsæ ¼å¼çš„ç›®å½•"""
        # æ£€æŸ¥ç›®å½•åæ˜¯å¦æ˜¯UUIDæ ¼å¼
        try:
            import uuid
            uuid.UUID(directory.name)
            # æ£€æŸ¥æ˜¯å¦åŒ…å«content.mdå’Œcontent_list.jsonæ–‡ä»¶
            content_file = directory / "content.md"
            content_list_file = directory / "content_list.json"
            return content_file.exists() and content_list_file.exists()
        except (ValueError, AttributeError):
            return False
    
    def _collect_from_knowledge_outputs(self, task: VectorStoreTask, directory: Path) -> List[Document]:
        """ä»knowledge/outputsæ ¼å¼çš„ç›®å½•æ”¶é›†æ–‡æ¡£"""
        documents = []
        
        try:
            # è¯»å–content.mdæ–‡ä»¶
            content_file = directory / "content.md"
            content_list_file = directory / "content_list.json"
            
            if content_file.exists() and content_list_file.exists():
                # è¯»å–markdownå†…å®¹
                content = content_file.read_text(encoding='utf-8')
                
                # è¯»å–content_list.jsonå…ƒæ•°æ®
                import json
                with open(content_list_file, 'r', encoding='utf-8') as f:
                    content_list_data = json.load(f)
                
                if content.strip():
                    # æ„å»ºå…ƒæ•°æ®
                    # åªä¿ç•™å¯¹æ£€ç´¢æœ‰ä»·å€¼çš„åŸºç¡€å…ƒæ•°æ®
                    metadata = {
                        "file_path": str(content_file),
                        "file_type": ".md"
                    }
                    
                    # ä»content_list.jsonæå–æœ‰ç”¨ä¿¡æ¯
                    if isinstance(content_list_data, list) and content_list_data:
                        # æå–é¡µç ä¿¡æ¯
                        page_indices = set()
                        for item in content_list_data:
                            if 'page_idx' in item:
                                page_indices.add(item['page_idx'])
                        
                        # åªåœ¨æœ‰é¡µç ä¿¡æ¯æ—¶æ‰æ·»åŠ é¡µç ç›¸å…³å…ƒæ•°æ®
                        if page_indices:
                            metadata['page_count'] = len(page_indices)
                            metadata['first_page'] = min(page_indices)
                            metadata['last_page'] = max(page_indices)
                        
                        # åªåœ¨æœ‰æ ‡é¢˜æ—¶æ‰æ·»åŠ æ ‡é¢˜å…ƒæ•°æ®
                        title_items = [item for item in content_list_data if item.get('type') == 'text' and item.get('text_level') == 1]
                        if title_items and title_items[0].get('text'):
                            metadata['title'] = title_items[0]['text']
                        
                        # æå–è¡¨æ ¼å’Œå›¾ç‰‡ä¿¡æ¯
                        tables = [item for item in content_list_data if item.get('type') == 'table']
                        images = [item for item in content_list_data if item.get('type') == 'image']
                        
                        # åªåœ¨æœ‰è¡¨æ ¼æˆ–å›¾ç‰‡æ—¶æ‰æ·»åŠ ç›¸å…³å…ƒæ•°æ®
                        if tables:
                            metadata['has_tables'] = True
                            metadata['tables_count'] = len(tables)
                        
                        if images:
                            metadata['has_images'] = True
                            metadata['images_count'] = len(images)
                    
                    doc = Document(
                        text=content,
                        metadata=metadata
                    )
                    documents.append(doc)
                    task.processed_files.append(str(content_file))
                    
                    logger.info(f"Successfully processed knowledge output: {directory.name}")
                    
        except Exception as e:
            logger.warning(f"Failed to process knowledge output directory {directory}: {e}")
        
        return documents
    
    def _load_metadata(self, file_path: Path) -> Dict[str, Any]:
        """åŠ è½½å¯¹åº”çš„JSONå…ƒæ•°æ®æ–‡ä»¶"""
        metadata = {}
        
        # æŸ¥æ‰¾å¯èƒ½çš„JSONå…ƒæ•°æ®æ–‡ä»¶
        json_patterns = [
            f"{file_path.stem}_content_list.json",
            f"{file_path.stem}_middle.json",
            f"{file_path.stem}_metadata.json"
        ]
        
        for pattern in json_patterns:
            json_path = file_path.parent / pattern
            if json_path.exists():
                try:
                    import json
                    with open(json_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                    
                    # æå–æœ‰ç”¨çš„å…ƒæ•°æ®
                    if isinstance(json_data, list) and json_data:
                        # å¦‚æœæ˜¯content_listæ ¼å¼ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯
                        text_items = [item for item in json_data if item.get('type') == 'text']
                        if text_items:
                            metadata['total_text_blocks'] = len(text_items)
                            metadata['has_structured_content'] = True
                            
                            # æå–é¡µé¢ä¿¡æ¯
                            pages = set(item.get('page_idx', 0) for item in json_data if 'page_idx' in item)
                            metadata['total_pages'] = len(pages)
                            
                            # æå–æ–‡æœ¬å±‚çº§ä¿¡æ¯
                            text_levels = [item.get('text_level') for item in text_items if 'text_level' in item]
                            if text_levels:
                                metadata['has_hierarchical_structure'] = True
                                metadata['max_text_level'] = max(text_levels)
                    
                    elif isinstance(json_data, dict):
                        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨éƒ¨åˆ†å­—æ®µ
                        for key in ['title', 'author', 'subject', 'creator', 'producer']:
                            if key in json_data:
                                metadata[key] = json_data[key]
                    
                    metadata['metadata_source'] = pattern
                    break
                    
                except Exception as e:
                    logger.warning(f"Failed to load metadata from {json_path}: {e}")
                    continue
        
        return metadata
    
    def _get_node_parser_for_file_type(self, file_type: str, config: Dict[str, Any]):
        """æ ¹æ®æ–‡ä»¶ç±»å‹è·å–åˆé€‚çš„èŠ‚ç‚¹è§£æå™¨"""
        chunk_size = config.get("chunk_size", settings.CHUNK_SIZE)
        chunk_overlap = config.get("chunk_overlap", settings.CHUNK_OVERLAP)
        
        if file_type == ".md":
            # Markdownæ–‡ä»¶ä½¿ç”¨MarkdownNodeParserï¼Œç„¶åé…åˆSentenceSplitter
            # MarkdownNodeParserä¸»è¦ç”¨äºè§£æmarkdownç»“æ„ï¼Œç„¶åç”¨SentenceSplitterè¿›è¡Œåˆ†å—
            return [
                MarkdownNodeParser.from_defaults(),
                SentenceSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
            ]
        elif file_type in [".html", ".htm"]:
            # HTMLæ–‡ä»¶ä½¿ç”¨HTMLNodeParserï¼Œç„¶åé…åˆSentenceSplitter
            return [
                HTMLNodeParser.from_defaults(),
                SentenceSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
            ]
        else:
            # æ–‡æœ¬æ–‡ä»¶ç›´æ¥ä½¿ç”¨SentenceSplitter
            return SentenceSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
    
    def _setup_extractors(self, config: Dict[str, Any]) -> List[Any]:
        """è®¾ç½®æå–å™¨"""
        extractors = []
        
        # åˆ›å»ºå¸¦æœ‰è¯¦ç»†æ—¥å¿—è®°å½•çš„LLMåŒ…è£…å™¨
        def create_logging_llm(llm, extractor_name):
            """
            åˆ›å»ºå¸¦æœ‰è¯¦ç»†æ—¥å¿—è®°å½•çš„LLMåŒ…è£…å™¨ï¼Œä½¿ç”¨LlamaIndexçš„CallbackManager
            """
            from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler
            import llama_index.core
            
            # è®¾ç½®å…¨å±€è°ƒè¯•å¤„ç†å™¨ä»¥è·å–è¯¦ç»†çš„LLMè°ƒç”¨ä¿¡æ¯
            llama_index.core.set_global_handler("simple")
            
            # åˆ›å»ºè‡ªå®šä¹‰çš„è°ƒè¯•å¤„ç†å™¨
            debug_handler = LlamaDebugHandler(print_trace_on_end=True)
            callback_manager = CallbackManager([debug_handler])
            
            # ä¸ºLLMè®¾ç½®callback manager
            if hasattr(llm, 'callback_manager'):
                llm.callback_manager = callback_manager
            
            logger.info(f"[{extractor_name}] LLM logging configured for: {type(llm).__name__} with detailed request/response tracking")
            return llm
        
        # å®šä¹‰ä¸­æ–‡æç¤ºè¯æ¨¡æ¿
        chinese_title_node_template = (
            "è¯·æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ¡æ–‡å†…å®¹ï¼Œæå–ä¸€ä¸ªç®€æ´å‡†ç¡®çš„æ ‡é¢˜ã€‚è¯·ä½¿ç”¨ä¸åŸæ–‡ç›¸åŒçš„è¯­è¨€å›ç­”ã€‚\n"
            "è¯·æå–æ¡æ–‡ç¼–å·å’Œæ¡æ–‡ä¸»é¢˜ï¼Œæ ¼å¼å¦‚ï¼šç¬¬Xæ¡ å…³äºXXXçš„è§„å®šã€‚\n"
            "æ–‡æœ¬å†…å®¹ï¼š\n"
            "{context_str}\n"
            "æ ‡é¢˜ï¼š"
        )
        
        chinese_title_combine_template = (
            "ä»¥ä¸‹æ˜¯ä»æ–‡æ¡£ä¸åŒéƒ¨åˆ†æå–çš„æ ‡é¢˜å€™é€‰ï¼š{context_str}\n"
            "è¯·æ ¹æ®è¿™äº›å€™é€‰æ ‡é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªæœ€èƒ½æ¦‚æ‹¬æ•´ä¸ªæ–‡æ¡£å†…å®¹çš„æ ‡é¢˜ã€‚è¯·ä½¿ç”¨ä¸åŸæ–‡ç›¸åŒçš„è¯­è¨€å›ç­”ã€‚\n"
            "æœ€ç»ˆæ ‡é¢˜ï¼š"
        )
        
        chinese_keyword_template = (
            "è¯·ä»ä»¥ä¸‹æ³•å¾‹æ¡æ–‡ä¸­æå– {max_keywords} ä¸ªæœ€é‡è¦çš„å…³é”®è¯ã€‚è¯·ä½¿ç”¨ä¸åŸæ–‡ç›¸åŒçš„è¯­è¨€å›ç­”ã€‚\n"
            "é‡ç‚¹æå–ï¼šæ³•å¾‹æ¦‚å¿µã€é€‚ç”¨èŒƒå›´ã€è´£ä»»ä¸»ä½“ã€å¤„ç½šæªæ–½ã€æ³•å¾‹æœ¯è¯­ç­‰ã€‚\n"
            "é¿å…ä½¿ç”¨åœç”¨è¯ã€‚\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›å…³é”®è¯ï¼š'å…³é”®è¯: <å…³é”®è¯1>, <å…³é”®è¯2>, <å…³é”®è¯3>...'\n"
        )
        
        chinese_summary_template = (
            "è¯·ä¸ºä»¥ä¸‹æ³•å¾‹æ¡æ–‡å†™ä¸€ä¸ªç®€æ´çš„æ‘˜è¦ã€‚è¯·ä½¿ç”¨ä¸åŸæ–‡ç›¸åŒçš„è¯­è¨€å›ç­”ã€‚\n"
            "è¯·æ¦‚æ‹¬æ¡æ–‡çš„é€‚ç”¨æƒ…å½¢ã€æ³•å¾‹åæœã€å…³é”®è¦ç´ ã€‚\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "æ‘˜è¦ï¼š"
        )
        
        chinese_qa_template = (
            "è¯·æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ¡æ–‡å†…å®¹ï¼Œç”Ÿæˆ {num_questions} ä¸ªè¿™æ®µæ–‡æœ¬å¯ä»¥å›ç­”çš„é—®é¢˜ã€‚è¯·ä½¿ç”¨ä¸åŸæ–‡ç›¸åŒçš„è¯­è¨€å›ç­”ã€‚\n"
            "é‡ç‚¹ç”Ÿæˆä»¥ä¸‹ç±»å‹é—®é¢˜ï¼šä»€ä¹ˆæƒ…å†µä¸‹é€‚ç”¨ã€è¿ååæœæ˜¯ä»€ä¹ˆã€é€‚ç”¨ä¸»ä½“æ˜¯è°ã€å¦‚ä½•æ‰§è¡Œç­‰ã€‚\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›é—®é¢˜ï¼š\n"
            "1. <é—®é¢˜1>\n"
            "2. <é—®é¢˜2>\n"
            "3. <é—®é¢˜3>\n"
            "4. <é—®é¢˜4>\n"
            "5. <é—®é¢˜5>\n"
        )
        
        # æ ‡é¢˜æå–å™¨
        logger.info("Setting up TitleExtractor with Chinese prompts")
        title_llm = create_logging_llm(self.llm, "TitleExtractor")
        extractors.append(TitleExtractor(
            llm=title_llm,
            node_template=chinese_title_node_template,
            combine_template=chinese_title_combine_template
        ))
        
        # å…³é”®è¯æå–å™¨
        if config.get("extract_keywords", True):
            logger.info("Setting up KeywordExtractor with Chinese prompts (8 keywords for legal docs)")
            keyword_llm = create_logging_llm(self.llm, "KeywordExtractor")
            extractors.append(
                KeywordExtractor(
                    llm=keyword_llm,
                    keywords=5,  # æå–8ä¸ªå…³é”®è¯ï¼ˆæ³•å¾‹æ–‡æ¡£ä¼˜åŒ–ï¼‰
                    prompt_template=chinese_keyword_template
                )
            )
        
        # æ‘˜è¦æå–å™¨
        if config.get("extract_summary", True):
            logger.info("Setting up SummaryExtractor with Chinese prompts (self summary for legal docs)")
            summary_llm = create_logging_llm(self.llm, "SummaryExtractor")
            extractors.append(
                SummaryExtractor(
                    llm=summary_llm,
                    summaries=["self"],  # æ³•å¾‹æ¡æ–‡ç‹¬ç«‹æ€§å¼ºï¼Œåªæå–å½“å‰æ–‡æœ¬æ‘˜è¦
                    prompt_template=chinese_summary_template
                )
            )
        
        # é—®ç­”å¯¹ç”Ÿæˆå™¨
        if config.get("generate_qa", True):
            logger.info("Setting up QuestionsAnsweredExtractor with Chinese prompts (5 questions for legal docs)")
            qa_llm = create_logging_llm(self.llm, "QuestionsAnsweredExtractor")
            extractors.append(
                QuestionsAnsweredExtractor(
                    llm=qa_llm,
                    questions=3,  # ç”Ÿæˆ3é—®ç­”å¯¹ï¼ˆæ³•å¾‹æ–‡æ¡£ä¼˜åŒ–ï¼‰
                    prompt_template=chinese_qa_template
                )
            )
        
        return extractors
    
    def _create_vector_store(self, nodes: List[Any], index_id: str) -> Tuple[Any, Any]:
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        import time
        
        if settings.VECTOR_STORE_TYPE == "postgres":
            # åˆ›å»ºPostgreSQLå‘é‡å­˜å‚¨
            logger.info(f"Creating PostgreSQL vector store with dimension {settings.VECTOR_DIM}")
            start_time = time.time()
            
            # åˆ›å»ºPostgreSQLå‘é‡å­˜å‚¨æ„å»ºå™¨
            postgres_builder = create_postgres_vector_store_builder(
                host=settings.POSTGRES_HOST,
                port=settings.POSTGRES_PORT,
                database=settings.POSTGRES_DATABASE,
                user=settings.POSTGRES_USER,
                password=settings.POSTGRES_PASSWORD,
                table_name=f"{settings.POSTGRES_TABLE_NAME}_{index_id.replace('-', '_')}",
                embed_dim=settings.VECTOR_DIM
            )
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            vector_store = postgres_builder.create_vector_store()
            store_time = time.time() - start_time
            logger.info(f"PostgreSQL vector store created in {store_time:.3f}s")
            
            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            logger.info("Creating storage context")
            start_time = time.time()
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            context_time = time.time() - start_time
            logger.info(f"Storage context created in {context_time:.3f}s")
            
            # åˆ›å»ºç´¢å¼•
            logger.info(f"Creating vector store index with {len(nodes)} nodes")
            start_time = time.time()
            index = VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                embed_model=self.embed_model
            )
            index_time = time.time() - start_time
            logger.info(f"Vector store index created in {index_time:.2f}s")
            
        else:
            # åˆ›å»ºFAISSç´¢å¼•ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
            logger.info(f"Creating FAISS index with dimension {settings.VECTOR_DIM}")
            start_time = time.time()
            faiss_index = faiss.IndexFlatL2(settings.VECTOR_DIM)
            faiss_time = time.time() - start_time
            logger.info(f"FAISS index created in {faiss_time:.3f}s")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            logger.info("Creating FAISS vector store")
            start_time = time.time()
            vector_store = FaissVectorStore(faiss_index=faiss_index)
            store_time = time.time() - start_time
            logger.info(f"Vector store created in {store_time:.3f}s")
            
            # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            logger.info("Creating storage context")
            start_time = time.time()
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            context_time = time.time() - start_time
            logger.info(f"Storage context created in {context_time:.3f}s")
            
            # åˆ›å»ºç´¢å¼•
            logger.info(f"Creating vector store index with {len(nodes)} nodes")
            start_time = time.time()
            index = VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                embed_model=self.embed_model
            )
            index_time = time.time() - start_time
            logger.info(f"Vector store index created in {index_time:.2f}s")
        
        return vector_store, index
    
    def _save_index(self, index: VectorStoreIndex, index_id: str) -> str:
        """ä¿å­˜ç´¢å¼•"""
        import time
        
        logger.info(f"Preparing index directory for {index_id}")
        index_dir = Path(settings.INDEX_STORE_PATH) / index_id
        index_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Index directory created: {index_dir}")
        
        # ä¿å­˜ç´¢å¼•
        logger.info(f"Starting index persistence to {index_dir}")
        start_time = time.time()
        index.storage_context.persist(persist_dir=str(index_dir))
        persist_time = time.time() - start_time
        
        logger.info(f"Index persistence completed in {persist_time:.2f}s to {index_dir}")
        return str(index_dir)
    
    def _map_page_info_to_nodes(self, nodes: List[Any]) -> List[Any]:
        """å°†é¡µç ä¿¡æ¯æ˜ å°„åˆ°æ–‡æ¡£åˆ‡ç‰‡èŠ‚ç‚¹
        
        è¿™ä¸ªæ–¹æ³•å°è¯•å°†content_list.jsonä¸­çš„é¡µç ä¿¡æ¯æ˜ å°„åˆ°æ¯ä¸ªæ–‡æ¡£åˆ‡ç‰‡çš„å…ƒæ•°æ®ä¸­ï¼Œ
        ä½¿å¾—åœ¨æ£€ç´¢æ—¶å¯ä»¥çŸ¥é“æ¯ä¸ªåˆ‡ç‰‡æ¥è‡ªåŸå§‹æ–‡æ¡£çš„å“ªä¸€é¡µã€‚
        
        Args:
            nodes: æ–‡æ¡£åˆ‡ç‰‡èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            æ·»åŠ äº†é¡µç ä¿¡æ¯çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        import json
        from pathlib import Path
        
        # æŒ‰æ–‡æ¡£åˆ†ç»„èŠ‚ç‚¹
        nodes_by_doc = {}
        for node in nodes:
            if not hasattr(node, 'metadata'):
                continue
                
            # è·å–åŸå§‹æ–‡æ¡£è·¯å¾„
            doc_path = node.metadata.get('file_path')
            if not doc_path:
                continue
                
            if doc_path not in nodes_by_doc:
                nodes_by_doc[doc_path] = []
            nodes_by_doc[doc_path].append(node)
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„èŠ‚ç‚¹
        for doc_path, doc_nodes in nodes_by_doc.items():
            # æ ¹æ®æ–‡æ¡£è·¯å¾„æ„å»ºcontent_list.jsonè·¯å¾„
            doc_file = Path(doc_path)
            content_list_path = doc_file.parent / "content_list.json"
            
            if not content_list_path.exists():
                logger.warning(f"No content_list.json found for document: {doc_path}")
                continue
                
            # è¯»å–content_list.json
            try:
                with open(content_list_path, 'r', encoding='utf-8') as f:
                    content_list = json.load(f)
            except Exception as e:
                logger.error(f"Failed to load content_list from {content_list_path}: {e}")
                continue
                
            if not isinstance(content_list, list) or not content_list:
                continue
                
            # åˆ›å»ºæ–‡æœ¬åˆ°é¡µç çš„æ˜ å°„
            text_to_page = {}
            for item in content_list:
                if item.get('type') == 'text' and 'text' in item and 'page_idx' in item:
                    text = item['text']
                    page_idx = item['page_idx']
                    text_to_page[text] = page_idx
            
            # ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…é¡µç 
            for node in doc_nodes:
                if not hasattr(node, 'text') or not node.text:
                    continue
                    
                # å°è¯•ç›´æ¥åŒ¹é…
                if node.text in text_to_page:
                    node.metadata['page_idx'] = text_to_page[node.text]
                    continue
                    
                # å°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆæŸ¥æ‰¾èŠ‚ç‚¹æ–‡æœ¬ä¸­åŒ…å«çš„æœ€é•¿content_listæ–‡æœ¬ï¼‰
                best_match = None
                best_match_length = 0
                for text, page_idx in text_to_page.items():
                    if text in node.text and len(text) > best_match_length:
                        best_match = text
                        best_match_length = len(text)
                
                if best_match:
                    node.metadata['page_idx'] = text_to_page[best_match]
                    continue
                    
                # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œå°è¯•åå‘åŒ¹é…ï¼ˆæŸ¥æ‰¾åŒ…å«èŠ‚ç‚¹æ–‡æœ¬çš„æœ€çŸ­content_listæ–‡æœ¬ï¼‰
                best_match = None
                best_match_length = float('inf')
                for text, page_idx in text_to_page.items():
                    if node.text in text and len(text) < best_match_length:
                        best_match = text
                        best_match_length = len(text)
                
                if best_match:
                    node.metadata['page_idx'] = text_to_page[best_match]
                    
            # ç»Ÿè®¡æ·»åŠ äº†é¡µç çš„èŠ‚ç‚¹æ•°é‡
            nodes_with_page = sum(1 for node in doc_nodes if 'page_idx' in node.metadata)
            logger.info(f"Added page information to {nodes_with_page}/{len(doc_nodes)} nodes for document: {doc_path}")
        
        return nodes
    
    def _generate_stats(self, documents: List[Document], nodes: List[Any], task: VectorStoreTask) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        total_chars = sum(len(doc.text) for doc in documents)
        avg_chunk_size = sum(len(getattr(node, 'text', '')) for node in nodes) / len(nodes) if nodes else 0
        
        # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
        file_types = {}
        for file_path in task.processed_files:
            ext = Path(file_path).suffix.lower()
            file_types[ext] = file_types.get(ext, 0) + 1
        
        # ç»Ÿè®¡æœ‰é¡µç ä¿¡æ¯çš„èŠ‚ç‚¹æ•°é‡
        nodes_with_page = sum(1 for node in nodes if hasattr(node, 'metadata') and 'page_idx' in node.metadata)
        
        return {
            "total_characters": total_chars,
            "average_chunk_size": avg_chunk_size,
            "file_types": file_types,
            "nodes_with_page_info": nodes_with_page,
            "nodes_with_page_percentage": f"{nodes_with_page/len(nodes)*100:.2f}%" if nodes else "0%",
            "processing_time": time.time() - task.started_at if task.started_at else 0
        }
    
    def _save_index_info_to_db(self, index_id: str, index_description: Optional[str] = None) -> None:
        """ä¿å­˜ç´¢å¼•ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            from models.database import SessionLocal
            from dao.index_dao import IndexDAO
            
            # åˆ›å»ºæ•°æ®åº“ä¼šè¯
            db = SessionLocal()
            try:
                # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
                existing_index = IndexDAO.get_index_by_id(db, index_id)
                
                if existing_index:
                    # æ›´æ–°ç´¢å¼•æè¿°
                    if index_description is not None:
                        IndexDAO.update_index_description(db, index_id, index_description)
                        logger.info(f"Updated description for index: {index_id}")
                else:
                    # åˆ›å»ºæ–°ç´¢å¼•ä¿¡æ¯
                    IndexDAO.create_index(db, index_id, index_description)
                    logger.info(f"Created new index info in database: {index_id}")
            finally:
                db.close()
        except Exception as e:
            logger.error(f"Error saving index info to database: {e}")
            raise
    
    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        task = self.tasks.get(task_id)
        return task.to_dict() if task else None
    
    def get_all_tasks(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
        return [task.to_dict() for task in self.tasks.values()]
    
    def cleanup_expired_tasks(self):
        """æ¸…ç†è¿‡æœŸä»»åŠ¡"""
        current_time = time.time()
        expired_tasks = [
            task_id for task_id, task in self.tasks.items()
            if current_time - task.created_at > settings.TASK_EXPIRE_TIME
        ]
        
        for task_id in expired_tasks:
            del self.tasks[task_id]
            logger.info(f"Cleaned up expired vector store task: {task_id}")

# å…¨å±€æ„å»ºå™¨å®ä¾‹
vector_store_builder = VectorStoreBuilder()