# -*- coding: utf-8 -*-
"""
å…ƒæ•°æ®æå–å¤±è´¥è®°å½•ç®¡ç†å™¨
ç”¨äºè®°å½•å’Œç®¡ç†å…ƒæ•°æ®æå–å¤±è´¥çš„æƒ…å†µ
"""

import json
import logging
import os
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path

logger = logging.getLogger(__name__)

class MetadataFailureManager:
    """å…ƒæ•°æ®æå–å¤±è´¥è®°å½•ç®¡ç†å™¨"""
    
    def __init__(self, failure_log_dir: str = "cache/metadata_failures"):
        """
        åˆå§‹åŒ–å¤±è´¥è®°å½•ç®¡ç†å™¨
        
        Args:
            failure_log_dir: å¤±è´¥è®°å½•å­˜å‚¨ç›®å½•
        """
        self.failure_log_dir = Path(failure_log_dir)
        self.failure_log_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡æ¡£çº§å¤±è´¥è®°å½•æ–‡ä»¶
        self.doc_failure_file = self.failure_log_dir / "document_failures.json"
        # chunkçº§å¤±è´¥è®°å½•æ–‡ä»¶
        self.chunk_failure_file = self.failure_log_dir / "chunk_failures.json"
        
        # åˆå§‹åŒ–å¤±è´¥è®°å½•æ–‡ä»¶
        self._init_failure_files()
    
    def _init_failure_files(self):
        """åˆå§‹åŒ–å¤±è´¥è®°å½•æ–‡ä»¶"""
        for file_path in [self.doc_failure_file, self.chunk_failure_file]:
            if not file_path.exists():
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump({}, f, ensure_ascii=False, indent=2)
    
    def record_document_failure(self, doc_id: str, text: str, error_reason: str):
        """
        è®°å½•æ–‡æ¡£çº§å…ƒæ•°æ®æå–å¤±è´¥
        
        Args:
            doc_id: æ–‡æ¡£ID
            text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            error_reason: å¤±è´¥åŸå› 
        """
        try:
            # è¯»å–ç°æœ‰å¤±è´¥è®°å½•
            with open(self.doc_failure_file, 'r', encoding='utf-8') as f:
                failures = json.load(f)
            
            # æ·»åŠ æ–°çš„å¤±è´¥è®°å½•
            failures[doc_id] = {
                "doc_id": doc_id,
                "text": text[:1000] + "..." if len(text) > 1000 else text,  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                "full_text_length": len(text),
                "error_reason": error_reason,
                "failure_time": datetime.now().isoformat(),
                "retry_count": failures.get(doc_id, {}).get("retry_count", 0) + 1
            }
            
            # ä¿å­˜å¤±è´¥è®°å½•
            with open(self.doc_failure_file, 'w', encoding='utf-8') as f:
                json.dump(failures, f, ensure_ascii=False, indent=2)
            
            logger.warning(f"ğŸ“ Recorded document failure for {doc_id}: {error_reason}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to record document failure: {e}")
    
    def record_chunk_failure(self, chunk_id: str, text: str, error_reason: str, doc_id: str = None):
        """
        è®°å½•chunkçº§å…ƒæ•°æ®æå–å¤±è´¥
        
        Args:
            chunk_id: chunk ID
            text: chunkæ–‡æœ¬å†…å®¹
            error_reason: å¤±è´¥åŸå› 
            doc_id: æ‰€å±æ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰
        """
        try:
            # è¯»å–ç°æœ‰å¤±è´¥è®°å½•
            with open(self.chunk_failure_file, 'r', encoding='utf-8') as f:
                failures = json.load(f)
            
            # æ·»åŠ æ–°çš„å¤±è´¥è®°å½•
            failures[chunk_id] = {
                "chunk_id": chunk_id,
                "doc_id": doc_id,
                "text": text[:500] + "..." if len(text) > 500 else text,  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                "full_text_length": len(text),
                "error_reason": error_reason,
                "failure_time": datetime.now().isoformat(),
                "retry_count": failures.get(chunk_id, {}).get("retry_count", 0) + 1
            }
            
            # ä¿å­˜å¤±è´¥è®°å½•
            with open(self.chunk_failure_file, 'w', encoding='utf-8') as f:
                json.dump(failures, f, ensure_ascii=False, indent=2)
            
            logger.warning(f"ğŸ“ Recorded chunk failure for {chunk_id}: {error_reason}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to record chunk failure: {e}")
    
    def remove_document_failure(self, doc_id: str):
        """
        åˆ é™¤æ–‡æ¡£çº§å¤±è´¥è®°å½•ï¼ˆæˆåŠŸæå–åè°ƒç”¨ï¼‰
        
        Args:
            doc_id: æ–‡æ¡£ID
        """
        try:
            with open(self.doc_failure_file, 'r', encoding='utf-8') as f:
                failures = json.load(f)
            
            if doc_id in failures:
                del failures[doc_id]
                
                with open(self.doc_failure_file, 'w', encoding='utf-8') as f:
                    json.dump(failures, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… Removed document failure record for {doc_id}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to remove document failure record: {e}")
    
    def remove_chunk_failure(self, chunk_id: str):
        """
        åˆ é™¤chunkçº§å¤±è´¥è®°å½•ï¼ˆæˆåŠŸæå–åè°ƒç”¨ï¼‰
        
        Args:
            chunk_id: chunk ID
        """
        try:
            with open(self.chunk_failure_file, 'r', encoding='utf-8') as f:
                failures = json.load(f)
            
            if chunk_id in failures:
                del failures[chunk_id]
                
                with open(self.chunk_failure_file, 'w', encoding='utf-8') as f:
                    json.dump(failures, f, ensure_ascii=False, indent=2)
                
                logger.info(f"âœ… Removed chunk failure record for {chunk_id}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to remove chunk failure record: {e}")
    
    def get_document_failures(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰æ–‡æ¡£çº§å¤±è´¥è®°å½•
        
        Returns:
            Dict[str, Any]: å¤±è´¥è®°å½•å­—å…¸
        """
        try:
            with open(self.doc_failure_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ Failed to read document failures: {e}")
            return {}
    
    def get_chunk_failures(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰chunkçº§å¤±è´¥è®°å½•
        
        Returns:
            Dict[str, Any]: å¤±è´¥è®°å½•å­—å…¸
        """
        try:
            with open(self.chunk_failure_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ Failed to read chunk failures: {e}")
            return {}
    
    def get_failure_summary(self) -> Dict[str, Any]:
        """
        è·å–å¤±è´¥è®°å½•æ‘˜è¦ç»Ÿè®¡
        
        Returns:
            Dict[str, Any]: å¤±è´¥è®°å½•ç»Ÿè®¡ä¿¡æ¯
        """
        doc_failures = self.get_document_failures()
        chunk_failures = self.get_chunk_failures()
        
        return {
            "document_failures_count": len(doc_failures),
            "chunk_failures_count": len(chunk_failures),
            "total_failures": len(doc_failures) + len(chunk_failures),
            "document_failure_ids": list(doc_failures.keys()),
            "chunk_failure_ids": list(chunk_failures.keys())
        }
    
    def cleanup_old_failures(self, days_old: int = 30):
        """
        æ¸…ç†æ—§çš„å¤±è´¥è®°å½•
        
        Args:
            days_old: æ¸…ç†å¤šå°‘å¤©å‰çš„è®°å½•
        """
        from datetime import timedelta
        
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        # æ¸…ç†æ–‡æ¡£çº§å¤±è´¥è®°å½•
        try:
            doc_failures = self.get_document_failures()
            cleaned_doc_failures = {}
            
            for doc_id, failure_info in doc_failures.items():
                failure_time = datetime.fromisoformat(failure_info.get("failure_time", ""))
                if failure_time > cutoff_date:
                    cleaned_doc_failures[doc_id] = failure_info
            
            with open(self.doc_failure_file, 'w', encoding='utf-8') as f:
                json.dump(cleaned_doc_failures, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ§¹ Cleaned {len(doc_failures) - len(cleaned_doc_failures)} old document failure records")
            
        except Exception as e:
            logger.error(f"âŒ Failed to cleanup old document failures: {e}")
        
        # æ¸…ç†chunkçº§å¤±è´¥è®°å½•
        try:
            chunk_failures = self.get_chunk_failures()
            cleaned_chunk_failures = {}
            
            for chunk_id, failure_info in chunk_failures.items():
                failure_time = datetime.fromisoformat(failure_info.get("failure_time", ""))
                if failure_time > cutoff_date:
                    cleaned_chunk_failures[chunk_id] = failure_info
            
            with open(self.chunk_failure_file, 'w', encoding='utf-8') as f:
                json.dump(cleaned_chunk_failures, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ§¹ Cleaned {len(chunk_failures) - len(cleaned_chunk_failures)} old chunk failure records")
            
        except Exception as e:
            logger.error(f"âŒ Failed to cleanup old chunk failures: {e}")