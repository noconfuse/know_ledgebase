# -*- coding: utf-8 -*-
"""
æ–‡æ¡£çº§å…ƒæ•°æ®æå–å™¨
åŸºäºLlamaIndexçš„BaseExtractorï¼Œè¿”å›å…ƒæ•°æ®å­—å…¸è€Œä¸æ˜¯ä¿®æ”¹èŠ‚ç‚¹
"""

import logging
import os
import time
from typing import List, Dict, Any, Optional
from pydantic import Field
from llama_index.core.extractors import BaseExtractor
from llama_index.core.schema import BaseNode
from llama_index.core.program import LLMTextCompletionProgram
from llama_index.core.llms.llm import LLM
from llama_index.core.output_parsers import PydanticOutputParser
from models.metadata_models import DocumentLevelMetadata, LegalDocumentMetadata, PolicyDocumentMetadata
from services.metadata_cache_manager import MetadataCacheManager
from services.metadata_failure_manager import MetadataFailureManager
from config import settings

logger = logging.getLogger(__name__)

# æ–‡æ¡£åˆ†ç±»æ¨¡æ¿
CLASSIFICATION_PROMPT_TEMPLATE = """
è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå°†å…¶åˆ†ç±»ä¸ºä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š
- 'legal_document'ï¼ˆæ³•å¾‹æ–‡æ¡£ï¼‰ï¼šåŒ…æ‹¬æ³•å¾‹æ¡æ–‡ã€å¸æ³•è§£é‡Šã€è¡Œæ”¿æ³•è§„ã€éƒ¨é—¨è§„ç« ç­‰
- 'policy_document'ï¼ˆæ”¿ç­–æ–‡æ¡£ï¼‰ï¼šåŒ…æ‹¬æ”¿ç­–è§£è¯»ã€å®æ–½ç»†åˆ™ã€é€šçŸ¥å…¬å‘Šã€å·¥ä½œæŒ‡å—ç­‰
- 'general_document'ï¼ˆé€šç”¨æ–‡æ¡£ï¼‰ï¼šå…¶ä»–ç±»å‹æ–‡æ¡£

åªè¿”å›ç±»åˆ«åç§°ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚

æ–‡æ¡£å†…å®¹ï¼š
---
{context_str}
---

ç±»åˆ«ï¼š"""


class DocumentLevelMetadataExtractor(BaseExtractor):
    """æ–‡æ¡£çº§å…ƒæ•°æ®æå–å™¨
    
    æ”¯æŒæ–‡æ¡£åˆ†ç±»å’ŒåŸºäºåˆ†ç±»çš„å…ƒæ•°æ®ç»“æ„é€‰æ‹©
    åŒ…å«æœ¬åœ°ç¼“å­˜åŠŸèƒ½ä»¥æé«˜è°ƒè¯•æ•ˆç‡
    æ”¯æŒé‡è¯•æœºåˆ¶ä»¥æé«˜æå–æˆåŠŸç‡
    """
    
    llm: LLM = Field(description="è¯­è¨€æ¨¡å‹å®ä¾‹")
    min_doc_size_for_extraction: int = Field(default=100, description="è¿›è¡Œå…ƒæ•°æ®æå–çš„æœ€å°æ–‡æ¡£å¤§å°")
    max_doc_size_for_extraction: int = Field(default=100000, description="è¿›è¡Œå…ƒæ•°æ®æå–çš„æœ€å¤§æ–‡æ¡£å¤§å°")
    enable_cache: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨ç¼“å­˜")
    cache_manager: Optional[MetadataCacheManager] = Field(default=None, description="ç¼“å­˜ç®¡ç†å™¨")
    failure_manager: Optional[MetadataFailureManager] = Field(default=None, description="å¤±è´¥è®°å½•ç®¡ç†å™¨")
    
    # é‡è¯•é…ç½®
    max_retries: int = Field(default=3, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
    retry_delay: float = Field(default=1.0, description="é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰")
    
    def __init__(self, 
                 llm: LLM, 
                 enable_cache: bool = True,
                 cache_dir: str = "cache/metadata",
                 min_doc_size_for_extraction: int = 500,
                 max_doc_size_for_extraction: int = 100000,
                 max_retries: int = 3,
                 retry_delay: float = 1.0,
                 **kwargs):
        # ä»kwargsä¸­ç§»é™¤å¯èƒ½å†²çªçš„å‚æ•°
        cache_manager_from_kwargs = kwargs.pop('cache_manager', None)
        failure_manager_from_kwargs = kwargs.pop('failure_manager', None)
        
        # åˆ›å»ºcache_managerå’Œfailure_managerï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        cache_manager = cache_manager_from_kwargs or (MetadataCacheManager(cache_dir) if enable_cache else None)
        failure_manager = failure_manager_from_kwargs or MetadataFailureManager()
        
        super().__init__(
            llm=llm,
            min_doc_size_for_extraction=min_doc_size_for_extraction,
            max_doc_size_for_extraction=max_doc_size_for_extraction,
            enable_cache=enable_cache,
            cache_manager=cache_manager,
            failure_manager=failure_manager,
            max_retries=max_retries,
            retry_delay=retry_delay,
            **kwargs
        )
        
    def extract(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        """æå–æ–‡æ¡£çº§å…ƒæ•°æ®
        
        Args:
            nodes: è¾“å…¥èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            List[Dict[str, Any]]: æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
        """
        logger.info(f"ğŸš€ [DOC METADATA EXTRACTOR] Starting document-level metadata extraction for {len(nodes)} documents")
        
        metadata_list = []
        
        for i, node in enumerate(nodes):
            metadata_dict = {}
            
            if hasattr(node, 'text'):  # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æ¡£èŠ‚ç‚¹
                doc_id = node.metadata.get("original_file_path", f"doc_{i}")
                text_length = len(node.text)
                
                logger.info(f"ğŸ“‹ [DOC METADATA EXTRACTOR] Processing document: {doc_id} (length: {text_length})")
                
                # æ£€æŸ¥æ–‡æ¡£å¤§å°æ˜¯å¦é€‚åˆæå–
                if text_length < self.min_doc_size_for_extraction:
                    logger.info(f"â­ï¸ Document too small ({text_length} < {self.min_doc_size_for_extraction}), skipping metadata extraction")
                    # å…ƒæ•°æ®ä¸ºç©ºå­—å…¸
                    metadata_list.append({})
                    continue
                
                # æ£€æŸ¥ç¼“å­˜
                if self.cache_manager:
                    cached_metadata = self.cache_manager.get_cached_metadata(doc_id, node.text)
                    if cached_metadata:
                        logger.info(f"ğŸ“‹ Document metadata cache hit for: {doc_id}")
                        # ç›´æ¥å±•å¼€å­˜å‚¨å…ƒæ•°æ®ï¼Œç§»é™¤å¤±è´¥è®°å½•
                        if self.failure_manager:
                            self.failure_manager.remove_document_failure(doc_id)
                        metadata_list.append(cached_metadata)
                        continue
                    
                try:
                    if text_length > self.max_doc_size_for_extraction:
                        logger.info(f"ğŸ“ Document too large ({text_length} > {self.max_doc_size_for_extraction}), using optimized content")
                        # ä½¿ç”¨ä¼˜åŒ–åçš„å†…å®¹è¿›è¡Œå…ƒæ•°æ®æå–
                        optimized_content = self._optimize_document_content(node.text)
                        doc_metadata = self._extract_document_metadata_with_retry(optimized_content, doc_id)
                    else:
                        # ç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹
                        doc_metadata = self._extract_document_metadata_with_retry(node.text, doc_id)
                    
                    if doc_metadata:
                        # ç›´æ¥å±•å¼€å­˜å‚¨å…ƒæ•°æ®
                        metadata_dict.update(doc_metadata)
                        
                        # ä¿å­˜åˆ°ç¼“å­˜
                        if self.cache_manager:
                            self.cache_manager.save_metadata_to_cache(doc_id, doc_metadata, node.text)
                            logger.info(f"ğŸ“‹ Document metadata cached for: {doc_id}")
                        
                        # ç§»é™¤å¤±è´¥è®°å½•
                        if self.failure_manager:
                            self.failure_manager.remove_document_failure(doc_id)
                        
                        logger.info(f"âœ… Document metadata extracted successfully for: {doc_id}")
                    else:
                        # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºå¤±è´¥ä¼šä»¥å¼‚å¸¸å½¢å¼æŠ›å‡º
                        logger.warning(f"âš ï¸ Metadata extraction returned None without error for: {doc_id}")
                        metadata_dict = {}

                except Exception as e:
                    error_message = f"å…ƒæ•°æ®æå–å¤±è´¥: {str(e)}"
                    logger.error(f"âŒ {error_message} for document: {doc_id}")
                    # è®°å½•å¤±è´¥ä¿¡æ¯
                    if self.failure_manager:
                        self.failure_manager.record_document_failure(doc_id, node.text, error_message)
                    # å…ƒæ•°æ®ä¸ºç©ºå­—å…¸
                    metadata_dict = {}
            
            metadata_list.append(metadata_dict)
        
        logger.info(f"ğŸ‰ [DOC METADATA EXTRACTOR] Document-level metadata extraction completed")
        return metadata_list
    
    async def aextract(self, nodes: List[BaseNode]) -> List[Dict[str, Any]]:
        """å¼‚æ­¥æå–æ–‡æ¡£çº§å…ƒæ•°æ®
        
        Args:
            nodes: è¾“å…¥èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            List[Dict[str, Any]]: æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
        """
        # å¯¹äºæ–‡æ¡£çº§æå–ï¼Œç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•
        return self.extract(nodes)
    
    def _optimize_document_content(self, text: str) -> str:
        """ä¼˜åŒ–è¶…å¤§æ–‡æ¡£å†…å®¹ç”¨äºå…ƒæ•°æ®æå–"""
        content_length = len(text)
        
        # å°æ–‡æ¡£ç›´æ¥è¿”å›
        if content_length <= 20000:
            return text
            
        logger.info(f"ğŸ“Š Optimizing large document content: {content_length} chars")
        
        try:
            # ç­–ç•¥1: æå–æ–‡æ¡£ç»“æ„å’Œå…³é”®éƒ¨åˆ†
            if content_length <= 50000:
                return self._extract_key_sections(text)
            
            # ç­–ç•¥2: è¶…å¤§æ–‡æ¡£ä½¿ç”¨æ™ºèƒ½æ‘˜è¦
            else:
                return self._extract_document_summary_sections(text)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Content optimization failed: {e}, using truncated content")
            # é™çº§ç­–ç•¥ï¼šæˆªå–å‰é…ç½®é•¿åº¦çš„å­—ç¬¦
            fallback_length = 30000
            return text[:fallback_length] + "\n\n[æ–‡æ¡£å†…å®¹å·²æˆªæ–­ä»¥ä¼˜åŒ–å¤„ç†æ•ˆç‡]" if content_length > fallback_length else text
    
    def _extract_key_sections(self, content: str) -> str:
        """æå–æ–‡æ¡£çš„å…³é”®ç« èŠ‚"""
        lines = content.split('\n')
        key_sections = []
        current_section = []
        
        # å®šä¹‰å…³é”®ç« èŠ‚æ ‡è¯†ç¬¦
        section_patterns = [
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« |Chapter\s+\d+)',
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¡|Article\s+\d+)',
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+èŠ‚|Section\s+\d+)',
            r'^(\d+\.|\d+ã€|\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\))',
            r'^(#{1,6}\s+)',
            r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|[A-Z]\.|\d+\.)\s*[^\s]',
        ]
        
        import re
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é”®ç« èŠ‚å¼€å§‹
            is_section_start = any(re.match(pattern, line) for pattern in section_patterns)
            
            if is_section_start:
                # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚
                if current_section and len('\n'.join(current_section)) <= 2000:
                    key_sections.extend(current_section)
                    key_sections.append('')
                
                current_section = [line]
            else:
                current_section.append(line)
                
                # å¦‚æœå½“å‰ç« èŠ‚è¿‡é•¿ï¼Œæˆªæ–­
                if len('\n'.join(current_section)) > 2000:
                    key_sections.extend(current_section[:20])
                    key_sections.append('[ç« èŠ‚å†…å®¹å·²æˆªæ–­]')
                    key_sections.append('')
                    current_section = []
        
        # å¤„ç†æœ€åä¸€ä¸ªç« èŠ‚
        if current_section and len('\n'.join(current_section)) <= 2000:
            key_sections.extend(current_section)
        
        result = '\n'.join(key_sections)
        
        # å¦‚æœæå–çš„å†…å®¹ä»ç„¶è¿‡é•¿ï¼Œè¿›ä¸€æ­¥æˆªæ–­
        if len(result) > 30000:
            result = result[:30000] + "\n\n[å†…å®¹å·²ä¼˜åŒ–æˆªæ–­]"
            
        return result
    
    def _extract_document_summary_sections(self, content: str) -> str:
        """æå–è¶…å¤§æ–‡æ¡£çš„æ‘˜è¦éƒ¨åˆ†"""
        lines = content.split('\n')
        summary_parts = []
        
        # 1. æ–‡æ¡£å¼€å¤´
        beginning = '\n'.join(lines[:50])
        if len(beginning) > 2000:
            beginning = beginning[:2000]
        summary_parts.append("=== æ–‡æ¡£å¼€å¤´ ===")
        summary_parts.append(beginning)
        summary_parts.append("")
        
        # 2. æå–ä¸»è¦æ ‡é¢˜å’Œç« èŠ‚
        import re
        title_patterns = [
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« .*)',
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¡.*)',
            r'^(#{1,3}\s+.*)',
            r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€.*)',
        ]
        
        summary_parts.append("=== ä¸»è¦ç« èŠ‚æ ‡é¢˜ ===")
        title_count = 0
        for line in lines[50:-50]:
            line = line.strip()
            if any(re.match(pattern, line) for pattern in title_patterns):
                summary_parts.append(line)
                title_count += 1
                if title_count >= 20:
                    break
        summary_parts.append("")
        
        # 3. æ–‡æ¡£ç»“å°¾
        ending = '\n'.join(lines[-20:])
        if len(ending) > 2000:
            ending = ending[-2000:]
        summary_parts.append("=== æ–‡æ¡£ç»“å°¾ ===")
        summary_parts.append(ending)
        
        result = '\n'.join(summary_parts)
        
        if len(result) > 25000:
            result = result[:25000] + "\n\n[è¶…å¤§æ–‡æ¡£å·²æ™ºèƒ½æ‘˜è¦]"
            
        return result
    
    def _extract_document_metadata_with_retry(self, text: str, doc_id: str) -> Optional[Dict[str, Any]]:
        """å¸¦é‡è¯•é€»è¾‘çš„æ–‡æ¡£çº§å…ƒæ•°æ®æå–
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            doc_id: æ–‡æ¡£æ ‡è¯†ç¬¦
            
        Returns:
            Optional[Dict[str, Any]]: æå–çš„å…ƒæ•°æ®å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›é»˜è®¤å€¼
        """
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                if attempt > 0:
                    logger.info(f"ğŸ”„ Retrying document metadata extraction (attempt {attempt + 1}/{self.max_retries + 1}) for document: {doc_id}")
                    time.sleep(self.retry_delay)
                
                # å°è¯•æå–å…ƒæ•°æ®
                result = self._extract_document_metadata(text, doc_id)
                
                if result:
                    if attempt > 0:
                        logger.info(f"âœ… Document metadata extraction succeeded on retry {attempt + 1} for document: {doc_id}")
                    return result
                else:
                    last_error = "LLM returned empty result"
                    
            except Exception as e:
                last_error = str(e)
                logger.warning(f"âš ï¸ Attempt {attempt + 1} failed for document {doc_id}: {last_error}")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œä¸å†é‡è¯•
                if attempt == self.max_retries:
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œå‘ä¸ŠæŠ›å‡ºæœ€åä¸€ä¸ªé‡åˆ°çš„å¼‚å¸¸
        logger.error(f"âŒ All {self.max_retries + 1} attempts failed for document metadata extraction. Document: {doc_id}. Last error: {last_error}")
        raise Exception(f"é‡è¯•{self.max_retries + 1}æ¬¡åä»å¤±è´¥: {last_error}")
    
    def _extract_document_metadata(self, text: str, doc_id: str) -> Optional[Dict[str, Any]]:
        """æå–æ–‡æ¡£çº§å…ƒæ•°æ®ï¼ˆå•æ¬¡å°è¯•ï¼‰"""
        try:
            # 1. æ–‡æ¡£åˆ†ç±»
            document_category = self._classify_document(text)
            logger.info(f"ğŸ“‹ Document classified as: {document_category}")
            
            # 2. æ ¹æ®åˆ†ç±»é€‰æ‹©å…ƒæ•°æ®æ¨¡å‹å’Œæ¨¡æ¿
            metadata_model, template = self._get_model_and_template(document_category)
            
            # 3. æå–å…ƒæ•°æ® - ä½¿ç”¨CustomOutputParserå¤„ç†LLMè¾“å‡º
            from .custom_output_parser import CustomOutputParser
            custom_parser = CustomOutputParser(output_cls=metadata_model, verbose=False)
            extraction_program = LLMTextCompletionProgram.from_defaults(
                output_parser=custom_parser,
                output_cls=metadata_model,
                llm=self.llm,
                prompt_template_str=template,
                verbose=False
            )
            
            # æ‰§è¡Œæå– - ä½¿ç”¨æ­£ç¡®çš„è°ƒç”¨æ–¹å¼
            result = extraction_program(
                context_str=text,
                text_length=len(text)
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼å¹¶æ·»åŠ åˆ†ç±»ä¿¡æ¯
            if hasattr(result, 'dict'):
                metadata = result.dict()
            elif hasattr(result, 'model_dump'):
                metadata = result.model_dump()
            else:
                metadata = dict(result)
            
            metadata['document_category'] = document_category
            
            return metadata
                
        except Exception as e:
            logger.error(f"âŒ Error extracting document metadata for {doc_id}: {str(e)}")
            # ä¸åœ¨è¿™é‡Œè®°å½•å¤±è´¥ï¼Œç”±ä¸Šå±‚é‡è¯•é€»è¾‘å¤„ç†
            raise e
    
    def _classify_document(self, content: str) -> str:
        """
        ä½¿ç”¨LLMåˆ†ç±»æ–‡æ¡£ç±»å‹
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            
        Returns:
            str: æ–‡æ¡£ç±»å‹
        """
        try:
            # æˆªå–å‰2000å­—ç¬¦ç”¨äºåˆ†ç±»
            classification_content = content[:2000] if len(content) > 2000 else content
            
            # ä½¿ç”¨LLMè¿›è¡Œåˆ†ç±»
            response = self.llm.complete(
                CLASSIFICATION_PROMPT_TEMPLATE.format(context_str=classification_content)
            )
            
            classification = response.text.strip().lower()
            
            # éªŒè¯åˆ†ç±»ç»“æœ
            if 'legal_document' in classification:
                return 'legal_document'
            elif 'policy_document' in classification:
                return 'policy_document'
            elif 'general_document' in classification:
                return 'general_document'
            else:
                logger.warning(f"æœªè¯†åˆ«çš„åˆ†ç±»ç»“æœ: {classification}ï¼Œé»˜è®¤ä¸ºgeneral_document")
                return 'general_document'
                
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†ç±»å¤±è´¥: {str(e)}ï¼Œé»˜è®¤ä¸ºgeneral_document")
            return 'general_document'
    
    def _get_model_and_template(self, document_category: str):
        """
        æ ¹æ®æ–‡æ¡£ç±»åˆ«è·å–å¯¹åº”çš„æ¨¡å‹å’Œæ¨¡æ¿
        
        Args:
            document_category: æ–‡æ¡£ç±»åˆ«
            
        Returns:
            tuple: (æ¨¡å‹ç±», æç¤ºæ¨¡æ¿)
        """
        if document_category == 'legal_document':
            return LegalDocumentMetadata, self._create_legal_document_template()
        elif document_category == 'policy_document':
            return PolicyDocumentMetadata, self._create_policy_document_template()
        else:
            # é»˜è®¤ä½¿ç”¨é€šç”¨æ–‡æ¡£å…ƒæ•°æ®æ¨¡å‹
            return DocumentLevelMetadata, self._create_general_document_template()
    
    def _create_legal_document_template(self) -> str:
        """åˆ›å»ºæ³•å¾‹æ–‡æ¡£å…ƒæ•°æ®æå–æ¨¡æ¿"""
        fields_desc = []
        for field_name, model_field in LegalDocumentMetadata.model_fields.items():
            fields_desc.append(f"- **{field_name}**: {model_field.description}")
        
        return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ³•å¾‹æ–‡æ¡£å†…å®¹ï¼Œæå–æ–‡æ¡£çº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

æ–‡æ¡£å†…å®¹:
----------------
{{context_str}}
----------------

è¯·æå–ä»¥ä¸‹æ³•å¾‹æ–‡æ¡£çº§ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""
    
    def _create_policy_document_template(self) -> str:
        """åˆ›å»ºæ”¿ç­–æ–‡æ¡£å…ƒæ•°æ®æå–æ¨¡æ¿"""
        fields_desc = []
        for field_name, model_field in PolicyDocumentMetadata.model_fields.items():
            fields_desc.append(f"- **{field_name}**: {model_field.description}")
        
        return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ”¿ç­–æ–‡æ¡£å†…å®¹ï¼Œæå–æ–‡æ¡£çº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

æ–‡æ¡£å†…å®¹:
----------------
{{context_str}}
----------------

è¯·æå–ä»¥ä¸‹æ”¿ç­–æ–‡æ¡£çº§ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""
    
    def _create_fallback_metadata(self) -> Dict[str, Any]:
        """åˆ›å»ºå›é€€å…ƒæ•°æ®ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return {
            'document_type': 'æœªçŸ¥æ–‡æ¡£',
            'target_audience': ['æœªçŸ¥'],
            'importance_level': 'ä¸€èˆ¬',
            'applicable_scenarios': ['æœªçŸ¥'],
            'related_articles': [],
            'document_summary': 'å…ƒæ•°æ®æå–å¤±è´¥',
            'document_category': 'legal_document',
            'is_fallback': True
        }
    
    def _create_general_document_template(self) -> str:
        """åˆ›å»ºé€šç”¨æ–‡æ¡£çº§å…ƒæ•°æ®æå–æ¨¡æ¿"""
        fields_desc = []
        for field_name, model_field in DocumentLevelMetadata.model_fields.items():
            fields_desc.append(f"- **{field_name}**: {model_field.description}")
        
        return f"""
è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œæå–æ–‡æ¡£çº§çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

æ–‡æ¡£å†…å®¹:
----------------
{{context_str}}
----------------

æ–‡æ¡£é•¿åº¦: {{text_length}} å­—ç¬¦

è¯·ä»”ç»†åˆ†ææ•´ä¸ªæ–‡æ¡£ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š

{chr(10).join(fields_desc)}

æ³¨æ„äº‹é¡¹ï¼š
- è¯·åŸºäºæ•´ä¸ªæ–‡æ¡£å†…å®¹è¿›è¡Œåˆ†æï¼Œä¸è¦åªå…³æ³¨å¼€å¤´éƒ¨åˆ†
- æå–çš„ä¿¡æ¯åº”è¯¥å‡†ç¡®ã€ç®€æ´ã€æœ‰ç”¨
- å…³é”®å®ä½“åº”è¯¥æ˜¯æ–‡æ¡£ä¸­çœŸå®å‡ºç°çš„é‡è¦æ¦‚å¿µ
- ä¸»è¦è¯é¢˜åº”è¯¥è¦†ç›–æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹é¢†åŸŸ
- å¯¹äºå¯é€‰å­—æ®µï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯å¯ä»¥ä¸å¡«å†™

è¯·æ ¹æ®ç»™å®šçš„ç»“æ„åŒ–æ ¼å¼è¿”å›ç»“æœã€‚
"""