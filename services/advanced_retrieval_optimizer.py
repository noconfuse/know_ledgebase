# -*- coding: utf-8 -*-
"""
é«˜çº§æ£€ç´¢ä¼˜åŒ–å™¨
å®ç°å¯¹è¯æ¥å£çš„å¢å¼ºæ£€ç´¢èƒ½åŠ›ï¼ŒåŒ…æ‹¬ï¼š
1. æŸ¥è¯¢æ‰©å±•å’Œé‡å†™
2. å¤šè·¯å¬å›èåˆ
3. åŠ¨æ€æƒé‡è°ƒæ•´
4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢
5. å¯¹è¯å†å²å¢å¼º
"""

import logging
import asyncio
import json
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from llama_index.core.schema import NodeWithScore, QueryBundle
from llama_index.core.retrievers import BaseRetriever
from llama_index.core.llms.llm import LLM
from llama_index.core.indices.vector_store import VectorStoreIndex
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.retrievers import QueryFusionRetriever
from services.filtered_retriever import FilteredRetriever
from config import settings

logger = logging.getLogger(__name__)


class QueryExpander:
    """æŸ¥è¯¢æ‰©å±•å™¨"""
    
    def __init__(self, llm: LLM):
        self.llm = llm
        self.expansion_cache = {}
    
    async def expand_query(self, query: str, context: Optional[str] = None) -> List[str]:
        """æ‰©å±•æŸ¥è¯¢ï¼Œç”Ÿæˆç›¸å…³çš„æŸ¥è¯¢å˜ä½“"""
        cache_key = f"{query}_{hash(context) if context else 'no_context'}"
        
        if cache_key in self.expansion_cache:
            return self.expansion_cache[cache_key]
        
        context_prompt = f"\nä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}" if context else ""
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹æŸ¥è¯¢ç”Ÿæˆ3-5ä¸ªç›¸å…³çš„æŸ¥è¯¢å˜ä½“ï¼Œç”¨äºæé«˜æ£€ç´¢å¬å›ç‡ã€‚
å˜ä½“åº”è¯¥ï¼š
1. ä½¿ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼
2. åŒ…å«åŒä¹‰è¯å’Œç›¸å…³æœ¯è¯­
3. è€ƒè™‘ä¸åŒçš„é—®æ³•è§’åº¦
4. ä¿æŒåŸå§‹æŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾
{context_prompt}

åŸå§‹æŸ¥è¯¢ï¼š{query}

è¯·ä»¥JSONæ ¼å¼è¿”å›æŸ¥è¯¢å˜ä½“åˆ—è¡¨ï¼š
{{"expanded_queries": ["å˜ä½“1", "å˜ä½“2", "å˜ä½“3"]}}
"""
        
        try:
            response = await self.llm.acomplete(prompt)
            result = json.loads(response.text.strip())
            expanded_queries = result.get("expanded_queries", [query])
            
            # ç¡®ä¿åŸå§‹æŸ¥è¯¢åœ¨åˆ—è¡¨ä¸­
            if query not in expanded_queries:
                expanded_queries.insert(0, query)
            
            self.expansion_cache[cache_key] = expanded_queries
            logger.info(f"Query expanded: {query} -> {len(expanded_queries)} variants")
            return expanded_queries
            
        except Exception as e:
            logger.error(f"Query expansion failed: {e}")
            return [query]
    
    async def rewrite_query_for_context(self, query: str, chat_history: List[Dict[str, str]]) -> str:
        """åŸºäºå¯¹è¯å†å²é‡å†™æŸ¥è¯¢"""
        if not chat_history:
            return query
        
        # è·å–æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡
        recent_context = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in chat_history[-6:]  # æœ€è¿‘3è½®å¯¹è¯
        ])
        
        prompt = f"""
åŸºäºä»¥ä¸‹å¯¹è¯å†å²ï¼Œé‡å†™ç”¨æˆ·çš„æœ€æ–°æŸ¥è¯¢ï¼Œä½¿å…¶æ›´åŠ å®Œæ•´å’Œæ˜ç¡®ã€‚
é‡å†™åçš„æŸ¥è¯¢åº”è¯¥ï¼š
1. åŒ…å«å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
2. æ¶ˆé™¤ä»£è¯æ­§ä¹‰
3. è¡¥å……éšå«çš„ä¿¡æ¯
4. ä¿æŒåŸå§‹æ„å›¾

å¯¹è¯å†å²ï¼š
{recent_context}

å½“å‰æŸ¥è¯¢ï¼š{query}

è¯·ç›´æ¥è¿”å›é‡å†™åçš„æŸ¥è¯¢ï¼ˆä¸è¦åŒ…å«å…¶ä»–è§£é‡Šï¼‰ï¼š
"""
        
        try:
            response = await self.llm.acomplete(prompt)
            rewritten_query = response.text.strip()
            
            if rewritten_query and len(rewritten_query) > len(query) * 0.5:
                logger.info(f"Query rewritten: '{query}' -> '{rewritten_query}'")
                return rewritten_query
            else:
                return query
                
        except Exception as e:
            logger.error(f"Query rewriting failed: {e}")
            return query


class MultiPathRetriever:
    """å¤šè·¯å¬å›æ£€ç´¢å™¨"""
    
    def __init__(self, indices: Dict[str, VectorStoreIndex]):
        self.indices = indices
        self.retrieval_paths = {
            "dense_vector": self._dense_vector_retrieve,
            "sparse_bm25": self._sparse_bm25_retrieve,
            "hybrid_fusion": self._hybrid_fusion_retrieve,
            "semantic_similarity": self._semantic_similarity_retrieve
        }
    
    async def _dense_vector_retrieve(self, query: str, top_k: int, index_ids: List[str]) -> List[NodeWithScore]:
        """å¯†é›†å‘é‡æ£€ç´¢"""
        all_nodes = []
        
        for index_id in index_ids:
            if index_id in self.indices:
                index = self.indices[index_id]
                retriever = index.as_retriever(similarity_top_k=top_k)
                nodes = await retriever.aretrieve(QueryBundle(query_str=query))
                
                # æ·»åŠ æ£€ç´¢è·¯å¾„æ ‡è¯†
                for node in nodes:
                    node.metadata = node.metadata or {}
                    node.metadata["retrieval_path"] = "dense_vector"
                    node.metadata["source_index"] = index_id
                
                all_nodes.extend(nodes)
        
        return all_nodes
    
    async def _sparse_bm25_retrieve(self, query: str, top_k: int, index_ids: List[str]) -> List[NodeWithScore]:
        """ç¨€ç–BM25æ£€ç´¢"""
        all_nodes = []
        
        for index_id in index_ids:
            if index_id in self.indices:
                index = self.indices[index_id]
                try:
                    if index._storage_context and index._storage_context.docstore:
                        bm25_retriever = BM25Retriever.from_defaults(
                            docstore=index._storage_context.docstore,
                            similarity_top_k=top_k
                        )
                        nodes = await bm25_retriever.aretrieve(QueryBundle(query_str=query))
                        
                        # æ·»åŠ æ£€ç´¢è·¯å¾„æ ‡è¯†
                        for node in nodes:
                            node.metadata = node.metadata or {}
                            node.metadata["retrieval_path"] = "sparse_bm25"
                            node.metadata["source_index"] = index_id
                        
                        all_nodes.extend(nodes)
                except Exception as e:
                    logger.warning(f"BM25 retrieval failed for index {index_id}: {e}")
        
        return all_nodes
    
    async def _hybrid_fusion_retrieve(self, query: str, top_k: int, index_ids: List[str]) -> List[NodeWithScore]:
        """æ··åˆèåˆæ£€ç´¢"""
        all_retrievers = []
        
        for index_id in index_ids:
            if index_id in self.indices:
                index = self.indices[index_id]
                
                # å‘é‡æ£€ç´¢å™¨
                vector_retriever = index.as_retriever(similarity_top_k=top_k)
                all_retrievers.append(vector_retriever)
                
                # BM25æ£€ç´¢å™¨
                try:
                    if index._storage_context and index._storage_context.docstore:
                        bm25_retriever = BM25Retriever.from_defaults(
                            docstore=index._storage_context.docstore,
                            similarity_top_k=top_k
                        )
                        all_retrievers.append(bm25_retriever)
                except Exception as e:
                    logger.warning(f"BM25 setup failed for index {index_id}: {e}")
        
        if not all_retrievers:
            return []
        
        try:
            fusion_retriever = QueryFusionRetriever(
                retrievers=all_retrievers,
                similarity_top_k=top_k,
                num_queries=3,
                mode="reciprocal_rerank",
                use_async=True
            )
            
            nodes = await fusion_retriever.aretrieve(QueryBundle(query_str=query))
            
            # æ·»åŠ æ£€ç´¢è·¯å¾„æ ‡è¯†
            for node in nodes:
                node.metadata = node.metadata or {}
                node.metadata["retrieval_path"] = "hybrid_fusion"
            
            return nodes
            
        except Exception as e:
            logger.error(f"Hybrid fusion retrieval failed: {e}")
            return []
    
    async def _semantic_similarity_retrieve(self, query: str, top_k: int, index_ids: List[str]) -> List[NodeWithScore]:
        """è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆä½¿ç”¨æ›´é«˜çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰"""
        all_nodes = []
        
        for index_id in index_ids:
            if index_id in self.indices:
                index = self.indices[index_id]
                retriever = index.as_retriever(similarity_top_k=top_k * 2)  # è·å–æ›´å¤šå€™é€‰
                
                # ä½¿ç”¨è¿‡æ»¤æ£€ç´¢å™¨æé«˜è´¨é‡
                filtered_retriever = FilteredRetriever(
                    base_retriever=retriever,
                    similarity_threshold=0.7  # æ›´é«˜çš„é˜ˆå€¼
                )
                
                nodes = await filtered_retriever.aretrieve(QueryBundle(query_str=query))
                
                # æ·»åŠ æ£€ç´¢è·¯å¾„æ ‡è¯†
                for node in nodes:
                    node.metadata = node.metadata or {}
                    node.metadata["retrieval_path"] = "semantic_similarity"
                    node.metadata["source_index"] = index_id
                
                all_nodes.extend(nodes)
        
        return all_nodes[:top_k]
    
    async def multi_path_retrieve(self, 
                                query: str, 
                                top_k: int, 
                                index_ids: List[str],
                                enabled_paths: List[str] = None) -> Dict[str, List[NodeWithScore]]:
        """æ‰§è¡Œå¤šè·¯å¬å›æ£€ç´¢"""
        if enabled_paths is None:
            enabled_paths = list(self.retrieval_paths.keys())
        
        results = {}
        tasks = []
        
        for path_name in enabled_paths:
            if path_name in self.retrieval_paths:
                task = self.retrieval_paths[path_name](query, top_k, index_ids)
                tasks.append((path_name, task))
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ£€ç´¢è·¯å¾„
        for path_name, task in tasks:
            try:
                nodes = await task
                results[path_name] = nodes
                logger.info(f"Path '{path_name}' retrieved {len(nodes)} nodes")
            except Exception as e:
                logger.error(f"Path '{path_name}' failed: {e}")
                results[path_name] = []
        
        return results


class DynamicWeightAdjuster:
    """åŠ¨æ€æƒé‡è°ƒæ•´å™¨"""
    
    def __init__(self):
        self.performance_history = {}
        self.base_weights = {
            "dense_vector": 0.4,
            "sparse_bm25": 0.3,
            "hybrid_fusion": 0.2,
            "semantic_similarity": 0.1
        }
    
    def calculate_adaptive_weights(self, 
                                 query_type: str,
                                 retrieval_results: Dict[str, List[NodeWithScore]]) -> Dict[str, float]:
        """åŸºäºæ£€ç´¢ç»“æœè´¨é‡åŠ¨æ€è°ƒæ•´æƒé‡"""
        weights = self.base_weights.copy()
        
        # åŸºäºç»“æœæ•°é‡è°ƒæ•´æƒé‡
        total_results = sum(len(nodes) for nodes in retrieval_results.values())
        if total_results == 0:
            return weights
        
        for path_name, nodes in retrieval_results.items():
            if path_name in weights:
                # åŸºäºç»“æœæ•°é‡å’Œå¹³å‡åˆ†æ•°è°ƒæ•´æƒé‡
                if nodes:
                    avg_score = sum(node.score for node in nodes) / len(nodes)
                    result_ratio = len(nodes) / total_results
                    
                    # æƒé‡è°ƒæ•´å› å­
                    quality_factor = min(avg_score * 2, 1.0)  # åŸºäºè´¨é‡
                    quantity_factor = min(result_ratio * 2, 1.0)  # åŸºäºæ•°é‡
                    
                    weights[path_name] *= (quality_factor + quantity_factor) / 2
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v / total_weight for k, v in weights.items()}
        
        logger.debug(f"Adaptive weights calculated: {weights}")
        return weights
    
    def record_performance(self, query: str, path_name: str, success_rate: float):
        """è®°å½•æ£€ç´¢è·¯å¾„æ€§èƒ½"""
        if path_name not in self.performance_history:
            self.performance_history[path_name] = []
        
        self.performance_history[path_name].append({
            "query": query,
            "success_rate": success_rate,
            "timestamp": datetime.now()
        })
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.performance_history[path_name]) > 100:
            self.performance_history[path_name] = self.performance_history[path_name][-100:]


class AdvancedRetrievalOptimizer:
    """é«˜çº§æ£€ç´¢ä¼˜åŒ–å™¨"""
    
    def __init__(self, llm: LLM, indices: Dict[str, VectorStoreIndex]):
        self.llm = llm
        self.indices = indices
        self.query_expander = QueryExpander(llm)
        self.multi_path_retriever = MultiPathRetriever(indices)
        self.weight_adjuster = DynamicWeightAdjuster()
        
        # æ£€ç´¢ç­–ç•¥é…ç½®
        self.strategies = {
            "conservative": {  # ä¿å®ˆç­–ç•¥ï¼šæ³¨é‡ç²¾ç¡®æ€§
                "enabled_paths": ["semantic_similarity", "dense_vector"],
                "expansion_enabled": False,
                "fusion_weight": 0.3
            },
            "balanced": {  # å¹³è¡¡ç­–ç•¥ï¼šç²¾ç¡®æ€§å’Œå¬å›ç‡å¹³è¡¡
                "enabled_paths": ["dense_vector", "sparse_bm25", "hybrid_fusion"],
                "expansion_enabled": True,
                "fusion_weight": 0.5
            },
            "aggressive": {  # æ¿€è¿›ç­–ç•¥ï¼šæ³¨é‡å¬å›ç‡
                "enabled_paths": list(self.multi_path_retriever.retrieval_paths.keys()),
                "expansion_enabled": True,
                "fusion_weight": 0.7
            }
        }
    
    async def optimize_retrieval_for_chat(self,
                                        query: str,
                                        index_ids: List[str],
                                        chat_history: List[Dict[str, str]] = None,
                                        top_k: int = 10,
                                        strategy: str = "balanced") -> List[NodeWithScore]:
        """ä¸ºå¯¹è¯ä¼˜åŒ–æ£€ç´¢"""
        logger.info(f"ğŸš€ Starting optimized retrieval for chat query: {query[:100]}...")
        
        # 1. åŸºäºå¯¹è¯å†å²é‡å†™æŸ¥è¯¢
        optimized_query = query
        if chat_history:
            optimized_query = await self.query_expander.rewrite_query_for_context(query, chat_history)
        
        # 2. æŸ¥è¯¢æ‰©å±•
        queries_to_search = [optimized_query]
        strategy_config = self.strategies.get(strategy, self.strategies["balanced"])
        
        if strategy_config["expansion_enabled"]:
            context = self._extract_context_from_history(chat_history) if chat_history else None
            expanded_queries = await self.query_expander.expand_query(optimized_query, context)
            queries_to_search.extend(expanded_queries[1:])  # æ’é™¤åŸå§‹æŸ¥è¯¢
        
        logger.info(f"ğŸ“ Using {len(queries_to_search)} queries for retrieval")
        
        # 3. å¤šè·¯å¬å›æ£€ç´¢
        all_retrieval_results = {}
        
        for i, search_query in enumerate(queries_to_search):
            logger.debug(f"ğŸ” Processing query {i+1}/{len(queries_to_search)}: {search_query[:50]}...")
            
            path_results = await self.multi_path_retriever.multi_path_retrieve(
                query=search_query,
                top_k=top_k,
                index_ids=index_ids,
                enabled_paths=strategy_config["enabled_paths"]
            )
            
            # åˆå¹¶ç»“æœ
            for path_name, nodes in path_results.items():
                if path_name not in all_retrieval_results:
                    all_retrieval_results[path_name] = []
                all_retrieval_results[path_name].extend(nodes)
        
        # 4. åŠ¨æ€æƒé‡è°ƒæ•´
        adaptive_weights = self.weight_adjuster.calculate_adaptive_weights(
            query_type=self._classify_query_type(query),
            retrieval_results=all_retrieval_results
        )
        
        # 5. ç»“æœèåˆå’Œå»é‡
        final_nodes = self._fuse_and_deduplicate_results(
            all_retrieval_results,
            adaptive_weights,
            top_k
        )
        
        # 6. å¯¹è¯ä¸Šä¸‹æ–‡å¢å¼º
        if chat_history:
            final_nodes = self._enhance_with_chat_context(final_nodes, chat_history, query)
        
        logger.info(f"âœ… Optimized retrieval completed, returning {len(final_nodes)} nodes")
        return final_nodes
    
    def _extract_context_from_history(self, chat_history: List[Dict[str, str]]) -> str:
        """ä»å¯¹è¯å†å²ä¸­æå–ä¸Šä¸‹æ–‡"""
        if not chat_history:
            return ""
        
        # æå–æœ€è¿‘å‡ è½®å¯¹è¯çš„å…³é”®ä¿¡æ¯
        recent_messages = chat_history[-4:]  # æœ€è¿‘2è½®å¯¹è¯
        context_parts = []
        
        for msg in recent_messages:
            if msg["role"] == "user":
                context_parts.append(f"ç”¨æˆ·é—®é¢˜: {msg['content']}")
            elif msg["role"] == "assistant":
                # åªä¿ç•™å›ç­”çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
                content = msg['content'][:100] + "..." if len(msg['content']) > 100 else msg['content']
                context_parts.append(f"åŠ©æ‰‹å›ç­”: {content}")
        
        return "\n".join(context_parts)
    
    def _classify_query_type(self, query: str) -> str:
        """åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        if any(keyword in query_lower for keyword in ["ä»€ä¹ˆæ˜¯", "å®šä¹‰", "æ¦‚å¿µ"]):
            return "definition"
        elif any(keyword in query_lower for keyword in ["å¦‚ä½•", "æ€ä¹ˆ", "æ–¹æ³•"]):
            return "how_to"
        elif any(keyword in query_lower for keyword in ["ä¸ºä»€ä¹ˆ", "åŸå› ", "why"]):
            return "explanation"
        elif any(keyword in query_lower for keyword in ["æ¯”è¾ƒ", "åŒºåˆ«", "å·®å¼‚"]):
            return "comparison"
        else:
            return "general"
    
    def _fuse_and_deduplicate_results(self,
                                     retrieval_results: Dict[str, List[NodeWithScore]],
                                     weights: Dict[str, float],
                                     top_k: int) -> List[NodeWithScore]:
        """èåˆå’Œå»é‡æ£€ç´¢ç»“æœ"""
        # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹å¹¶è®¡ç®—åŠ æƒåˆ†æ•°
        node_scores = {}
        
        for path_name, nodes in retrieval_results.items():
            weight = weights.get(path_name, 0.0)
            
            for node in nodes:
                node_id = self._get_node_id(node)
                
                if node_id not in node_scores:
                    node_scores[node_id] = {
                        "node": node,
                        "weighted_score": 0.0,
                        "path_scores": {}
                    }
                
                # ç´¯åŠ åŠ æƒåˆ†æ•°
                node_scores[node_id]["weighted_score"] += node.score * weight
                node_scores[node_id]["path_scores"][path_name] = node.score
        
        # æŒ‰åŠ æƒåˆ†æ•°æ’åº
        sorted_nodes = sorted(
            node_scores.values(),
            key=lambda x: x["weighted_score"],
            reverse=True
        )
        
        # æ›´æ–°èŠ‚ç‚¹åˆ†æ•°å¹¶è¿”å›
        final_nodes = []
        for item in sorted_nodes[:top_k]:
            node = item["node"]
            node.score = item["weighted_score"]
            
            # æ·»åŠ èåˆä¿¡æ¯åˆ°å…ƒæ•°æ®
            if not hasattr(node, 'metadata') or node.metadata is None:
                node.metadata = {}
            node.metadata["fusion_score"] = item["weighted_score"]
            node.metadata["path_scores"] = item["path_scores"]
            
            final_nodes.append(node)
        
        return final_nodes
    
    def _get_node_id(self, node: NodeWithScore) -> str:
        """è·å–èŠ‚ç‚¹å”¯ä¸€æ ‡è¯†"""
        # ä½¿ç”¨èŠ‚ç‚¹æ–‡æœ¬çš„å“ˆå¸Œä½œä¸ºå”¯ä¸€æ ‡è¯†
        return str(hash(node.node.text))
    
    def _enhance_with_chat_context(self,
                                 nodes: List[NodeWithScore],
                                 chat_history: List[Dict[str, str]],
                                 current_query: str) -> List[NodeWithScore]:
        """åŸºäºå¯¹è¯ä¸Šä¸‹æ–‡å¢å¼ºæ£€ç´¢ç»“æœ"""
        if not chat_history:
            return nodes
        
        # æå–å¯¹è¯ä¸­çš„å…³é”®è¯
        chat_keywords = set()
        for msg in chat_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
            content = msg["content"].lower()
            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
            words = content.split()
            chat_keywords.update([word for word in words if len(word) > 2])
        
        # åŸºäºå¯¹è¯ä¸Šä¸‹æ–‡è°ƒæ•´èŠ‚ç‚¹åˆ†æ•°
        for node in nodes:
            context_boost = 0.0
            node_text = node.node.text.lower()
            
            # è®¡ç®—ä¸å¯¹è¯å†å²çš„ç›¸å…³æ€§
            matching_keywords = sum(1 for keyword in chat_keywords if keyword in node_text)
            if matching_keywords > 0:
                context_boost = min(matching_keywords * 0.05, 0.2)  # æœ€å¤šæå‡0.2åˆ†
            
            # åº”ç”¨ä¸Šä¸‹æ–‡å¢å¼º
            node.score = min(node.score + context_boost, 1.0)
            
            # è®°å½•å¢å¼ºä¿¡æ¯
            if not hasattr(node, 'metadata') or node.metadata is None:
                node.metadata = {}
            node.metadata["context_boost"] = context_boost
            node.metadata["matching_keywords"] = matching_keywords
        
        # é‡æ–°æ’åº
        nodes.sort(key=lambda x: x.score, reverse=True)
        return nodes
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "available_strategies": list(self.strategies.keys()),
            "retrieval_paths": list(self.multi_path_retriever.retrieval_paths.keys()),
            "performance_history": {
                path: len(history) for path, history in self.weight_adjuster.performance_history.items()
            },
            "cache_stats": {
                "expansion_cache_size": len(self.query_expander.expansion_cache)
            }
        }