#!/usr/bin/env python3
"""
CUDAå†…å­˜ä¸è¶³é—®é¢˜è§£å†³è„šæœ¬

æ­¤è„šæœ¬æä¾›äº†è§£å†³CUDA out of memoryé”™è¯¯çš„å¤šç§æ–¹æ³•ï¼š
1. æ¸…ç†GPUå†…å­˜
2. è°ƒæ•´åµŒå…¥æ¨¡å‹æ‰¹å¤„ç†å¤§å°
3. æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
4. æä¾›ä¼˜åŒ–å»ºè®®
"""

import os
import sys
import torch
import gc
import subprocess
from pathlib import Path

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("ğŸ” æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥GPUé©±åŠ¨å’ŒPyTorchå®‰è£…")
        return False
    
    device_count = torch.cuda.device_count()
    print(f"ğŸ“Š æ£€æµ‹åˆ° {device_count} ä¸ªGPUè®¾å¤‡")
    
    for i in range(device_count):
        device = f"cuda:{i}"
        props = torch.cuda.get_device_properties(device)
        total_memory = props.total_memory / 1024**3  # GB
        allocated = torch.cuda.memory_allocated(device) / 1024**3  # GB
        cached = torch.cuda.memory_reserved(device) / 1024**3  # GB
        free = total_memory - cached
        
        print(f"\nğŸ¯ GPU {i} ({props.name}):")
        print(f"   æ€»å†…å­˜: {total_memory:.2f} GB")
        print(f"   å·²åˆ†é…: {allocated:.2f} GB")
        print(f"   å·²ç¼“å­˜: {cached:.2f} GB")
        print(f"   å¯ç”¨å†…å­˜: {free:.2f} GB")
        
        if free < 1.0:  # å°‘äº1GBå¯ç”¨å†…å­˜
            print(f"   âš ï¸  è­¦å‘Š: GPU {i} å¯ç”¨å†…å­˜ä¸è¶³ ({free:.2f} GB)")
    
    return True

def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    print("\nğŸ§¹ æ¸…ç†GPUå†…å­˜...")
    
    if torch.cuda.is_available():
        # æ¸…ç†PyTorchç¼“å­˜
        torch.cuda.empty_cache()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        print("âœ… GPUå†…å­˜æ¸…ç†å®Œæˆ")
        
        # å†æ¬¡æ£€æŸ¥å†…å­˜
        check_gpu_memory()
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•æ¸…ç†GPUå†…å­˜")

def update_embedding_batch_size(batch_size=1):
    """æ›´æ–°åµŒå…¥æ¨¡å‹æ‰¹å¤„ç†å¤§å°"""
    print(f"\nâš™ï¸  è®¾ç½®åµŒå…¥æ¨¡å‹æ‰¹å¤„ç†å¤§å°ä¸º {batch_size}...")
    
    # æ£€æŸ¥.envæ–‡ä»¶
    env_file = Path(".env")
    env_content = ""
    
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            env_content = f.read()
    
    # æ›´æ–°æˆ–æ·»åŠ EMBEDDING_BATCH_SIZE
    lines = env_content.split('\n')
    updated = False
    
    for i, line in enumerate(lines):
        if line.startswith('EMBEDDING_BATCH_SIZE='):
            lines[i] = f'EMBEDDING_BATCH_SIZE={batch_size}'
            updated = True
            break
    
    if not updated:
        lines.append(f'EMBEDDING_BATCH_SIZE={batch_size}')
    
    # å†™å›æ–‡ä»¶
    with open(env_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… å·²è®¾ç½® EMBEDDING_BATCH_SIZE={batch_size}")
    print("ğŸ“ è¯·é‡å¯åº”ç”¨ä»¥ä½¿é…ç½®ç”Ÿæ•ˆ")

def kill_gpu_processes():
    """ç»ˆæ­¢å ç”¨GPUçš„è¿›ç¨‹ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰"""
    print("\nâš ï¸  å°è¯•ç»ˆæ­¢å ç”¨GPUçš„è¿›ç¨‹...")
    
    try:
        # ä½¿ç”¨nvidia-smiæŸ¥æ‰¾GPUè¿›ç¨‹
        result = subprocess.run(
            ['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader,nounits'],
            capture_output=True, text=True
        )
        
        if result.returncode == 0:
            pids = [pid.strip() for pid in result.stdout.split('\n') if pid.strip()]
            
            if pids:
                print(f"ğŸ” å‘ç° {len(pids)} ä¸ªGPUè¿›ç¨‹: {', '.join(pids)}")
                
                response = input("æ˜¯å¦è¦ç»ˆæ­¢è¿™äº›è¿›ç¨‹ï¼Ÿè¿™å¯èƒ½ä¼šå½±å“å…¶ä»–æ­£åœ¨è¿è¡Œçš„ç¨‹åº (y/N): ")
                if response.lower() == 'y':
                    for pid in pids:
                        try:
                            subprocess.run(['kill', '-9', pid], check=True)
                            print(f"âœ… å·²ç»ˆæ­¢è¿›ç¨‹ {pid}")
                        except subprocess.CalledProcessError:
                            print(f"âŒ æ— æ³•ç»ˆæ­¢è¿›ç¨‹ {pid}")
                else:
                    print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            else:
                print("âœ… æ²¡æœ‰å‘ç°GPUè¿›ç¨‹")
        else:
            print("âŒ æ— æ³•æŸ¥è¯¢GPUè¿›ç¨‹ï¼Œè¯·ç¡®ä¿å®‰è£…äº†nvidia-smi")
    
    except FileNotFoundError:
        print("âŒ nvidia-smiæœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å®‰è£…äº†NVIDIAé©±åŠ¨")

def show_optimization_tips():
    """æ˜¾ç¤ºä¼˜åŒ–å»ºè®®"""
    print("\nğŸ’¡ CUDAå†…å­˜ä¼˜åŒ–å»ºè®®:")
    print("\n1. ğŸ”§ è°ƒæ•´æ‰¹å¤„ç†å¤§å°:")
    print("   - å½“å‰é»˜è®¤æ‰¹å¤„ç†å¤§å°ä¸º2ï¼Œå¯ä»¥è¿›ä¸€æ­¥é™ä½åˆ°1")
    print("   - ä½¿ç”¨ç¯å¢ƒå˜é‡: EMBEDDING_BATCH_SIZE=1")
    
    print("\n2. ğŸ¯ ä¼˜åŒ–å‘é‡å­˜å‚¨æ„å»º:")
    print("   - åˆ†æ‰¹å¤„ç†æ–‡æ¡£ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤§é‡æ–‡ä»¶")
    print("   - è€ƒè™‘ä½¿ç”¨CPUè¿›è¡ŒåµŒå…¥è®¡ç®—ï¼ˆé€Ÿåº¦è¾ƒæ…¢ä½†å†…å­˜å ç”¨å°‘ï¼‰")
    
    print("\n3. ğŸ§¹ å®šæœŸæ¸…ç†å†…å­˜:")
    print("   - åœ¨å¤„ç†å¤§æ–‡ä»¶å‰è¿è¡Œæ­¤è„šæœ¬æ¸…ç†GPUå†…å­˜")
    print("   - é‡å¯åº”ç”¨ä»¥å®Œå…¨é‡Šæ”¾å†…å­˜")
    
    print("\n4. âš™ï¸  ç³»ç»Ÿçº§ä¼˜åŒ–:")
    print("   - å…³é—­å…¶ä»–å ç”¨GPUçš„ç¨‹åº")
    print("   - è€ƒè™‘å‡çº§GPUå†…å­˜")
    print("   - ä½¿ç”¨æ›´å°çš„åµŒå…¥æ¨¡å‹")
    
    print("\n5. ğŸ”„ åº”æ€¥æ–¹æ¡ˆ:")
    print("   - è®¾ç½® EMBEDDING_PROVIDER_NAME=siliconflow ä½¿ç”¨åœ¨çº¿API")
    print("   - ä¸´æ—¶ç¦ç”¨GPU: USE_GPU=false")

def main():
    print("ğŸš€ CUDAå†…å­˜ä¸è¶³é—®é¢˜è§£å†³å·¥å…·")
    print("=" * 50)
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
        print("2. æ¸…ç†GPUå†…å­˜")
        print("3. è®¾ç½®åµŒå…¥æ¨¡å‹æ‰¹å¤„ç†å¤§å°")
        print("4. ç»ˆæ­¢GPUè¿›ç¨‹ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰")
        print("5. æ˜¾ç¤ºä¼˜åŒ–å»ºè®®")
        print("6. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-6): ").strip()
        
        if choice == '1':
            check_gpu_memory()
        elif choice == '2':
            clear_gpu_memory()
        elif choice == '3':
            try:
                batch_size = int(input("è¯·è¾“å…¥æ‰¹å¤„ç†å¤§å° (æ¨è1-2): "))
                if batch_size < 1:
                    print("âŒ æ‰¹å¤„ç†å¤§å°å¿…é¡»å¤§äº0")
                    continue
                update_embedding_batch_size(batch_size)
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        elif choice == '4':
            kill_gpu_processes()
        elif choice == '5':
            show_optimization_tips()
        elif choice == '6':
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")

if __name__ == "__main__":
    main()